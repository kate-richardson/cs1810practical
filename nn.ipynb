{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONE\n",
    "Step 1: Preprocess the data - use the same function across the board\n",
    "Clearing out the capitalization\n",
    "Clearing out em dashes, symbols\n",
    "Clearing out names\n",
    "\n",
    "TO DO\n",
    "Step 2: Implementing the features\n",
    "- Goal: Train a logistic regression on 2 feature representations\n",
    "\n",
    "Step 3: Implementing the regression based on that data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Imports and Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kate/anaconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "val = pd.read_csv('data/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for cleaning text\n",
    "def clean_html(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # Remove HTML tags\n",
    "    clean = re.sub(r'<.*?>', '', str(text))\n",
    "    # Remove extra whitespaces\n",
    "    clean = re.sub(r'\\s+', ' ', clean).strip()\n",
    "    # Replace HTML entities\n",
    "    clean = re.sub(r'&amp;', '&', clean)\n",
    "    clean = re.sub(r'&lt;', '<', clean)\n",
    "    clean = re.sub(r'&gt;', '>', clean)\n",
    "    clean = re.sub(r'&quot;|&#34;', '\"', clean)\n",
    "    clean = re.sub(r'&apos;|&#39;', \"'\", clean)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    snip   channel\n",
      "0      first of all, it feels like covid again but in...  FOXNEWSW\n",
      "1      to be a software drivenrganization where softw...     CSPAN\n",
      "2      you discuss the power of ai to revolutionize t...    CSPAN2\n",
      "3      ai bots like chatgpt and google's bard gained ...   BBCNEWS\n",
      "4      . >> i could sleep ten hours ai night if i was...  FOXNEWSW\n",
      "...                                                  ...       ...\n",
      "19868  cardiovascular science, but they're also pione...  FOXNEWSW\n",
      "19869  i of ai in different fields. have of ai in dif...   BBCNEWS\n",
      "19870  weighing down on the major averages, both tech...      KTVU\n",
      "19871  i also think crypto ai that legislation be fro...    CSPAN2\n",
      "19872  as we have worked to monitor the adoption iden...    CSPAN2\n",
      "\n",
      "[19873 rows x 2 columns]\n",
      "                                                   snip    channel\n",
      "0     . ♪ >> there's a kyu cho right have things tha...  BLOOMBERG\n",
      "1     he says the ai tool helped create a new fronti...       KPIX\n",
      "2     . >> the all new godaddy arrow put your busine...       CNNW\n",
      "3     in some cases they are powered by generative a...      CSPAN\n",
      "4     this was a ivotal it comes to ai. this was a p...    BBCNEWS\n",
      "...                                                 ...        ...\n",
      "3034  however, the ai trade is only one part of the ...       CNBC\n",
      "3035  oz but also was highlighted as a product by cr...     CSPAN2\n",
      "3036  the all new godaddy airo helps you get your bu...       CNBC\n",
      "3037  we are going to be way ahead on ai. we have to...       CNBC\n",
      "3038  his fourth management role after spells at der...    BBCNEWS\n",
      "\n",
      "[3039 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# use the clean_html function to clean the training data\n",
    "train['snip'] = train['snip'].apply(clean_html)\n",
    "val['snip'] = val['snip'].apply(clean_html)\n",
    "\n",
    "print(train)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metric equation\n",
    "def eval(y_pred, y_true):\n",
    "    correct = (y_pred == y_true)   # Boolean array: True if correct, False if wrong\n",
    "    accuracy = correct.sum() / len(y_true)  # Correct / Total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    snip   channel  \\\n",
      "0      first of all, it feels like covid again but in...  FOXNEWSW   \n",
      "1      to be a software drivenrganization where softw...     CSPAN   \n",
      "2      you discuss the power of ai to revolutionize t...    CSPAN2   \n",
      "3      ai bots like chatgpt and google's bard gained ...   BBCNEWS   \n",
      "4      . >> i could sleep ten hours ai night if i was...  FOXNEWSW   \n",
      "...                                                  ...       ...   \n",
      "19868  cardiovascular science, but they're also pione...  FOXNEWSW   \n",
      "19869  i of ai in different fields. have of ai in dif...   BBCNEWS   \n",
      "19870  weighing down on the major averages, both tech...      KTVU   \n",
      "19871  i also think crypto ai that legislation be fro...    CSPAN2   \n",
      "19872  as we have worked to monitor the adoption iden...    CSPAN2   \n",
      "\n",
      "       word_complexity  \n",
      "0             0.057407  \n",
      "1             0.079768  \n",
      "2             0.076151  \n",
      "3             0.077301  \n",
      "4             0.075782  \n",
      "...                ...  \n",
      "19868         0.080295  \n",
      "19869         0.093252  \n",
      "19870         0.080397  \n",
      "19871         0.079097  \n",
      "19872         0.080723  \n",
      "\n",
      "[19873 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(train['snip'])\n",
    "\n",
    "# Calculate word complexity per snip\n",
    "word_complexity = X_tfidf.sum(axis=1) / (X_tfidf != 0).sum(axis=1)\n",
    "word_complexity = np.array(word_complexity).flatten()\n",
    "\n",
    "# Add it to the train dataframe\n",
    "train['word_complexity'] = word_complexity\n",
    "\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   snip    channel  \\\n",
      "0     . ♪ >> there's a kyu cho right have things tha...  BLOOMBERG   \n",
      "1     he says the ai tool helped create a new fronti...       KPIX   \n",
      "2     . >> the all new godaddy arrow put your busine...       CNNW   \n",
      "3     in some cases they are powered by generative a...      CSPAN   \n",
      "4     this was a ivotal it comes to ai. this was a p...    BBCNEWS   \n",
      "...                                                 ...        ...   \n",
      "3034  however, the ai trade is only one part of the ...       CNBC   \n",
      "3035  oz but also was highlighted as a product by cr...     CSPAN2   \n",
      "3036  the all new godaddy airo helps you get your bu...       CNBC   \n",
      "3037  we are going to be way ahead on ai. we have to...       CNBC   \n",
      "3038  his fourth management role after spells at der...    BBCNEWS   \n",
      "\n",
      "      word_complexity  \n",
      "0            0.088216  \n",
      "1            0.081808  \n",
      "2            0.074722  \n",
      "3            0.082919  \n",
      "4            0.083567  \n",
      "...               ...  \n",
      "3034         0.079930  \n",
      "3035         0.081606  \n",
      "3036         0.080997  \n",
      "3037         0.078856  \n",
      "3038         0.075032  \n",
      "\n",
      "[3039 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Transform validation snips using the same TF-IDF vectorizer\n",
    "x_val_tfidf = vectorizer.transform(val['snip'])\n",
    "\n",
    "# Compute complexity\n",
    "val_word_complexity = x_val_tfidf.sum(axis=1) / (x_val_tfidf != 0).sum(axis=1)\n",
    "val_word_complexity = np.array(val_word_complexity).flatten()\n",
    "\n",
    "# Add to val DataFrame\n",
    "val['word_complexity'] = val_word_complexity\n",
    "\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = features, y = labels\n",
    "x_train = train[['word_complexity']]  # Needs to be 2D\n",
    "y_train = train['channel']\n",
    "\n",
    "x_val = val[['word_complexity']]\n",
    "y_val = val['channel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNNW' 'CNNW' 'CNNW' ... 'CNNW' 'CNNW' 'CNNW']\n",
      "0.07535373478117802\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "val_preds = model.predict(x_val)\n",
    "print(val_preds)\n",
    "\n",
    "# Evaluate\n",
    "val_accuracy = eval(val_preds, y_val)\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now do the min/max training to create new average values of scores\n",
    "\n",
    "scaled = current - min/ max-min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0.057407\n",
      "1        0.079768\n",
      "2        0.076151\n",
      "3        0.077301\n",
      "4        0.075782\n",
      "           ...   \n",
      "19868    0.080295\n",
      "19869    0.093252\n",
      "19870    0.080397\n",
      "19871    0.079097\n",
      "19872    0.080723\n",
      "Name: word_complexity, Length: 19873, dtype: float64\n",
      "0.035929255415410526 0.6822632439493239\n"
     ]
    }
   ],
   "source": [
    "min = 1\n",
    "max = 0\n",
    "\n",
    "complexity = train['word_complexity']\n",
    "print(complexity)\n",
    "\n",
    "for i in range(len(complexity)):\n",
    "    if complexity[i] <= min:\n",
    "        min = complexity[i]\n",
    "    if complexity[i] >= max:\n",
    "        max = complexity[i]\n",
    "    else:\n",
    "        min = min\n",
    "        max = max\n",
    "\n",
    "print(min,max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complexity is 2D\n",
    "def scale(complexity):\n",
    "    x_train = (train[['word_complexity']] - min )/ (max-min)\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_complexity = train[\"word_complexity\"].min()\n",
    "max_complexity = train[\"word_complexity\"].max()\n",
    "train[\"word_complexity_scaled\"] = (train[\"word_complexity\"] - min_complexity) / (max_complexity - min_complexity)\n",
    "val[\"word_complexity_scaled\"] = (val[\"word_complexity\"] - min_complexity) / (max_complexity - min_complexity)\n",
    "\n",
    "# X = features, y = labels\n",
    "x_train = train[['word_complexity_scaled']]\n",
    "y_train = train['channel']\n",
    "\n",
    "x_val = val[['word_complexity_scaled']]\n",
    "y_val = val['channel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNNW' 'CNNW' 'CNNW' ... 'CNNW' 'CNNW' 'CNNW']\n",
      "0.07666995722277065\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "val_preds = model.predict(x_val)\n",
    "print(val_preds)\n",
    "\n",
    "# Evaluate\n",
    "val_accuracy = eval(val_preds, y_val)\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FOXNEWSW', 'CSPAN', 'CSPAN2', 'BBCNEWS', 'GBN', 'KPIX', 'KGO', 'KNTV', '1TV', 'KRON', 'CSPAN3', 'SFGTV', 'RUSSIA24', 'KSTS', 'BLOOMBERG', 'MSNBCW', 'PRESSTV', 'KTVU', 'CNNW', 'FBC', 'CNBC', 'RUSSIA1', 'KDTV', 'DW', 'KQED', 'NTV', 'BELARUSTV', 'ALJAZ', 'RT', 'LINKTV', 'COM']\n"
     ]
    }
   ],
   "source": [
    "# create list of all of the channels\n",
    "channels = []\n",
    "\n",
    "for i in range(len(train)):\n",
    "    if train['channel'][i] not in channels:\n",
    "        channels.append(train['channel'][i])\n",
    "    else:\n",
    "        channels = channels\n",
    "\n",
    "# \n",
    "\n",
    "print(channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "train_corpus = list(train['snip'].values)\n",
    "val_corpus = list(val['snip'].values)\n",
    "nlp_sentiment = pipeline(\"sentiment-analysis\")\n",
    "train[\"Sentiment\"] = nlp_sentiment(train_corpus)\n",
    "val[\"Sentiment\"] = nlp_sentiment(val_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snip</th>\n",
       "      <th>channel</th>\n",
       "      <th>word_complexity</th>\n",
       "      <th>word_complexity_scaled</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Label</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first of all, it feels like covid again but in...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.057407</td>\n",
       "      <td>0.033231</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9903525710105...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.990353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to be a software drivenrganization where softw...</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>0.079768</td>\n",
       "      <td>0.067826</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9933253526687...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.993325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you discuss the power of ai to revolutionize t...</td>\n",
       "      <td>CSPAN2</td>\n",
       "      <td>0.076151</td>\n",
       "      <td>0.062230</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9971946477890...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.997195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai bots like chatgpt and google's bard gained ...</td>\n",
       "      <td>BBCNEWS</td>\n",
       "      <td>0.077301</td>\n",
       "      <td>0.064009</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9900817275047...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.990082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>. &gt;&gt; i could sleep ten hours ai night if i was...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.075782</td>\n",
       "      <td>0.061660</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.8428422808647...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.842842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                snip   channel  \\\n",
       "0  first of all, it feels like covid again but in...  FOXNEWSW   \n",
       "1  to be a software drivenrganization where softw...     CSPAN   \n",
       "2  you discuss the power of ai to revolutionize t...    CSPAN2   \n",
       "3  ai bots like chatgpt and google's bard gained ...   BBCNEWS   \n",
       "4  . >> i could sleep ten hours ai night if i was...  FOXNEWSW   \n",
       "\n",
       "   word_complexity  word_complexity_scaled  \\\n",
       "0         0.057407                0.033231   \n",
       "1         0.079768                0.067826   \n",
       "2         0.076151                0.062230   \n",
       "3         0.077301                0.064009   \n",
       "4         0.075782                0.061660   \n",
       "\n",
       "                                           Sentiment Sentiment_Label  \\\n",
       "0  {'label': 'NEGATIVE', 'score': 0.9903525710105...        NEGATIVE   \n",
       "1  {'label': 'POSITIVE', 'score': 0.9933253526687...        POSITIVE   \n",
       "2  {'label': 'NEGATIVE', 'score': 0.9971946477890...        NEGATIVE   \n",
       "3  {'label': 'POSITIVE', 'score': 0.9900817275047...        POSITIVE   \n",
       "4  {'label': 'NEGATIVE', 'score': 0.8428422808647...        NEGATIVE   \n",
       "\n",
       "   Sentiment_Score  \n",
       "0         0.990353  \n",
       "1         0.993325  \n",
       "2         0.997195  \n",
       "3         0.990082  \n",
       "4         0.842842  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Sentiment_Label'] = [x.get('label') for x in train['Sentiment']]\n",
    "train['Sentiment_Score'] = [x.get('score') for x in train['Sentiment']]\n",
    "val['Sentiment_Label'] = [x.get('label') for x in val['Sentiment']]\n",
    "val['Sentiment_Score'] = [x.get('score') for x in val['Sentiment']]\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snip</th>\n",
       "      <th>channel</th>\n",
       "      <th>word_complexity</th>\n",
       "      <th>word_complexity_scaled</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Label</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first of all, it feels like covid again but in...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.057407</td>\n",
       "      <td>0.033231</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9903525710105...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.990353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to be a software drivenrganization where softw...</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>0.079768</td>\n",
       "      <td>0.067826</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9933253526687...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.993325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you discuss the power of ai to revolutionize t...</td>\n",
       "      <td>CSPAN2</td>\n",
       "      <td>0.076151</td>\n",
       "      <td>0.062230</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9971946477890...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.997195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai bots like chatgpt and google's bard gained ...</td>\n",
       "      <td>BBCNEWS</td>\n",
       "      <td>0.077301</td>\n",
       "      <td>0.064009</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9900817275047...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.990082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>. &gt;&gt; i could sleep ten hours ai night if i was...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.075782</td>\n",
       "      <td>0.061660</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.8428422808647...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.842842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                snip   channel  \\\n",
       "0  first of all, it feels like covid again but in...  FOXNEWSW   \n",
       "1  to be a software drivenrganization where softw...     CSPAN   \n",
       "2  you discuss the power of ai to revolutionize t...    CSPAN2   \n",
       "3  ai bots like chatgpt and google's bard gained ...   BBCNEWS   \n",
       "4  . >> i could sleep ten hours ai night if i was...  FOXNEWSW   \n",
       "\n",
       "   word_complexity  word_complexity_scaled  \\\n",
       "0         0.057407                0.033231   \n",
       "1         0.079768                0.067826   \n",
       "2         0.076151                0.062230   \n",
       "3         0.077301                0.064009   \n",
       "4         0.075782                0.061660   \n",
       "\n",
       "                                           Sentiment Sentiment_Label  \\\n",
       "0  {'label': 'NEGATIVE', 'score': 0.9903525710105...        NEGATIVE   \n",
       "1  {'label': 'POSITIVE', 'score': 0.9933253526687...        POSITIVE   \n",
       "2  {'label': 'NEGATIVE', 'score': 0.9971946477890...        NEGATIVE   \n",
       "3  {'label': 'POSITIVE', 'score': 0.9900817275047...        POSITIVE   \n",
       "4  {'label': 'NEGATIVE', 'score': 0.8428422808647...        NEGATIVE   \n",
       "\n",
       "   Sentiment_Score  \n",
       "0        -0.990353  \n",
       "1         0.993325  \n",
       "2        -0.997195  \n",
       "3         0.990082  \n",
       "4        -0.842842  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Sentiment_Score\"] = np.where(\n",
    "    train[\"Sentiment_Label\"] == \"NEGATIVE\", -(train[\"Sentiment_Score\"]), train[\"Sentiment_Score\"]\n",
    ")\n",
    "\n",
    "val[\"Sentiment_Score\"] = np.where(\n",
    "    val[\"Sentiment_Label\"] == \"NEGATIVE\", -(val[\"Sentiment_Score\"]), val[\"Sentiment_Score\"]\n",
    ")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snip</th>\n",
       "      <th>channel</th>\n",
       "      <th>word_complexity</th>\n",
       "      <th>word_complexity_scaled</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Label</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <th>Sentiment_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first of all, it feels like covid again but in...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.057407</td>\n",
       "      <td>0.033231</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9903525710105...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.990353</td>\n",
       "      <td>0.004713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to be a software drivenrganization where softw...</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>0.079768</td>\n",
       "      <td>0.067826</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9933253526687...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.993325</td>\n",
       "      <td>0.996727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you discuss the power of ai to revolutionize t...</td>\n",
       "      <td>CSPAN2</td>\n",
       "      <td>0.076151</td>\n",
       "      <td>0.062230</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9971946477890...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.997195</td>\n",
       "      <td>0.001292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai bots like chatgpt and google's bard gained ...</td>\n",
       "      <td>BBCNEWS</td>\n",
       "      <td>0.077301</td>\n",
       "      <td>0.064009</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9900817275047...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.990082</td>\n",
       "      <td>0.995105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>. &gt;&gt; i could sleep ten hours ai night if i was...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.075782</td>\n",
       "      <td>0.061660</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.8428422808647...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.842842</td>\n",
       "      <td>0.078481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                snip   channel  \\\n",
       "0  first of all, it feels like covid again but in...  FOXNEWSW   \n",
       "1  to be a software drivenrganization where softw...     CSPAN   \n",
       "2  you discuss the power of ai to revolutionize t...    CSPAN2   \n",
       "3  ai bots like chatgpt and google's bard gained ...   BBCNEWS   \n",
       "4  . >> i could sleep ten hours ai night if i was...  FOXNEWSW   \n",
       "\n",
       "   word_complexity  word_complexity_scaled  \\\n",
       "0         0.057407                0.033231   \n",
       "1         0.079768                0.067826   \n",
       "2         0.076151                0.062230   \n",
       "3         0.077301                0.064009   \n",
       "4         0.075782                0.061660   \n",
       "\n",
       "                                           Sentiment Sentiment_Label  \\\n",
       "0  {'label': 'NEGATIVE', 'score': 0.9903525710105...        NEGATIVE   \n",
       "1  {'label': 'POSITIVE', 'score': 0.9933253526687...        POSITIVE   \n",
       "2  {'label': 'NEGATIVE', 'score': 0.9971946477890...        NEGATIVE   \n",
       "3  {'label': 'POSITIVE', 'score': 0.9900817275047...        POSITIVE   \n",
       "4  {'label': 'NEGATIVE', 'score': 0.8428422808647...        NEGATIVE   \n",
       "\n",
       "   Sentiment_Score  Sentiment_scaled  \n",
       "0        -0.990353          0.004713  \n",
       "1         0.993325          0.996727  \n",
       "2        -0.997195          0.001292  \n",
       "3         0.990082          0.995105  \n",
       "4        -0.842842          0.078481  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_sentiment = train[\"Sentiment_Score\"].min()\n",
    "max_sentiment = train[\"Sentiment_Score\"].max()\n",
    "\n",
    "train[\"Sentiment_scaled\"] = (train[\"Sentiment_Score\"] - min_sentiment) / (max_sentiment - min_sentiment)\n",
    "val[\"Sentiment_scaled\"] = (val[\"Sentiment_Score\"] - min_sentiment) / (max_sentiment - min_sentiment)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNNW' 'CNNW' 'CNNW' ... 'CNNW' 'CNNW' 'CNNW']\n",
      "0.07634090161237249\n"
     ]
    }
   ],
   "source": [
    "x_train = train[['word_complexity_scaled', 'Sentiment_scaled']]  # Needs to be 2D\n",
    "x_val = val[['word_complexity_scaled', 'Sentiment_scaled']]\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "val_preds = model.predict(x_val)\n",
    "print(val_preds)\n",
    "\n",
    "# Evaluate\n",
    "val_accuracy = eval(val_preds, y_val)\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "channel\n",
      "CNNW         2725\n",
      "FBC          1608\n",
      "BBCNEWS      1576\n",
      "BLOOMBERG    1441\n",
      "CNBC         1319\n",
      "MSNBCW       1141\n",
      "FOXNEWSW     1106\n",
      "CSPAN         913\n",
      "CSPAN2        904\n",
      "KNTV          842\n",
      "KTVU          766\n",
      "KGO           763\n",
      "KRON          760\n",
      "GBN           717\n",
      "CSPAN3        664\n",
      "KPIX          400\n",
      "SFGTV         360\n",
      "DW            253\n",
      "ALJAZ         230\n",
      "NTV           209\n",
      "KDTV          176\n",
      "1TV           158\n",
      "KSTS          142\n",
      "RUSSIA24      122\n",
      "PRESSTV       115\n",
      "KQED          113\n",
      "BELARUSTV     109\n",
      "RUSSIA1       108\n",
      "RT             50\n",
      "LINKTV         43\n",
      "COM            40\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(train[\"channel\"].value_counts()))\n",
    "print(train[\"channel\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1TV': 0, 'ALJAZ': 1, 'BBCNEWS': 2, 'BELARUSTV': 3, 'BLOOMBERG': 4, 'CNBC': 5, 'CNNW': 6, 'COM': 7, 'CSPAN': 8, 'CSPAN2': 9, 'CSPAN3': 10, 'DW': 11, 'FBC': 12, 'FOXNEWSW': 13, 'GBN': 14, 'KDTV': 15, 'KGO': 16, 'KNTV': 17, 'KPIX': 18, 'KQED': 19, 'KRON': 20, 'KSTS': 21, 'KTVU': 22, 'LINKTV': 23, 'MSNBCW': 24, 'NTV': 25, 'PRESSTV': 26, 'RT': 27, 'RUSSIA1': 28, 'RUSSIA24': 29, 'SFGTV': 30}\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train['channel'])\n",
    "training_labels = torch.tensor(encoder.transform(train['channel']))\n",
    "val_labels = torch.tensor(encoder.transform(val['channel']))\n",
    "\n",
    "class_mapping = dict(zip(encoder.classes_, range(len(encoder.classes_))))\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 2])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "training_data = torch.tensor(train[['word_complexity_scaled', 'Sentiment_scaled']].values)\n",
    "val_data = torch.tensor(val[['word_complexity_scaled', 'Sentiment_scaled']].values)\n",
    "train_tensor = torch.utils.data.TensorDataset(training_data, training_labels)\n",
    "val_tensor = torch.utils.data.TensorDataset(val_data, val_labels)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_tensor, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_tensor, batch_size=batch_size)\n",
    "\n",
    "for X, y in val_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=31, bias=True)\n",
      "  )\n",
      ")\n",
      "Predicted class: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "X = torch.rand(1, 2)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.419219  [   64/19873]\n",
      "loss: 3.365708  [ 6464/19873]\n",
      "loss: 3.298280  [12864/19873]\n",
      "loss: 3.198684  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.282800 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.359068  [   64/19873]\n",
      "loss: 3.266758  [ 6464/19873]\n",
      "loss: 3.176203  [12864/19873]\n",
      "loss: 3.025360  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.195135 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.332412  [   64/19873]\n",
      "loss: 3.202513  [ 6464/19873]\n",
      "loss: 3.096110  [12864/19873]\n",
      "loss: 2.906666  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.138198 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.321265  [   64/19873]\n",
      "loss: 3.162539  [ 6464/19873]\n",
      "loss: 3.049148  [12864/19873]\n",
      "loss: 2.831460  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.097648 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.309592  [   64/19873]\n",
      "loss: 3.132886  [ 6464/19873]\n",
      "loss: 3.021311  [12864/19873]\n",
      "loss: 2.783782  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.068686 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_fxn(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.float()\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.float()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.437797  [   64/19873]\n",
      "loss: 3.448026  [ 6464/19873]\n",
      "loss: 3.438915  [12864/19873]\n",
      "loss: 3.463523  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.9%, Avg loss: 3.419558 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.431638  [   64/19873]\n",
      "loss: 3.432540  [ 6464/19873]\n",
      "loss: 3.427800  [12864/19873]\n",
      "loss: 3.440774  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.9%, Avg loss: 3.415326 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.426932  [   64/19873]\n",
      "loss: 3.420790  [ 6464/19873]\n",
      "loss: 3.418674  [12864/19873]\n",
      "loss: 3.422359  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.8%, Avg loss: 3.412041 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.423030  [   64/19873]\n",
      "loss: 3.411697  [ 6464/19873]\n",
      "loss: 3.411285  [12864/19873]\n",
      "loss: 3.407199  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.0%, Avg loss: 3.409397 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.419549  [   64/19873]\n",
      "loss: 3.404591  [ 6464/19873]\n",
      "loss: 3.405027  [12864/19873]\n",
      "loss: 3.394940  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.2%, Avg loss: 3.407352 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.416288  [   64/19873]\n",
      "loss: 3.399162  [ 6464/19873]\n",
      "loss: 3.399616  [12864/19873]\n",
      "loss: 3.384650  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.8%, Avg loss: 3.405692 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.413256  [   64/19873]\n",
      "loss: 3.394891  [ 6464/19873]\n",
      "loss: 3.394902  [12864/19873]\n",
      "loss: 3.375922  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.5%, Avg loss: 3.404303 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.410249  [   64/19873]\n",
      "loss: 3.391494  [ 6464/19873]\n",
      "loss: 3.390655  [12864/19873]\n",
      "loss: 3.368613  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.3%, Avg loss: 3.403062 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.407370  [   64/19873]\n",
      "loss: 3.388666  [ 6464/19873]\n",
      "loss: 3.386904  [12864/19873]\n",
      "loss: 3.362378  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.5%, Avg loss: 3.401904 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.404592  [   64/19873]\n",
      "loss: 3.386439  [ 6464/19873]\n",
      "loss: 3.383587  [12864/19873]\n",
      "loss: 3.357158  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.5%, Avg loss: 3.400851 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.FloatTensor(weights)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Re-initialize the model\n",
    "model = NeuralNetwork()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# Train the model again with class weights\n",
    "for t in range(10):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing size: 4\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.458644  [   64/19873]\n",
      "loss: 3.527867  [ 6464/19873]\n",
      "loss: 3.445127  [12864/19873]\n",
      "loss: 3.598428  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.458466 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.457662  [   64/19873]\n",
      "loss: 3.525720  [ 6464/19873]\n",
      "loss: 3.444061  [12864/19873]\n",
      "loss: 3.595410  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.457672 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.456709  [   64/19873]\n",
      "loss: 3.523627  [ 6464/19873]\n",
      "loss: 3.443017  [12864/19873]\n",
      "loss: 3.592421  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.456897 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.455788  [   64/19873]\n",
      "loss: 3.521582  [ 6464/19873]\n",
      "loss: 3.441993  [12864/19873]\n",
      "loss: 3.589475  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.456139 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.454893  [   64/19873]\n",
      "loss: 3.519593  [ 6464/19873]\n",
      "loss: 3.440988  [12864/19873]\n",
      "loss: 3.586599  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.455402 \n",
      "\n",
      "Testing size: 8\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.486430  [   64/19873]\n",
      "loss: 3.490359  [ 6464/19873]\n",
      "loss: 3.469829  [12864/19873]\n",
      "loss: 3.418020  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.489648 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.485094  [   64/19873]\n",
      "loss: 3.488075  [ 6464/19873]\n",
      "loss: 3.468391  [12864/19873]\n",
      "loss: 3.416832  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.488316 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.483808  [   64/19873]\n",
      "loss: 3.485874  [ 6464/19873]\n",
      "loss: 3.466993  [12864/19873]\n",
      "loss: 3.415682  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.8%, Avg loss: 3.487029 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.482569  [   64/19873]\n",
      "loss: 3.483753  [ 6464/19873]\n",
      "loss: 3.465632  [12864/19873]\n",
      "loss: 3.414572  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 3.485787 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.481372  [   64/19873]\n",
      "loss: 3.481707  [ 6464/19873]\n",
      "loss: 3.464306  [12864/19873]\n",
      "loss: 3.413496  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.1%, Avg loss: 3.484586 \n",
      "\n",
      "Testing size: 16\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.431023  [   64/19873]\n",
      "loss: 3.440659  [ 6464/19873]\n",
      "loss: 3.424115  [12864/19873]\n",
      "loss: 3.464257  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.426891 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.430449  [   64/19873]\n",
      "loss: 3.438948  [ 6464/19873]\n",
      "loss: 3.423247  [12864/19873]\n",
      "loss: 3.461898  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.426523 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.429921  [   64/19873]\n",
      "loss: 3.437347  [ 6464/19873]\n",
      "loss: 3.422415  [12864/19873]\n",
      "loss: 3.459615  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.426193 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.429434  [   64/19873]\n",
      "loss: 3.435873  [ 6464/19873]\n",
      "loss: 3.421632  [12864/19873]\n",
      "loss: 3.457412  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.425909 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.428995  [   64/19873]\n",
      "loss: 3.434501  [ 6464/19873]\n",
      "loss: 3.420864  [12864/19873]\n",
      "loss: 3.455288  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.425675 \n",
      "\n",
      "Testing size: 32\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.426955  [   64/19873]\n",
      "loss: 3.471366  [ 6464/19873]\n",
      "loss: 3.446016  [12864/19873]\n",
      "loss: 3.450985  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.437333 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.425849  [   64/19873]\n",
      "loss: 3.468748  [ 6464/19873]\n",
      "loss: 3.444219  [12864/19873]\n",
      "loss: 3.449107  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.436750 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.424802  [   64/19873]\n",
      "loss: 3.466256  [ 6464/19873]\n",
      "loss: 3.442524  [12864/19873]\n",
      "loss: 3.447264  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.0%, Avg loss: 3.436212 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.423805  [   64/19873]\n",
      "loss: 3.463827  [ 6464/19873]\n",
      "loss: 3.440642  [12864/19873]\n",
      "loss: 3.445520  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.1%, Avg loss: 3.435974 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.422814  [   64/19873]\n",
      "loss: 3.461501  [ 6464/19873]\n",
      "loss: 3.438880  [12864/19873]\n",
      "loss: 3.444090  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.1%, Avg loss: 3.435854 \n",
      "\n",
      "Testing size: 64\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.422286  [   64/19873]\n",
      "loss: 3.457205  [ 6464/19873]\n",
      "loss: 3.457896  [12864/19873]\n",
      "loss: 3.430914  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.434205 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.422211  [   64/19873]\n",
      "loss: 3.454200  [ 6464/19873]\n",
      "loss: 3.455795  [12864/19873]\n",
      "loss: 3.426773  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.433093 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.422145  [   64/19873]\n",
      "loss: 3.451373  [ 6464/19873]\n",
      "loss: 3.453798  [12864/19873]\n",
      "loss: 3.422827  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.432056 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.422076  [   64/19873]\n",
      "loss: 3.448715  [ 6464/19873]\n",
      "loss: 3.451893  [12864/19873]\n",
      "loss: 3.419099  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.431074 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.422006  [   64/19873]\n",
      "loss: 3.446171  [ 6464/19873]\n",
      "loss: 3.450063  [12864/19873]\n",
      "loss: 3.415655  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.430148 \n",
      "\n",
      "Testing size: 128\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.437363  [   64/19873]\n",
      "loss: 3.441896  [ 6464/19873]\n",
      "loss: 3.432772  [12864/19873]\n",
      "loss: 3.443990  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.2%, Avg loss: 3.438141 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.435461  [   64/19873]\n",
      "loss: 3.438355  [ 6464/19873]\n",
      "loss: 3.430516  [12864/19873]\n",
      "loss: 3.438410  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.0%, Avg loss: 3.435908 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.433696  [   64/19873]\n",
      "loss: 3.435058  [ 6464/19873]\n",
      "loss: 3.428353  [12864/19873]\n",
      "loss: 3.433203  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.3%, Avg loss: 3.433842 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.432061  [   64/19873]\n",
      "loss: 3.432006  [ 6464/19873]\n",
      "loss: 3.426276  [12864/19873]\n",
      "loss: 3.428325  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.4%, Avg loss: 3.431924 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.430496  [   64/19873]\n",
      "loss: 3.429183  [ 6464/19873]\n",
      "loss: 3.424270  [12864/19873]\n",
      "loss: 3.423747  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 3.430127 \n",
      "\n",
      "Testing size: 256\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.417058  [   64/19873]\n",
      "loss: 3.406251  [ 6464/19873]\n",
      "loss: 3.394193  [12864/19873]\n",
      "loss: 3.428434  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.440098 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.415987  [   64/19873]\n",
      "loss: 3.403607  [ 6464/19873]\n",
      "loss: 3.392380  [12864/19873]\n",
      "loss: 3.420142  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.435916 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.414886  [   64/19873]\n",
      "loss: 3.401285  [ 6464/19873]\n",
      "loss: 3.390598  [12864/19873]\n",
      "loss: 3.412721  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.432255 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.413738  [   64/19873]\n",
      "loss: 3.399095  [ 6464/19873]\n",
      "loss: 3.388991  [12864/19873]\n",
      "loss: 3.405995  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.429125 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.412536  [   64/19873]\n",
      "loss: 3.397133  [ 6464/19873]\n",
      "loss: 3.387539  [12864/19873]\n",
      "loss: 3.399880  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.426405 \n",
      "\n",
      "Testing size: 512\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.448421  [   64/19873]\n",
      "loss: 3.421709  [ 6464/19873]\n",
      "loss: 3.434429  [12864/19873]\n",
      "loss: 3.417336  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.7%, Avg loss: 3.423141 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.439457  [   64/19873]\n",
      "loss: 3.413191  [ 6464/19873]\n",
      "loss: 3.425819  [12864/19873]\n",
      "loss: 3.400750  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.5%, Avg loss: 3.418144 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.432093  [   64/19873]\n",
      "loss: 3.406479  [ 6464/19873]\n",
      "loss: 3.418672  [12864/19873]\n",
      "loss: 3.387427  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.9%, Avg loss: 3.414239 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.425885  [   64/19873]\n",
      "loss: 3.401152  [ 6464/19873]\n",
      "loss: 3.412503  [12864/19873]\n",
      "loss: 3.376594  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.5%, Avg loss: 3.411079 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.420619  [   64/19873]\n",
      "loss: 3.396842  [ 6464/19873]\n",
      "loss: 3.407120  [12864/19873]\n",
      "loss: 3.367778  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.4%, Avg loss: 3.408439 \n",
      "\n",
      "Testing size: 1024\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.459437  [   64/19873]\n",
      "loss: 3.456033  [ 6464/19873]\n",
      "loss: 3.422045  [12864/19873]\n",
      "loss: 3.418701  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 3.420739 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.446440  [   64/19873]\n",
      "loss: 3.428461  [ 6464/19873]\n",
      "loss: 3.410248  [12864/19873]\n",
      "loss: 3.393915  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 3.413668 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.437287  [   64/19873]\n",
      "loss: 3.411197  [ 6464/19873]\n",
      "loss: 3.401626  [12864/19873]\n",
      "loss: 3.377142  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 3.408866 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.429521  [   64/19873]\n",
      "loss: 3.400044  [ 6464/19873]\n",
      "loss: 3.394894  [12864/19873]\n",
      "loss: 3.365176  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.5%, Avg loss: 3.405361 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.422414  [   64/19873]\n",
      "loss: 3.392777  [ 6464/19873]\n",
      "loss: 3.389592  [12864/19873]\n",
      "loss: 3.356821  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.2%, Avg loss: 3.402742 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=512):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "sizes_to_test = [4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "for size in sizes_to_test:\n",
    "    print(f\"Testing size: {size}\")\n",
    "    model = NeuralNetwork(size=size)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing learning rate: 0.1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.437389  [   64/19873]\n",
      "loss: 3.407884  [ 6464/19873]\n",
      "loss: 3.334379  [12864/19873]\n",
      "loss: 3.372323  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.9%, Avg loss: 3.352206 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.430663  [   64/19873]\n",
      "loss: 3.397813  [ 6464/19873]\n",
      "loss: 3.313618  [12864/19873]\n",
      "loss: 3.392911  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.366621 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.415348  [   64/19873]\n",
      "loss: 3.387073  [ 6464/19873]\n",
      "loss: 3.298946  [12864/19873]\n",
      "loss: 3.390493  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.370650 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.431707  [   64/19873]\n",
      "loss: 3.395008  [ 6464/19873]\n",
      "loss: 3.319283  [12864/19873]\n",
      "loss: 3.387866  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.356523 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.412568  [   64/19873]\n",
      "loss: 3.405586  [ 6464/19873]\n",
      "loss: 3.339729  [12864/19873]\n",
      "loss: 3.390145  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.354722 \n",
      "\n",
      "Testing learning rate: 0.01\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.453733  [   64/19873]\n",
      "loss: 3.420547  [ 6464/19873]\n",
      "loss: 3.433820  [12864/19873]\n",
      "loss: 3.391527  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.2%, Avg loss: 3.414000 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.423888  [   64/19873]\n",
      "loss: 3.391928  [ 6464/19873]\n",
      "loss: 3.401382  [12864/19873]\n",
      "loss: 3.363069  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.402785 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.405226  [   64/19873]\n",
      "loss: 3.384970  [ 6464/19873]\n",
      "loss: 3.387514  [12864/19873]\n",
      "loss: 3.351761  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.394144 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.389814  [   64/19873]\n",
      "loss: 3.384833  [ 6464/19873]\n",
      "loss: 3.379639  [12864/19873]\n",
      "loss: 3.350237  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.3%, Avg loss: 3.386932 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.377383  [   64/19873]\n",
      "loss: 3.387524  [ 6464/19873]\n",
      "loss: 3.375989  [12864/19873]\n",
      "loss: 3.351687  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.2%, Avg loss: 3.381312 \n",
      "\n",
      "Testing learning rate: 0.001\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.439699  [   64/19873]\n",
      "loss: 3.456103  [ 6464/19873]\n",
      "loss: 3.439999  [12864/19873]\n",
      "loss: 3.443509  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.3%, Avg loss: 3.448109 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.436802  [   64/19873]\n",
      "loss: 3.446575  [ 6464/19873]\n",
      "loss: 3.434359  [12864/19873]\n",
      "loss: 3.432244  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.3%, Avg loss: 3.443216 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.434350  [   64/19873]\n",
      "loss: 3.438743  [ 6464/19873]\n",
      "loss: 3.429384  [12864/19873]\n",
      "loss: 3.422526  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.2%, Avg loss: 3.439084 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.432273  [   64/19873]\n",
      "loss: 3.432093  [ 6464/19873]\n",
      "loss: 3.425160  [12864/19873]\n",
      "loss: 3.414010  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.4%, Avg loss: 3.435523 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.430195  [   64/19873]\n",
      "loss: 3.426266  [ 6464/19873]\n",
      "loss: 3.421412  [12864/19873]\n",
      "loss: 3.406418  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.3%, Avg loss: 3.432454 \n",
      "\n",
      "Testing learning rate: 0.0001\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.431407  [   64/19873]\n",
      "loss: 3.435841  [ 6464/19873]\n",
      "loss: 3.441599  [12864/19873]\n",
      "loss: 3.437104  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.448318 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.431141  [   64/19873]\n",
      "loss: 3.434953  [ 6464/19873]\n",
      "loss: 3.441060  [12864/19873]\n",
      "loss: 3.435732  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.447784 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.430880  [   64/19873]\n",
      "loss: 3.434083  [ 6464/19873]\n",
      "loss: 3.440526  [12864/19873]\n",
      "loss: 3.434383  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.447260 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.430619  [   64/19873]\n",
      "loss: 3.433227  [ 6464/19873]\n",
      "loss: 3.440004  [12864/19873]\n",
      "loss: 3.433058  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.446745 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.430359  [   64/19873]\n",
      "loss: 3.432391  [ 6464/19873]\n",
      "loss: 3.439491  [12864/19873]\n",
      "loss: 3.431756  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.446239 \n",
      "\n",
      "Testing learning rate: 1e-05\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.453931  [   64/19873]\n",
      "loss: 3.469440  [ 6464/19873]\n",
      "loss: 3.445604  [12864/19873]\n",
      "loss: 3.448429  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465378 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.453874  [   64/19873]\n",
      "loss: 3.469347  [ 6464/19873]\n",
      "loss: 3.445537  [12864/19873]\n",
      "loss: 3.448304  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465315 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.453818  [   64/19873]\n",
      "loss: 3.469254  [ 6464/19873]\n",
      "loss: 3.445469  [12864/19873]\n",
      "loss: 3.448179  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465253 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.453762  [   64/19873]\n",
      "loss: 3.469161  [ 6464/19873]\n",
      "loss: 3.445403  [12864/19873]\n",
      "loss: 3.448053  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465190 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.453706  [   64/19873]\n",
      "loss: 3.469069  [ 6464/19873]\n",
      "loss: 3.445335  [12864/19873]\n",
      "loss: 3.447929  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465128 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_to_test = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "for lr in lr_to_test:\n",
    "    print(f\"Testing learning rate: {lr}\")\n",
    "    model = NeuralNetwork(size=256)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.453650  [   64/19873]\n",
      "loss: 3.468778  [ 6464/19873]\n",
      "loss: 3.445128  [12864/19873]\n",
      "loss: 3.446953  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.464507 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.453095  [   64/19873]\n",
      "loss: 3.467870  [ 6464/19873]\n",
      "loss: 3.444466  [12864/19873]\n",
      "loss: 3.445726  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.463897 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.452548  [   64/19873]\n",
      "loss: 3.466980  [ 6464/19873]\n",
      "loss: 3.443809  [12864/19873]\n",
      "loss: 3.444516  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.463295 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.452004  [   64/19873]\n",
      "loss: 3.466105  [ 6464/19873]\n",
      "loss: 3.443157  [12864/19873]\n",
      "loss: 3.443324  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.462704 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.451467  [   64/19873]\n",
      "loss: 3.465248  [ 6464/19873]\n",
      "loss: 3.442511  [12864/19873]\n",
      "loss: 3.442147  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.462119 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=256):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "for t in range(5):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5264889766370516\n"
     ]
    }
   ],
   "source": [
    "# Use TF-IDF vectors as features\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(train['snip'])\n",
    "X_val_tfidf = tfidf.transform(val['snip'])\n",
    "\n",
    "# Train a model on these features\n",
    "model = LogisticRegression(C=1, max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "val_preds = model.predict(X_val_tfidf)\n",
    "accuracy = np.mean(val_preds == y_val)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Features Accuracy: 0.5330700888450148\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "x_train_cv = vectorizer.fit_transform(train['snip'])\n",
    "x_val_cv = vectorizer.transform(val['snip'])\n",
    "\n",
    "x_train = [X_train_tfidf, x_train_cv]\n",
    "x_val = [X_val_tfidf, x_val_cv]\n",
    "# Combine TF-IDF and CountVectorizer features\n",
    "from scipy.sparse import hstack\n",
    "x_train_combined = hstack(x_train)\n",
    "x_val_combined = hstack(x_val)\n",
    "# Train a model on the combined features\n",
    "model = LogisticRegression(C=1, max_iter=1000, class_weight='balanced')\n",
    "model.fit(x_train_combined, y_train)\n",
    "val_preds = model.predict(x_val_combined)\n",
    "accuracy = np.mean(val_preds == y_val)\n",
    "print(f\"Combined Features Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 71636])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "training_data = torch.tensor(x_train_combined.toarray(), dtype=torch.float32)\n",
    "val_data = torch.tensor(x_val_combined.toarray(), dtype=torch.float32)\n",
    "train_tensor = torch.utils.data.TensorDataset(training_data, training_labels)\n",
    "val_tensor = torch.utils.data.TensorDataset(val_data, val_labels)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_tensor, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_tensor, batch_size=batch_size)\n",
    "\n",
    "for X, y in val_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=71636, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=31, bias=True)\n",
      "  )\n",
      ")\n",
      "Predicted class: tensor([7])\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(71636, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "X = torch.rand(1, 71636)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.434559  [   64/19873]\n",
      "loss: 3.428117  [ 6464/19873]\n",
      "loss: 3.405618  [12864/19873]\n",
      "loss: 3.373003  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 3.389523 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.408284  [   64/19873]\n",
      "loss: 3.376514  [ 6464/19873]\n",
      "loss: 3.322723  [12864/19873]\n",
      "loss: 3.241123  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.308714 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.367684  [   64/19873]\n",
      "loss: 3.293944  [ 6464/19873]\n",
      "loss: 3.182109  [12864/19873]\n",
      "loss: 3.027426  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.195545 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.328003  [   64/19873]\n",
      "loss: 3.206158  [ 6464/19873]\n",
      "loss: 3.045216  [12864/19873]\n",
      "loss: 2.853983  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.114789 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.313701  [   64/19873]\n",
      "loss: 3.155625  [ 6464/19873]\n",
      "loss: 2.972900  [12864/19873]\n",
      "loss: 2.765722  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.059111 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_fxn(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.float()\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.float()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.430328  [   64/19873]\n",
      "loss: 3.428810  [ 6464/19873]\n",
      "loss: 3.422170  [12864/19873]\n",
      "loss: 3.432064  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.5%, Avg loss: 3.428553 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.418210  [   64/19873]\n",
      "loss: 3.420495  [ 6464/19873]\n",
      "loss: 3.403784  [12864/19873]\n",
      "loss: 3.422399  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.2%, Avg loss: 3.419557 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.405757  [   64/19873]\n",
      "loss: 3.411347  [ 6464/19873]\n",
      "loss: 3.384280  [12864/19873]\n",
      "loss: 3.411191  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.1%, Avg loss: 3.409535 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.391715  [   64/19873]\n",
      "loss: 3.401117  [ 6464/19873]\n",
      "loss: 3.361994  [12864/19873]\n",
      "loss: 3.398981  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.9%, Avg loss: 3.398097 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.375106  [   64/19873]\n",
      "loss: 3.389582  [ 6464/19873]\n",
      "loss: 3.334901  [12864/19873]\n",
      "loss: 3.386453  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.3%, Avg loss: 3.385687 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.355259  [   64/19873]\n",
      "loss: 3.377243  [ 6464/19873]\n",
      "loss: 3.302362  [12864/19873]\n",
      "loss: 3.372976  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.5%, Avg loss: 3.371131 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.331575  [   64/19873]\n",
      "loss: 3.363435  [ 6464/19873]\n",
      "loss: 3.262287  [12864/19873]\n",
      "loss: 3.358761  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg loss: 3.353942 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.303061  [   64/19873]\n",
      "loss: 3.348527  [ 6464/19873]\n",
      "loss: 3.212983  [12864/19873]\n",
      "loss: 3.344238  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.1%, Avg loss: 3.334226 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.268876  [   64/19873]\n",
      "loss: 3.333044  [ 6464/19873]\n",
      "loss: 3.151452  [12864/19873]\n",
      "loss: 3.330046  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.0%, Avg loss: 3.311452 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.227689  [   64/19873]\n",
      "loss: 3.317555  [ 6464/19873]\n",
      "loss: 3.074322  [12864/19873]\n",
      "loss: 3.316724  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.6%, Avg loss: 3.284940 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.FloatTensor(weights)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Re-initialize the model\n",
    "model = NeuralNetwork()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# Train the model again with class weights\n",
    "for t in range(10):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.437139  [   64/19873]\n",
      "loss: 3.428693  [ 6464/19873]\n",
      "loss: 3.420851  [12864/19873]\n",
      "loss: 3.429789  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.2%, Avg loss: 3.426797 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.426727  [   64/19873]\n",
      "loss: 3.420564  [ 6464/19873]\n",
      "loss: 3.401973  [12864/19873]\n",
      "loss: 3.420819  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.0%, Avg loss: 3.419440 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.415125  [   64/19873]\n",
      "loss: 3.411568  [ 6464/19873]\n",
      "loss: 3.381195  [12864/19873]\n",
      "loss: 3.411841  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.6%, Avg loss: 3.411292 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.401210  [   64/19873]\n",
      "loss: 3.402690  [ 6464/19873]\n",
      "loss: 3.357353  [12864/19873]\n",
      "loss: 3.402523  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.0%, Avg loss: 3.401492 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.384081  [   64/19873]\n",
      "loss: 3.392769  [ 6464/19873]\n",
      "loss: 3.327887  [12864/19873]\n",
      "loss: 3.393290  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.6%, Avg loss: 3.389996 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.362796  [   64/19873]\n",
      "loss: 3.381441  [ 6464/19873]\n",
      "loss: 3.290392  [12864/19873]\n",
      "loss: 3.383351  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.8%, Avg loss: 3.376354 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.336525  [   64/19873]\n",
      "loss: 3.368424  [ 6464/19873]\n",
      "loss: 3.243084  [12864/19873]\n",
      "loss: 3.372513  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.2%, Avg loss: 3.359725 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.304143  [   64/19873]\n",
      "loss: 3.354419  [ 6464/19873]\n",
      "loss: 3.182530  [12864/19873]\n",
      "loss: 3.361187  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.2%, Avg loss: 3.339284 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.263969  [   64/19873]\n",
      "loss: 3.339501  [ 6464/19873]\n",
      "loss: 3.105270  [12864/19873]\n",
      "loss: 3.349694  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 3.314202 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.215314  [   64/19873]\n",
      "loss: 3.323749  [ 6464/19873]\n",
      "loss: 3.008626  [12864/19873]\n",
      "loss: 3.337867  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 3.284272 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 3.160035  [   64/19873]\n",
      "loss: 3.308221  [ 6464/19873]\n",
      "loss: 2.895859  [12864/19873]\n",
      "loss: 3.325834  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg loss: 3.251581 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 3.104494  [   64/19873]\n",
      "loss: 3.292898  [ 6464/19873]\n",
      "loss: 2.780222  [12864/19873]\n",
      "loss: 3.314020  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.2%, Avg loss: 3.220324 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 3.057972  [   64/19873]\n",
      "loss: 3.277612  [ 6464/19873]\n",
      "loss: 2.681134  [12864/19873]\n",
      "loss: 3.302654  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.9%, Avg loss: 3.193912 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 3.023967  [   64/19873]\n",
      "loss: 3.262586  [ 6464/19873]\n",
      "loss: 2.607747  [12864/19873]\n",
      "loss: 3.291786  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.9%, Avg loss: 3.172638 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.999267  [   64/19873]\n",
      "loss: 3.247920  [ 6464/19873]\n",
      "loss: 2.556454  [12864/19873]\n",
      "loss: 3.281443  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.6%, Avg loss: 3.155161 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.980234  [   64/19873]\n",
      "loss: 3.233587  [ 6464/19873]\n",
      "loss: 2.519806  [12864/19873]\n",
      "loss: 3.272045  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.5%, Avg loss: 3.140021 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.964109  [   64/19873]\n",
      "loss: 3.219416  [ 6464/19873]\n",
      "loss: 2.492136  [12864/19873]\n",
      "loss: 3.263194  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.0%, Avg loss: 3.126306 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.949305  [   64/19873]\n",
      "loss: 3.204959  [ 6464/19873]\n",
      "loss: 2.469692  [12864/19873]\n",
      "loss: 3.254375  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.7%, Avg loss: 3.113282 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.935094  [   64/19873]\n",
      "loss: 3.189862  [ 6464/19873]\n",
      "loss: 2.450305  [12864/19873]\n",
      "loss: 3.245846  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.9%, Avg loss: 3.100531 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.920880  [   64/19873]\n",
      "loss: 3.173835  [ 6464/19873]\n",
      "loss: 2.432770  [12864/19873]\n",
      "loss: 3.237242  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.8%, Avg loss: 3.087637 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(71636, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "part_one_model = NeuralNetwork()\n",
    "optimizer = torch.optim.SGD(part_one_model.parameters(), lr=1e-3)\n",
    "part_one_accuracies = []\n",
    "part_one_losses = []\n",
    "for e in range(20):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, part_one_model, loss_fn, optimizer)\n",
    "    c, l = test(val_dataloader, part_one_model, loss_fn)\n",
    "    part_one_accuracies.append(c)\n",
    "    part_one_losses.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel 0 has no samples in validation set.\n",
      "Channel 1 Accuracy: 10.3448\n",
      "Channel 2 Accuracy: 0.0000\n",
      "Channel 3 has no samples in validation set.\n",
      "Channel 4 Accuracy: 31.1765\n",
      "Channel 5 Accuracy: 16.4062\n",
      "Channel 6 Accuracy: 34.7032\n",
      "Channel 7 has no samples in validation set.\n",
      "Channel 8 Accuracy: 5.3922\n",
      "Channel 9 Accuracy: 69.0909\n",
      "Channel 10 Accuracy: 0.0000\n",
      "Channel 11 Accuracy: 0.0000\n",
      "Channel 12 Accuracy: 0.0000\n",
      "Channel 13 Accuracy: 0.0000\n",
      "Channel 14 Accuracy: 21.4286\n",
      "Channel 15 Accuracy: 96.8750\n",
      "Channel 16 Accuracy: 0.0000\n",
      "Channel 17 Accuracy: 1.0811\n",
      "Channel 18 Accuracy: 0.0000\n",
      "Channel 19 Accuracy: 0.0000\n",
      "Channel 20 Accuracy: 52.3179\n",
      "Channel 21 Accuracy: 9.0909\n",
      "Channel 22 Accuracy: 5.2174\n",
      "Channel 23 has no samples in validation set.\n",
      "Channel 24 Accuracy: 8.0717\n",
      "Channel 25 has no samples in validation set.\n",
      "Channel 26 has no samples in validation set.\n",
      "Channel 27 Accuracy: 0.0000\n",
      "Channel 28 has no samples in validation set.\n",
      "Channel 29 has no samples in validation set.\n",
      "Channel 30 Accuracy: 29.7297\n",
      "0.14840408028956895\n"
     ]
    }
   ],
   "source": [
    "num_batches = len(val_dataloader)\n",
    "part_one_model.eval()\n",
    "correct = np.zeros(31)\n",
    "sizes = np.zeros(31)\n",
    "with torch.no_grad():\n",
    "    for X, y in val_dataloader:\n",
    "        X = X.float()\n",
    "        pred = part_one_model(X)\n",
    "        for yi in range(31):\n",
    "            for i in range(len(y)):\n",
    "                if y[i] == yi:\n",
    "                    if pred[i].argmax(0) == y[i]:\n",
    "                        correct[yi] += 1\n",
    "                    sizes[yi] += 1\n",
    "\n",
    "for yi in range(31):\n",
    "    if sizes[yi] > 0:\n",
    "        accuracy = correct[yi] / sizes[yi]\n",
    "        print(f\"Channel {yi} Accuracy: {accuracy*100:.4f}\")\n",
    "    else:\n",
    "        print(f\"Channel {yi} has no samples in validation set.\")\n",
    "\n",
    "print(sum(correct) / sum(sizes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing size: 4\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.402915  [   64/19873]\n",
      "loss: 3.432564  [ 6464/19873]\n",
      "loss: 3.543827  [12864/19873]\n",
      "loss: 3.447044  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.451771 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.396417  [   64/19873]\n",
      "loss: 3.428012  [ 6464/19873]\n",
      "loss: 3.540450  [12864/19873]\n",
      "loss: 3.440862  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.451913 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.394105  [   64/19873]\n",
      "loss: 3.425907  [ 6464/19873]\n",
      "loss: 3.539143  [12864/19873]\n",
      "loss: 3.435375  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.453221 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.395595  [   64/19873]\n",
      "loss: 3.423532  [ 6464/19873]\n",
      "loss: 3.537412  [12864/19873]\n",
      "loss: 3.431342  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.453774 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.397021  [   64/19873]\n",
      "loss: 3.421161  [ 6464/19873]\n",
      "loss: 3.534748  [12864/19873]\n",
      "loss: 3.427881  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.453381 \n",
      "\n",
      "Testing size: 8\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.479362  [   64/19873]\n",
      "loss: 3.395927  [ 6464/19873]\n",
      "loss: 3.523820  [12864/19873]\n",
      "loss: 3.481232  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.2%, Avg loss: 3.446070 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.479168  [   64/19873]\n",
      "loss: 3.391809  [ 6464/19873]\n",
      "loss: 3.520635  [12864/19873]\n",
      "loss: 3.477242  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.2%, Avg loss: 3.443989 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.475273  [   64/19873]\n",
      "loss: 3.386813  [ 6464/19873]\n",
      "loss: 3.513307  [12864/19873]\n",
      "loss: 3.473772  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.2%, Avg loss: 3.442015 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.469245  [   64/19873]\n",
      "loss: 3.381234  [ 6464/19873]\n",
      "loss: 3.507710  [12864/19873]\n",
      "loss: 3.471232  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.2%, Avg loss: 3.439584 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.464604  [   64/19873]\n",
      "loss: 3.375283  [ 6464/19873]\n",
      "loss: 3.503025  [12864/19873]\n",
      "loss: 3.468647  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.2%, Avg loss: 3.437233 \n",
      "\n",
      "Testing size: 16\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.463140  [   64/19873]\n",
      "loss: 3.396588  [ 6464/19873]\n",
      "loss: 3.449048  [12864/19873]\n",
      "loss: 3.440449  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.431759 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.452724  [   64/19873]\n",
      "loss: 3.394173  [ 6464/19873]\n",
      "loss: 3.440188  [12864/19873]\n",
      "loss: 3.435639  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.428462 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.446311  [   64/19873]\n",
      "loss: 3.391269  [ 6464/19873]\n",
      "loss: 3.431220  [12864/19873]\n",
      "loss: 3.431103  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.425416 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.440876  [   64/19873]\n",
      "loss: 3.384218  [ 6464/19873]\n",
      "loss: 3.422391  [12864/19873]\n",
      "loss: 3.427737  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.5%, Avg loss: 3.424179 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.435932  [   64/19873]\n",
      "loss: 3.375902  [ 6464/19873]\n",
      "loss: 3.411589  [12864/19873]\n",
      "loss: 3.424763  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.9%, Avg loss: 3.420668 \n",
      "\n",
      "Testing size: 32\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.417110  [   64/19873]\n",
      "loss: 3.427480  [ 6464/19873]\n",
      "loss: 3.369092  [12864/19873]\n",
      "loss: 3.389271  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.8%, Avg loss: 3.419802 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.404366  [   64/19873]\n",
      "loss: 3.414600  [ 6464/19873]\n",
      "loss: 3.358005  [12864/19873]\n",
      "loss: 3.383717  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.9%, Avg loss: 3.411963 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.396748  [   64/19873]\n",
      "loss: 3.406552  [ 6464/19873]\n",
      "loss: 3.347377  [12864/19873]\n",
      "loss: 3.380816  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.8%, Avg loss: 3.405378 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.389344  [   64/19873]\n",
      "loss: 3.400475  [ 6464/19873]\n",
      "loss: 3.332312  [12864/19873]\n",
      "loss: 3.377691  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.398625 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.380871  [   64/19873]\n",
      "loss: 3.395662  [ 6464/19873]\n",
      "loss: 3.315721  [12864/19873]\n",
      "loss: 3.374271  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.6%, Avg loss: 3.390683 \n",
      "\n",
      "Testing size: 64\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.418517  [   64/19873]\n",
      "loss: 3.415454  [ 6464/19873]\n",
      "loss: 3.432164  [12864/19873]\n",
      "loss: 3.451925  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.8%, Avg loss: 3.426432 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.406725  [   64/19873]\n",
      "loss: 3.405242  [ 6464/19873]\n",
      "loss: 3.414254  [12864/19873]\n",
      "loss: 3.438085  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.5%, Avg loss: 3.418209 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.397415  [   64/19873]\n",
      "loss: 3.396772  [ 6464/19873]\n",
      "loss: 3.397720  [12864/19873]\n",
      "loss: 3.425625  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.410474 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.388530  [   64/19873]\n",
      "loss: 3.388748  [ 6464/19873]\n",
      "loss: 3.382231  [12864/19873]\n",
      "loss: 3.414591  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.9%, Avg loss: 3.403064 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.379233  [   64/19873]\n",
      "loss: 3.380889  [ 6464/19873]\n",
      "loss: 3.364755  [12864/19873]\n",
      "loss: 3.404135  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.5%, Avg loss: 3.395028 \n",
      "\n",
      "Testing size: 128\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.445489  [   64/19873]\n",
      "loss: 3.439589  [ 6464/19873]\n",
      "loss: 3.412845  [12864/19873]\n",
      "loss: 3.423648  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.3%, Avg loss: 3.429330 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.433228  [   64/19873]\n",
      "loss: 3.437489  [ 6464/19873]\n",
      "loss: 3.400120  [12864/19873]\n",
      "loss: 3.419584  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.0%, Avg loss: 3.421475 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.421005  [   64/19873]\n",
      "loss: 3.434302  [ 6464/19873]\n",
      "loss: 3.388674  [12864/19873]\n",
      "loss: 3.413754  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 3.412535 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.406834  [   64/19873]\n",
      "loss: 3.429973  [ 6464/19873]\n",
      "loss: 3.372707  [12864/19873]\n",
      "loss: 3.406639  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.5%, Avg loss: 3.402120 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.388471  [   64/19873]\n",
      "loss: 3.422845  [ 6464/19873]\n",
      "loss: 3.351058  [12864/19873]\n",
      "loss: 3.397968  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 3.389856 \n",
      "\n",
      "Testing size: 256\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.442314  [   64/19873]\n",
      "loss: 3.412101  [ 6464/19873]\n",
      "loss: 3.414319  [12864/19873]\n",
      "loss: 3.423735  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.9%, Avg loss: 3.423108 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.427003  [   64/19873]\n",
      "loss: 3.404778  [ 6464/19873]\n",
      "loss: 3.397000  [12864/19873]\n",
      "loss: 3.417403  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.9%, Avg loss: 3.415500 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.413430  [   64/19873]\n",
      "loss: 3.397776  [ 6464/19873]\n",
      "loss: 3.377228  [12864/19873]\n",
      "loss: 3.410630  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.5%, Avg loss: 3.406676 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.398846  [   64/19873]\n",
      "loss: 3.389768  [ 6464/19873]\n",
      "loss: 3.353375  [12864/19873]\n",
      "loss: 3.401215  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 3.396121 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.380884  [   64/19873]\n",
      "loss: 3.380497  [ 6464/19873]\n",
      "loss: 3.322924  [12864/19873]\n",
      "loss: 3.390501  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.5%, Avg loss: 3.384108 \n",
      "\n",
      "Testing size: 512\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.434092  [   64/19873]\n",
      "loss: 3.426777  [ 6464/19873]\n",
      "loss: 3.419371  [12864/19873]\n",
      "loss: 3.423434  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.4%, Avg loss: 3.424094 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.420086  [   64/19873]\n",
      "loss: 3.418455  [ 6464/19873]\n",
      "loss: 3.400634  [12864/19873]\n",
      "loss: 3.416853  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.0%, Avg loss: 3.413567 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.405901  [   64/19873]\n",
      "loss: 3.409181  [ 6464/19873]\n",
      "loss: 3.379846  [12864/19873]\n",
      "loss: 3.410385  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.2%, Avg loss: 3.402301 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.390208  [   64/19873]\n",
      "loss: 3.399818  [ 6464/19873]\n",
      "loss: 3.355627  [12864/19873]\n",
      "loss: 3.403380  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.3%, Avg loss: 3.389798 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.371686  [   64/19873]\n",
      "loss: 3.389231  [ 6464/19873]\n",
      "loss: 3.326609  [12864/19873]\n",
      "loss: 3.395491  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.3%, Avg loss: 3.375581 \n",
      "\n",
      "Testing size: 1024\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.427153  [   64/19873]\n",
      "loss: 3.430657  [ 6464/19873]\n",
      "loss: 3.423244  [12864/19873]\n",
      "loss: 3.430078  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.428236 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.414685  [   64/19873]\n",
      "loss: 3.422326  [ 6464/19873]\n",
      "loss: 3.405539  [12864/19873]\n",
      "loss: 3.421941  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.5%, Avg loss: 3.421018 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.401618  [   64/19873]\n",
      "loss: 3.413295  [ 6464/19873]\n",
      "loss: 3.386519  [12864/19873]\n",
      "loss: 3.414127  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.4%, Avg loss: 3.413140 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.387065  [   64/19873]\n",
      "loss: 3.403687  [ 6464/19873]\n",
      "loss: 3.364489  [12864/19873]\n",
      "loss: 3.405830  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.4%, Avg loss: 3.403964 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.370663  [   64/19873]\n",
      "loss: 3.393227  [ 6464/19873]\n",
      "loss: 3.337797  [12864/19873]\n",
      "loss: 3.396659  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.4%, Avg loss: 3.392992 \n",
      "\n",
      "Testing size: 2048\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.431548  [   64/19873]\n",
      "loss: 3.431872  [ 6464/19873]\n",
      "loss: 3.424892  [12864/19873]\n",
      "loss: 3.428817  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 3.422011 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.420356  [   64/19873]\n",
      "loss: 3.423877  [ 6464/19873]\n",
      "loss: 3.407967  [12864/19873]\n",
      "loss: 3.421660  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.7%, Avg loss: 3.413660 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.408260  [   64/19873]\n",
      "loss: 3.415294  [ 6464/19873]\n",
      "loss: 3.388644  [12864/19873]\n",
      "loss: 3.413869  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 3.404573 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.394279  [   64/19873]\n",
      "loss: 3.405428  [ 6464/19873]\n",
      "loss: 3.365691  [12864/19873]\n",
      "loss: 3.404993  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg loss: 3.394303 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.378091  [   64/19873]\n",
      "loss: 3.393972  [ 6464/19873]\n",
      "loss: 3.338054  [12864/19873]\n",
      "loss: 3.394866  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.6%, Avg loss: 3.382417 \n",
      "\n",
      "Testing size: 4096\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.431428  [   64/19873]\n",
      "loss: 3.434120  [ 6464/19873]\n",
      "loss: 3.422175  [12864/19873]\n",
      "loss: 3.427072  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.5%, Avg loss: 3.426499 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.419234  [   64/19873]\n",
      "loss: 3.425740  [ 6464/19873]\n",
      "loss: 3.403899  [12864/19873]\n",
      "loss: 3.419055  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg loss: 3.418358 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.406377  [   64/19873]\n",
      "loss: 3.417174  [ 6464/19873]\n",
      "loss: 3.383790  [12864/19873]\n",
      "loss: 3.410534  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.4%, Avg loss: 3.409494 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.391922  [   64/19873]\n",
      "loss: 3.407772  [ 6464/19873]\n",
      "loss: 3.360447  [12864/19873]\n",
      "loss: 3.401273  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.3%, Avg loss: 3.399295 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.375054  [   64/19873]\n",
      "loss: 3.397227  [ 6464/19873]\n",
      "loss: 3.332397  [12864/19873]\n",
      "loss: 3.391159  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.1%, Avg loss: 3.387359 \n",
      "\n",
      "Testing size: 8192\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.433052  [   64/19873]\n",
      "loss: 3.430113  [ 6464/19873]\n",
      "loss: 3.424755  [12864/19873]\n",
      "loss: 3.423829  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.7%, Avg loss: 3.425800 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.419499  [   64/19873]\n",
      "loss: 3.421037  [ 6464/19873]\n",
      "loss: 3.406061  [12864/19873]\n",
      "loss: 3.415708  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.8%, Avg loss: 3.417444 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.405203  [   64/19873]\n",
      "loss: 3.411750  [ 6464/19873]\n",
      "loss: 3.385007  [12864/19873]\n",
      "loss: 3.407087  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.9%, Avg loss: 3.408262 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.389159  [   64/19873]\n",
      "loss: 3.401875  [ 6464/19873]\n",
      "loss: 3.360141  [12864/19873]\n",
      "loss: 3.397648  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.8%, Avg loss: 3.397748 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.370291  [   64/19873]\n",
      "loss: 3.391054  [ 6464/19873]\n",
      "loss: 3.329724  [12864/19873]\n",
      "loss: 3.387196  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.0%, Avg loss: 3.385425 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=512):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(71636, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "sizes_to_test = [4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "for size in sizes_to_test:\n",
    "    print(f\"Testing size: {size}\")\n",
    "    model = NeuralNetwork(size=size)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing size: 4\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.408898  [   64/19873]\n",
      "loss: 3.566106  [ 6464/19873]\n",
      "loss: 3.425845  [12864/19873]\n",
      "loss: 3.469388  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.413628 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.404488  [   64/19873]\n",
      "loss: 3.562757  [ 6464/19873]\n",
      "loss: 3.423323  [12864/19873]\n",
      "loss: 3.466210  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.410467 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.403158  [   64/19873]\n",
      "loss: 3.558125  [ 6464/19873]\n",
      "loss: 3.420927  [12864/19873]\n",
      "loss: 3.464231  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.406890 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.401541  [   64/19873]\n",
      "loss: 3.554965  [ 6464/19873]\n",
      "loss: 3.418216  [12864/19873]\n",
      "loss: 3.462960  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.402931 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.399511  [   64/19873]\n",
      "loss: 3.552695  [ 6464/19873]\n",
      "loss: 3.415090  [12864/19873]\n",
      "loss: 3.462414  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.398773 \n",
      "\n",
      "Testing size: 8\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.421062  [   64/19873]\n",
      "loss: 3.489963  [ 6464/19873]\n",
      "loss: 3.464026  [12864/19873]\n",
      "loss: 3.449370  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.459817 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.420954  [   64/19873]\n",
      "loss: 3.485709  [ 6464/19873]\n",
      "loss: 3.458102  [12864/19873]\n",
      "loss: 3.442622  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.457910 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.421549  [   64/19873]\n",
      "loss: 3.483339  [ 6464/19873]\n",
      "loss: 3.454328  [12864/19873]\n",
      "loss: 3.437456  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.456362 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.421681  [   64/19873]\n",
      "loss: 3.481622  [ 6464/19873]\n",
      "loss: 3.451307  [12864/19873]\n",
      "loss: 3.434201  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.455880 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.421688  [   64/19873]\n",
      "loss: 3.479647  [ 6464/19873]\n",
      "loss: 3.448720  [12864/19873]\n",
      "loss: 3.431637  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.455862 \n",
      "\n",
      "Testing size: 16\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.456981  [   64/19873]\n",
      "loss: 3.451030  [ 6464/19873]\n",
      "loss: 3.466917  [12864/19873]\n",
      "loss: 3.481651  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.7%, Avg loss: 3.435068 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.454716  [   64/19873]\n",
      "loss: 3.450223  [ 6464/19873]\n",
      "loss: 3.463263  [12864/19873]\n",
      "loss: 3.477715  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.7%, Avg loss: 3.432694 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.454301  [   64/19873]\n",
      "loss: 3.448397  [ 6464/19873]\n",
      "loss: 3.459429  [12864/19873]\n",
      "loss: 3.474401  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.9%, Avg loss: 3.430008 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.454433  [   64/19873]\n",
      "loss: 3.447344  [ 6464/19873]\n",
      "loss: 3.455362  [12864/19873]\n",
      "loss: 3.471404  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.3%, Avg loss: 3.427604 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.454349  [   64/19873]\n",
      "loss: 3.446252  [ 6464/19873]\n",
      "loss: 3.452193  [12864/19873]\n",
      "loss: 3.468578  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.8%, Avg loss: 3.425031 \n",
      "\n",
      "Testing size: 32\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.429279  [   64/19873]\n",
      "loss: 3.424176  [ 6464/19873]\n",
      "loss: 3.446534  [12864/19873]\n",
      "loss: 3.440797  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.6%, Avg loss: 3.411859 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.425877  [   64/19873]\n",
      "loss: 3.422336  [ 6464/19873]\n",
      "loss: 3.441524  [12864/19873]\n",
      "loss: 3.437520  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.6%, Avg loss: 3.410935 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.422870  [   64/19873]\n",
      "loss: 3.420912  [ 6464/19873]\n",
      "loss: 3.436922  [12864/19873]\n",
      "loss: 3.434862  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.6%, Avg loss: 3.409955 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.420891  [   64/19873]\n",
      "loss: 3.419730  [ 6464/19873]\n",
      "loss: 3.432899  [12864/19873]\n",
      "loss: 3.432429  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.6%, Avg loss: 3.409148 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.419072  [   64/19873]\n",
      "loss: 3.418516  [ 6464/19873]\n",
      "loss: 3.429073  [12864/19873]\n",
      "loss: 3.430173  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.6%, Avg loss: 3.408359 \n",
      "\n",
      "Testing size: 64\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.436248  [   64/19873]\n",
      "loss: 3.442492  [ 6464/19873]\n",
      "loss: 3.438823  [12864/19873]\n",
      "loss: 3.445508  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.0%, Avg loss: 3.444825 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.433841  [   64/19873]\n",
      "loss: 3.440529  [ 6464/19873]\n",
      "loss: 3.434588  [12864/19873]\n",
      "loss: 3.442109  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.0%, Avg loss: 3.442734 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.430689  [   64/19873]\n",
      "loss: 3.438476  [ 6464/19873]\n",
      "loss: 3.429025  [12864/19873]\n",
      "loss: 3.438211  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.1%, Avg loss: 3.440635 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.427373  [   64/19873]\n",
      "loss: 3.436514  [ 6464/19873]\n",
      "loss: 3.423955  [12864/19873]\n",
      "loss: 3.434780  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.0%, Avg loss: 3.438716 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.424087  [   64/19873]\n",
      "loss: 3.435175  [ 6464/19873]\n",
      "loss: 3.418997  [12864/19873]\n",
      "loss: 3.431994  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.8%, Avg loss: 3.437107 \n",
      "\n",
      "Testing size: 128\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.430741  [   64/19873]\n",
      "loss: 3.426291  [ 6464/19873]\n",
      "loss: 3.433274  [12864/19873]\n",
      "loss: 3.448116  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.441710 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.429105  [   64/19873]\n",
      "loss: 3.424124  [ 6464/19873]\n",
      "loss: 3.429306  [12864/19873]\n",
      "loss: 3.445324  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.439131 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.427411  [   64/19873]\n",
      "loss: 3.422170  [ 6464/19873]\n",
      "loss: 3.425340  [12864/19873]\n",
      "loss: 3.441891  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.1%, Avg loss: 3.436574 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.425795  [   64/19873]\n",
      "loss: 3.419995  [ 6464/19873]\n",
      "loss: 3.421576  [12864/19873]\n",
      "loss: 3.438893  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.5%, Avg loss: 3.434252 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.424201  [   64/19873]\n",
      "loss: 3.417680  [ 6464/19873]\n",
      "loss: 3.417642  [12864/19873]\n",
      "loss: 3.435913  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.6%, Avg loss: 3.431896 \n",
      "\n",
      "Testing size: 256\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.438660  [   64/19873]\n",
      "loss: 3.441538  [ 6464/19873]\n",
      "loss: 3.445272  [12864/19873]\n",
      "loss: 3.441363  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.438838 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.436474  [   64/19873]\n",
      "loss: 3.439518  [ 6464/19873]\n",
      "loss: 3.441510  [12864/19873]\n",
      "loss: 3.438514  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.6%, Avg loss: 3.437386 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.434349  [   64/19873]\n",
      "loss: 3.437373  [ 6464/19873]\n",
      "loss: 3.437722  [12864/19873]\n",
      "loss: 3.435447  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.6%, Avg loss: 3.435870 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.432207  [   64/19873]\n",
      "loss: 3.435027  [ 6464/19873]\n",
      "loss: 3.434176  [12864/19873]\n",
      "loss: 3.432465  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.6%, Avg loss: 3.434409 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.430052  [   64/19873]\n",
      "loss: 3.432590  [ 6464/19873]\n",
      "loss: 3.430792  [12864/19873]\n",
      "loss: 3.429618  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.8%, Avg loss: 3.432976 \n",
      "\n",
      "Testing size: 512\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.437609  [   64/19873]\n",
      "loss: 3.429032  [ 6464/19873]\n",
      "loss: 3.439580  [12864/19873]\n",
      "loss: 3.429970  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.5%, Avg loss: 3.431529 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.435633  [   64/19873]\n",
      "loss: 3.427340  [ 6464/19873]\n",
      "loss: 3.436420  [12864/19873]\n",
      "loss: 3.427483  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.6%, Avg loss: 3.430217 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.433636  [   64/19873]\n",
      "loss: 3.425630  [ 6464/19873]\n",
      "loss: 3.433146  [12864/19873]\n",
      "loss: 3.425205  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.8%, Avg loss: 3.428866 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.431453  [   64/19873]\n",
      "loss: 3.423969  [ 6464/19873]\n",
      "loss: 3.429735  [12864/19873]\n",
      "loss: 3.422904  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.3%, Avg loss: 3.427503 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.429014  [   64/19873]\n",
      "loss: 3.422258  [ 6464/19873]\n",
      "loss: 3.426240  [12864/19873]\n",
      "loss: 3.420561  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.7%, Avg loss: 3.426087 \n",
      "\n",
      "Testing size: 1024\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.430113  [   64/19873]\n",
      "loss: 3.437578  [ 6464/19873]\n",
      "loss: 3.427781  [12864/19873]\n",
      "loss: 3.427255  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 3.430534 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.428147  [   64/19873]\n",
      "loss: 3.435431  [ 6464/19873]\n",
      "loss: 3.424416  [12864/19873]\n",
      "loss: 3.425018  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 3.428887 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.426129  [   64/19873]\n",
      "loss: 3.433296  [ 6464/19873]\n",
      "loss: 3.421031  [12864/19873]\n",
      "loss: 3.422773  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.4%, Avg loss: 3.427227 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.424110  [   64/19873]\n",
      "loss: 3.431142  [ 6464/19873]\n",
      "loss: 3.417512  [12864/19873]\n",
      "loss: 3.420509  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.5%, Avg loss: 3.425479 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.422173  [   64/19873]\n",
      "loss: 3.429062  [ 6464/19873]\n",
      "loss: 3.413882  [12864/19873]\n",
      "loss: 3.418214  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.8%, Avg loss: 3.423694 \n",
      "\n",
      "Testing size: 2048\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.434841  [   64/19873]\n",
      "loss: 3.431907  [ 6464/19873]\n",
      "loss: 3.435506  [12864/19873]\n",
      "loss: 3.427737  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.6%, Avg loss: 3.430232 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.433041  [   64/19873]\n",
      "loss: 3.430317  [ 6464/19873]\n",
      "loss: 3.432545  [12864/19873]\n",
      "loss: 3.425919  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.0%, Avg loss: 3.428935 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.431224  [   64/19873]\n",
      "loss: 3.428733  [ 6464/19873]\n",
      "loss: 3.429508  [12864/19873]\n",
      "loss: 3.424128  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.1%, Avg loss: 3.427620 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.429259  [   64/19873]\n",
      "loss: 3.427093  [ 6464/19873]\n",
      "loss: 3.426401  [12864/19873]\n",
      "loss: 3.422328  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.0%, Avg loss: 3.426247 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.427170  [   64/19873]\n",
      "loss: 3.425376  [ 6464/19873]\n",
      "loss: 3.423066  [12864/19873]\n",
      "loss: 3.420497  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 3.424792 \n",
      "\n",
      "Testing size: 4096\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.433882  [   64/19873]\n",
      "loss: 3.430556  [ 6464/19873]\n",
      "loss: 3.434592  [12864/19873]\n",
      "loss: 3.432803  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.4%, Avg loss: 3.430338 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.431729  [   64/19873]\n",
      "loss: 3.428720  [ 6464/19873]\n",
      "loss: 3.431496  [12864/19873]\n",
      "loss: 3.430742  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.4%, Avg loss: 3.429114 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.429600  [   64/19873]\n",
      "loss: 3.426912  [ 6464/19873]\n",
      "loss: 3.428314  [12864/19873]\n",
      "loss: 3.428704  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.4%, Avg loss: 3.427863 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.427424  [   64/19873]\n",
      "loss: 3.425081  [ 6464/19873]\n",
      "loss: 3.424986  [12864/19873]\n",
      "loss: 3.426643  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.8%, Avg loss: 3.426545 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.425107  [   64/19873]\n",
      "loss: 3.423224  [ 6464/19873]\n",
      "loss: 3.421461  [12864/19873]\n",
      "loss: 3.424523  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.2%, Avg loss: 3.425143 \n",
      "\n",
      "Testing size: 8192\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.435421  [   64/19873]\n",
      "loss: 3.435006  [ 6464/19873]\n",
      "loss: 3.436190  [12864/19873]\n",
      "loss: 3.433785  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.432648 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.433021  [   64/19873]\n",
      "loss: 3.432737  [ 6464/19873]\n",
      "loss: 3.432377  [12864/19873]\n",
      "loss: 3.431265  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.5%, Avg loss: 3.431055 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.430596  [   64/19873]\n",
      "loss: 3.430475  [ 6464/19873]\n",
      "loss: 3.428457  [12864/19873]\n",
      "loss: 3.428746  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.3%, Avg loss: 3.429403 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.428086  [   64/19873]\n",
      "loss: 3.428205  [ 6464/19873]\n",
      "loss: 3.424345  [12864/19873]\n",
      "loss: 3.426182  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.7%, Avg loss: 3.427658 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.425407  [   64/19873]\n",
      "loss: 3.425912  [ 6464/19873]\n",
      "loss: 3.419919  [12864/19873]\n",
      "loss: 3.423527  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.8%, Avg loss: 3.425777 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=512):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(71636, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 2*size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "sizes_to_test = [4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "for size in sizes_to_test:\n",
    "    print(f\"Testing size: {size}\")\n",
    "    model = NeuralNetwork(size=size)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing learning rate: 0.1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.438538  [   64/19873]\n",
      "loss: 3.232512  [ 6464/19873]\n",
      "loss: 2.405616  [12864/19873]\n",
      "loss: 2.543983  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.0%, Avg loss: 2.796548 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.152519  [   64/19873]\n",
      "loss: 2.420685  [ 6464/19873]\n",
      "loss: 1.841949  [12864/19873]\n",
      "loss: 2.246860  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 17.5%, Avg loss: 2.967362 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.870271  [   64/19873]\n",
      "loss: 2.210711  [ 6464/19873]\n",
      "loss: 1.516833  [12864/19873]\n",
      "loss: 2.150987  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.2%, Avg loss: 2.890151 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.235148  [   64/19873]\n",
      "loss: 1.844906  [ 6464/19873]\n",
      "loss: 1.348147  [12864/19873]\n",
      "loss: 2.500719  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.7%, Avg loss: 5.179556 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.653522  [   64/19873]\n",
      "loss: 1.453320  [ 6464/19873]\n",
      "loss: 1.100212  [12864/19873]\n",
      "loss: 2.447454  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 2.715824 \n",
      "\n",
      "Testing learning rate: 0.01\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.438311  [   64/19873]\n",
      "loss: 3.408638  [ 6464/19873]\n",
      "loss: 3.286444  [12864/19873]\n",
      "loss: 3.345181  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.5%, Avg loss: 3.288057 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.209617  [   64/19873]\n",
      "loss: 3.285846  [ 6464/19873]\n",
      "loss: 2.503124  [12864/19873]\n",
      "loss: 3.249554  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.6%, Avg loss: 3.070137 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.020800  [   64/19873]\n",
      "loss: 3.140428  [ 6464/19873]\n",
      "loss: 2.323559  [12864/19873]\n",
      "loss: 3.087327  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.9%, Avg loss: 2.917675 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.917052  [   64/19873]\n",
      "loss: 2.907793  [ 6464/19873]\n",
      "loss: 2.192784  [12864/19873]\n",
      "loss: 2.881922  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.8%, Avg loss: 2.755246 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.745766  [   64/19873]\n",
      "loss: 2.634583  [ 6464/19873]\n",
      "loss: 2.067301  [12864/19873]\n",
      "loss: 2.673718  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.5%, Avg loss: 2.597900 \n",
      "\n",
      "Testing learning rate: 0.001\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.434576  [   64/19873]\n",
      "loss: 3.429962  [ 6464/19873]\n",
      "loss: 3.425077  [12864/19873]\n",
      "loss: 3.427764  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.3%, Avg loss: 3.425235 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.421201  [   64/19873]\n",
      "loss: 3.421767  [ 6464/19873]\n",
      "loss: 3.408158  [12864/19873]\n",
      "loss: 3.419171  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg loss: 3.417814 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.407462  [   64/19873]\n",
      "loss: 3.413454  [ 6464/19873]\n",
      "loss: 3.389299  [12864/19873]\n",
      "loss: 3.410134  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.2%, Avg loss: 3.409501 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.392337  [   64/19873]\n",
      "loss: 3.404465  [ 6464/19873]\n",
      "loss: 3.367292  [12864/19873]\n",
      "loss: 3.400237  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.0%, Avg loss: 3.399927 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.375172  [   64/19873]\n",
      "loss: 3.394237  [ 6464/19873]\n",
      "loss: 3.340902  [12864/19873]\n",
      "loss: 3.389340  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.4%, Avg loss: 3.388576 \n",
      "\n",
      "Testing learning rate: 0.0001\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.434455  [   64/19873]\n",
      "loss: 3.432490  [ 6464/19873]\n",
      "loss: 3.426924  [12864/19873]\n",
      "loss: 3.436389  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.6%, Avg loss: 3.433572 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.433110  [   64/19873]\n",
      "loss: 3.431558  [ 6464/19873]\n",
      "loss: 3.425081  [12864/19873]\n",
      "loss: 3.435608  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.0%, Avg loss: 3.432743 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.431788  [   64/19873]\n",
      "loss: 3.430645  [ 6464/19873]\n",
      "loss: 3.423254  [12864/19873]\n",
      "loss: 3.434834  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.0%, Avg loss: 3.431920 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.430474  [   64/19873]\n",
      "loss: 3.429745  [ 6464/19873]\n",
      "loss: 3.421441  [12864/19873]\n",
      "loss: 3.434055  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.2%, Avg loss: 3.431107 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.429165  [   64/19873]\n",
      "loss: 3.428860  [ 6464/19873]\n",
      "loss: 3.419643  [12864/19873]\n",
      "loss: 3.433272  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.430300 \n",
      "\n",
      "Testing learning rate: 1e-05\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.434235  [   64/19873]\n",
      "loss: 3.434117  [ 6464/19873]\n",
      "loss: 3.427557  [12864/19873]\n",
      "loss: 3.435192  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.3%, Avg loss: 3.439867 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.434109  [   64/19873]\n",
      "loss: 3.434039  [ 6464/19873]\n",
      "loss: 3.427393  [12864/19873]\n",
      "loss: 3.435115  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.3%, Avg loss: 3.439796 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.433984  [   64/19873]\n",
      "loss: 3.433962  [ 6464/19873]\n",
      "loss: 3.427230  [12864/19873]\n",
      "loss: 3.435038  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.3%, Avg loss: 3.439724 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.433858  [   64/19873]\n",
      "loss: 3.433885  [ 6464/19873]\n",
      "loss: 3.427066  [12864/19873]\n",
      "loss: 3.434961  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.3%, Avg loss: 3.439653 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.433733  [   64/19873]\n",
      "loss: 3.433808  [ 6464/19873]\n",
      "loss: 3.426903  [12864/19873]\n",
      "loss: 3.434884  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.4%, Avg loss: 3.439581 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=512):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(71636, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "r_to_test = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "for lr in lr_to_test:\n",
    "    print(f\"Testing learning rate: {lr}\")\n",
    "    model = NeuralNetwork(size=2048)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.437656  [   64/19873]\n",
      "loss: 3.411568  [ 6464/19873]\n",
      "loss: 3.299207  [12864/19873]\n",
      "loss: 3.356930  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.5%, Avg loss: 3.290393 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.206783  [   64/19873]\n",
      "loss: 3.288518  [ 6464/19873]\n",
      "loss: 2.500663  [12864/19873]\n",
      "loss: 3.264573  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.7%, Avg loss: 3.077556 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.024760  [   64/19873]\n",
      "loss: 3.155875  [ 6464/19873]\n",
      "loss: 2.324517  [12864/19873]\n",
      "loss: 3.111621  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.1%, Avg loss: 2.928454 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.940517  [   64/19873]\n",
      "loss: 2.943711  [ 6464/19873]\n",
      "loss: 2.198317  [12864/19873]\n",
      "loss: 2.902158  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.3%, Avg loss: 2.772678 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.780496  [   64/19873]\n",
      "loss: 2.668342  [ 6464/19873]\n",
      "loss: 2.075574  [12864/19873]\n",
      "loss: 2.686620  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 17.9%, Avg loss: 2.616878 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.549902  [   64/19873]\n",
      "loss: 2.439976  [ 6464/19873]\n",
      "loss: 1.938313  [12864/19873]\n",
      "loss: 2.504743  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.2%, Avg loss: 2.485929 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.325297  [   64/19873]\n",
      "loss: 2.272206  [ 6464/19873]\n",
      "loss: 1.803018  [12864/19873]\n",
      "loss: 2.356519  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 21.5%, Avg loss: 2.387650 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.153159  [   64/19873]\n",
      "loss: 2.120801  [ 6464/19873]\n",
      "loss: 1.681291  [12864/19873]\n",
      "loss: 2.234636  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.7%, Avg loss: 2.316141 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.027330  [   64/19873]\n",
      "loss: 1.969004  [ 6464/19873]\n",
      "loss: 1.571426  [12864/19873]\n",
      "loss: 2.124441  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.7%, Avg loss: 2.271550 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.931458  [   64/19873]\n",
      "loss: 1.819063  [ 6464/19873]\n",
      "loss: 1.475349  [12864/19873]\n",
      "loss: 2.019705  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.3%, Avg loss: 2.255732 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.853337  [   64/19873]\n",
      "loss: 1.680074  [ 6464/19873]\n",
      "loss: 1.391549  [12864/19873]\n",
      "loss: 1.922509  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.2%, Avg loss: 2.258900 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.783228  [   64/19873]\n",
      "loss: 1.557801  [ 6464/19873]\n",
      "loss: 1.319806  [12864/19873]\n",
      "loss: 1.832688  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 29.0%, Avg loss: 2.270188 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.714830  [   64/19873]\n",
      "loss: 1.447812  [ 6464/19873]\n",
      "loss: 1.255432  [12864/19873]\n",
      "loss: 1.751049  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.0%, Avg loss: 2.280533 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.644142  [   64/19873]\n",
      "loss: 1.348347  [ 6464/19873]\n",
      "loss: 1.195720  [12864/19873]\n",
      "loss: 1.676870  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.0%, Avg loss: 2.280421 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.569591  [   64/19873]\n",
      "loss: 1.255982  [ 6464/19873]\n",
      "loss: 1.138067  [12864/19873]\n",
      "loss: 1.607711  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 2.270144 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.492670  [   64/19873]\n",
      "loss: 1.167378  [ 6464/19873]\n",
      "loss: 1.082211  [12864/19873]\n",
      "loss: 1.540930  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 2.250878 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.414730  [   64/19873]\n",
      "loss: 1.081942  [ 6464/19873]\n",
      "loss: 1.028415  [12864/19873]\n",
      "loss: 1.475166  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 36.3%, Avg loss: 2.229559 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.339286  [   64/19873]\n",
      "loss: 0.999907  [ 6464/19873]\n",
      "loss: 0.977056  [12864/19873]\n",
      "loss: 1.409488  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 2.207993 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.266528  [   64/19873]\n",
      "loss: 0.920689  [ 6464/19873]\n",
      "loss: 0.927835  [12864/19873]\n",
      "loss: 1.343224  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 2.189759 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.196828  [   64/19873]\n",
      "loss: 0.846721  [ 6464/19873]\n",
      "loss: 0.880510  [12864/19873]\n",
      "loss: 1.275558  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 40.8%, Avg loss: 2.171791 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.124231  [   64/19873]\n",
      "loss: 0.781169  [ 6464/19873]\n",
      "loss: 0.835443  [12864/19873]\n",
      "loss: 1.207343  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 41.7%, Avg loss: 2.153726 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.048813  [   64/19873]\n",
      "loss: 0.723769  [ 6464/19873]\n",
      "loss: 0.792387  [12864/19873]\n",
      "loss: 1.139720  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.7%, Avg loss: 2.136228 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.970952  [   64/19873]\n",
      "loss: 0.674516  [ 6464/19873]\n",
      "loss: 0.751539  [12864/19873]\n",
      "loss: 1.073373  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 44.0%, Avg loss: 2.120017 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.893713  [   64/19873]\n",
      "loss: 0.631282  [ 6464/19873]\n",
      "loss: 0.712596  [12864/19873]\n",
      "loss: 1.010021  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.3%, Avg loss: 2.104444 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.821693  [   64/19873]\n",
      "loss: 0.592890  [ 6464/19873]\n",
      "loss: 0.675907  [12864/19873]\n",
      "loss: 0.949475  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 46.1%, Avg loss: 2.091028 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.758758  [   64/19873]\n",
      "loss: 0.557972  [ 6464/19873]\n",
      "loss: 0.641060  [12864/19873]\n",
      "loss: 0.892032  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 2.077885 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.705506  [   64/19873]\n",
      "loss: 0.525854  [ 6464/19873]\n",
      "loss: 0.607637  [12864/19873]\n",
      "loss: 0.837430  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 2.065233 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.661055  [   64/19873]\n",
      "loss: 0.495698  [ 6464/19873]\n",
      "loss: 0.575827  [12864/19873]\n",
      "loss: 0.785862  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 2.055496 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.624160  [   64/19873]\n",
      "loss: 0.467472  [ 6464/19873]\n",
      "loss: 0.545365  [12864/19873]\n",
      "loss: 0.736717  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 48.4%, Avg loss: 2.046685 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.593097  [   64/19873]\n",
      "loss: 0.440859  [ 6464/19873]\n",
      "loss: 0.516185  [12864/19873]\n",
      "loss: 0.690105  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 2.038909 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.565972  [   64/19873]\n",
      "loss: 0.415656  [ 6464/19873]\n",
      "loss: 0.488323  [12864/19873]\n",
      "loss: 0.646233  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 2.033793 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.542547  [   64/19873]\n",
      "loss: 0.392047  [ 6464/19873]\n",
      "loss: 0.461649  [12864/19873]\n",
      "loss: 0.605002  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 2.029102 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.521273  [   64/19873]\n",
      "loss: 0.369764  [ 6464/19873]\n",
      "loss: 0.436252  [12864/19873]\n",
      "loss: 0.566178  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 2.025358 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.501868  [   64/19873]\n",
      "loss: 0.348539  [ 6464/19873]\n",
      "loss: 0.412241  [12864/19873]\n",
      "loss: 0.530041  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 2.022781 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.484032  [   64/19873]\n",
      "loss: 0.328547  [ 6464/19873]\n",
      "loss: 0.389412  [12864/19873]\n",
      "loss: 0.495999  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 2.021942 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.467264  [   64/19873]\n",
      "loss: 0.309747  [ 6464/19873]\n",
      "loss: 0.368049  [12864/19873]\n",
      "loss: 0.464377  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 2.022060 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.451347  [   64/19873]\n",
      "loss: 0.291959  [ 6464/19873]\n",
      "loss: 0.347665  [12864/19873]\n",
      "loss: 0.434998  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 2.023093 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.435873  [   64/19873]\n",
      "loss: 0.275271  [ 6464/19873]\n",
      "loss: 0.328690  [12864/19873]\n",
      "loss: 0.407696  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 2.025789 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.421039  [   64/19873]\n",
      "loss: 0.259614  [ 6464/19873]\n",
      "loss: 0.310963  [12864/19873]\n",
      "loss: 0.382289  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 2.029290 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.406463  [   64/19873]\n",
      "loss: 0.245020  [ 6464/19873]\n",
      "loss: 0.294367  [12864/19873]\n",
      "loss: 0.358701  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 2.034125 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.392285  [   64/19873]\n",
      "loss: 0.231338  [ 6464/19873]\n",
      "loss: 0.278967  [12864/19873]\n",
      "loss: 0.336696  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 2.039627 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.378282  [   64/19873]\n",
      "loss: 0.218761  [ 6464/19873]\n",
      "loss: 0.264490  [12864/19873]\n",
      "loss: 0.316262  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 2.045934 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.364645  [   64/19873]\n",
      "loss: 0.207019  [ 6464/19873]\n",
      "loss: 0.251018  [12864/19873]\n",
      "loss: 0.297199  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 2.053639 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.351315  [   64/19873]\n",
      "loss: 0.196167  [ 6464/19873]\n",
      "loss: 0.238438  [12864/19873]\n",
      "loss: 0.279569  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 2.061524 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.338207  [   64/19873]\n",
      "loss: 0.186070  [ 6464/19873]\n",
      "loss: 0.226921  [12864/19873]\n",
      "loss: 0.263377  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 2.070737 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.325607  [   64/19873]\n",
      "loss: 0.176687  [ 6464/19873]\n",
      "loss: 0.216047  [12864/19873]\n",
      "loss: 0.248480  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 2.080069 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.313167  [   64/19873]\n",
      "loss: 0.168032  [ 6464/19873]\n",
      "loss: 0.206096  [12864/19873]\n",
      "loss: 0.234792  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 2.090119 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.301525  [   64/19873]\n",
      "loss: 0.159968  [ 6464/19873]\n",
      "loss: 0.196750  [12864/19873]\n",
      "loss: 0.222408  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 2.100776 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.290284  [   64/19873]\n",
      "loss: 0.152545  [ 6464/19873]\n",
      "loss: 0.188185  [12864/19873]\n",
      "loss: 0.211208  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 2.111399 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.279466  [   64/19873]\n",
      "loss: 0.145570  [ 6464/19873]\n",
      "loss: 0.180097  [12864/19873]\n",
      "loss: 0.201003  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 2.123302 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.269375  [   64/19873]\n",
      "loss: 0.139006  [ 6464/19873]\n",
      "loss: 0.172655  [12864/19873]\n",
      "loss: 0.191773  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 2.134893 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.259985  [   64/19873]\n",
      "loss: 0.133042  [ 6464/19873]\n",
      "loss: 0.165670  [12864/19873]\n",
      "loss: 0.183569  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.146852 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.251070  [   64/19873]\n",
      "loss: 0.127295  [ 6464/19873]\n",
      "loss: 0.159312  [12864/19873]\n",
      "loss: 0.176204  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.158650 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.242719  [   64/19873]\n",
      "loss: 0.122012  [ 6464/19873]\n",
      "loss: 0.153269  [12864/19873]\n",
      "loss: 0.169573  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.171110 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.234982  [   64/19873]\n",
      "loss: 0.117041  [ 6464/19873]\n",
      "loss: 0.147785  [12864/19873]\n",
      "loss: 0.163688  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.183770 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.227672  [   64/19873]\n",
      "loss: 0.112365  [ 6464/19873]\n",
      "loss: 0.142600  [12864/19873]\n",
      "loss: 0.158370  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.196444 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.220883  [   64/19873]\n",
      "loss: 0.107960  [ 6464/19873]\n",
      "loss: 0.137768  [12864/19873]\n",
      "loss: 0.153465  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.209066 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.214570  [   64/19873]\n",
      "loss: 0.103780  [ 6464/19873]\n",
      "loss: 0.133120  [12864/19873]\n",
      "loss: 0.149076  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.222087 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.208632  [   64/19873]\n",
      "loss: 0.099965  [ 6464/19873]\n",
      "loss: 0.128890  [12864/19873]\n",
      "loss: 0.145086  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.234091 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.203009  [   64/19873]\n",
      "loss: 0.096347  [ 6464/19873]\n",
      "loss: 0.124728  [12864/19873]\n",
      "loss: 0.141444  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.246549 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.197968  [   64/19873]\n",
      "loss: 0.092962  [ 6464/19873]\n",
      "loss: 0.120888  [12864/19873]\n",
      "loss: 0.137994  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.259298 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.192844  [   64/19873]\n",
      "loss: 0.089823  [ 6464/19873]\n",
      "loss: 0.117328  [12864/19873]\n",
      "loss: 0.134876  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.271490 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.188348  [   64/19873]\n",
      "loss: 0.086816  [ 6464/19873]\n",
      "loss: 0.113819  [12864/19873]\n",
      "loss: 0.132109  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 2.283396 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.184223  [   64/19873]\n",
      "loss: 0.084112  [ 6464/19873]\n",
      "loss: 0.110578  [12864/19873]\n",
      "loss: 0.129388  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.295288 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.180021  [   64/19873]\n",
      "loss: 0.081477  [ 6464/19873]\n",
      "loss: 0.107507  [12864/19873]\n",
      "loss: 0.126962  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.307609 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.176283  [   64/19873]\n",
      "loss: 0.079074  [ 6464/19873]\n",
      "loss: 0.104495  [12864/19873]\n",
      "loss: 0.124618  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.319301 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.172767  [   64/19873]\n",
      "loss: 0.076871  [ 6464/19873]\n",
      "loss: 0.101730  [12864/19873]\n",
      "loss: 0.122574  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.330827 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.169469  [   64/19873]\n",
      "loss: 0.074780  [ 6464/19873]\n",
      "loss: 0.099053  [12864/19873]\n",
      "loss: 0.120617  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.341963 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.166257  [   64/19873]\n",
      "loss: 0.072795  [ 6464/19873]\n",
      "loss: 0.096484  [12864/19873]\n",
      "loss: 0.118816  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.353650 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.163377  [   64/19873]\n",
      "loss: 0.070989  [ 6464/19873]\n",
      "loss: 0.094107  [12864/19873]\n",
      "loss: 0.117113  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.364065 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.160569  [   64/19873]\n",
      "loss: 0.069313  [ 6464/19873]\n",
      "loss: 0.091729  [12864/19873]\n",
      "loss: 0.115528  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.375347 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.157956  [   64/19873]\n",
      "loss: 0.067686  [ 6464/19873]\n",
      "loss: 0.089544  [12864/19873]\n",
      "loss: 0.114054  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.386576 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.155733  [   64/19873]\n",
      "loss: 0.066147  [ 6464/19873]\n",
      "loss: 0.087445  [12864/19873]\n",
      "loss: 0.112604  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.396199 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.153490  [   64/19873]\n",
      "loss: 0.064728  [ 6464/19873]\n",
      "loss: 0.085391  [12864/19873]\n",
      "loss: 0.111210  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.405629 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.151336  [   64/19873]\n",
      "loss: 0.063396  [ 6464/19873]\n",
      "loss: 0.083468  [12864/19873]\n",
      "loss: 0.109932  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.416048 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.149448  [   64/19873]\n",
      "loss: 0.062093  [ 6464/19873]\n",
      "loss: 0.081658  [12864/19873]\n",
      "loss: 0.108848  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.425524 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.147798  [   64/19873]\n",
      "loss: 0.060910  [ 6464/19873]\n",
      "loss: 0.079878  [12864/19873]\n",
      "loss: 0.107566  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.435502 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.145971  [   64/19873]\n",
      "loss: 0.059818  [ 6464/19873]\n",
      "loss: 0.078172  [12864/19873]\n",
      "loss: 0.106586  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.444065 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.144436  [   64/19873]\n",
      "loss: 0.058784  [ 6464/19873]\n",
      "loss: 0.076521  [12864/19873]\n",
      "loss: 0.105509  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.453436 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.143032  [   64/19873]\n",
      "loss: 0.057754  [ 6464/19873]\n",
      "loss: 0.074993  [12864/19873]\n",
      "loss: 0.104564  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.461738 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.141557  [   64/19873]\n",
      "loss: 0.056823  [ 6464/19873]\n",
      "loss: 0.073459  [12864/19873]\n",
      "loss: 0.103675  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.470587 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.140448  [   64/19873]\n",
      "loss: 0.055877  [ 6464/19873]\n",
      "loss: 0.072004  [12864/19873]\n",
      "loss: 0.102692  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.479049 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.139203  [   64/19873]\n",
      "loss: 0.055057  [ 6464/19873]\n",
      "loss: 0.070581  [12864/19873]\n",
      "loss: 0.101639  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.487720 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.138337  [   64/19873]\n",
      "loss: 0.054245  [ 6464/19873]\n",
      "loss: 0.069198  [12864/19873]\n",
      "loss: 0.100901  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.495089 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.137272  [   64/19873]\n",
      "loss: 0.053420  [ 6464/19873]\n",
      "loss: 0.067921  [12864/19873]\n",
      "loss: 0.100219  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.503426 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.136190  [   64/19873]\n",
      "loss: 0.052704  [ 6464/19873]\n",
      "loss: 0.066652  [12864/19873]\n",
      "loss: 0.099344  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.511397 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.135242  [   64/19873]\n",
      "loss: 0.052044  [ 6464/19873]\n",
      "loss: 0.065425  [12864/19873]\n",
      "loss: 0.098951  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.519365 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.134663  [   64/19873]\n",
      "loss: 0.051366  [ 6464/19873]\n",
      "loss: 0.064229  [12864/19873]\n",
      "loss: 0.098073  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.526118 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.133717  [   64/19873]\n",
      "loss: 0.050690  [ 6464/19873]\n",
      "loss: 0.063043  [12864/19873]\n",
      "loss: 0.097431  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.533116 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.133131  [   64/19873]\n",
      "loss: 0.050109  [ 6464/19873]\n",
      "loss: 0.061922  [12864/19873]\n",
      "loss: 0.096698  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.539902 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.132522  [   64/19873]\n",
      "loss: 0.049521  [ 6464/19873]\n",
      "loss: 0.060838  [12864/19873]\n",
      "loss: 0.096112  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.547817 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.132029  [   64/19873]\n",
      "loss: 0.048930  [ 6464/19873]\n",
      "loss: 0.059798  [12864/19873]\n",
      "loss: 0.095474  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.553847 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.131046  [   64/19873]\n",
      "loss: 0.048376  [ 6464/19873]\n",
      "loss: 0.058754  [12864/19873]\n",
      "loss: 0.094927  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 2.560931 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.130786  [   64/19873]\n",
      "loss: 0.047858  [ 6464/19873]\n",
      "loss: 0.057803  [12864/19873]\n",
      "loss: 0.094466  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.566334 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.130139  [   64/19873]\n",
      "loss: 0.047326  [ 6464/19873]\n",
      "loss: 0.056822  [12864/19873]\n",
      "loss: 0.093839  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.573215 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.129661  [   64/19873]\n",
      "loss: 0.046965  [ 6464/19873]\n",
      "loss: 0.055868  [12864/19873]\n",
      "loss: 0.093360  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.580444 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.129384  [   64/19873]\n",
      "loss: 0.046473  [ 6464/19873]\n",
      "loss: 0.054967  [12864/19873]\n",
      "loss: 0.092931  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.586575 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.128791  [   64/19873]\n",
      "loss: 0.046140  [ 6464/19873]\n",
      "loss: 0.054112  [12864/19873]\n",
      "loss: 0.092403  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.592888 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.128400  [   64/19873]\n",
      "loss: 0.045717  [ 6464/19873]\n",
      "loss: 0.053224  [12864/19873]\n",
      "loss: 0.092030  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.598113 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.127955  [   64/19873]\n",
      "loss: 0.045324  [ 6464/19873]\n",
      "loss: 0.052412  [12864/19873]\n",
      "loss: 0.091576  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.604170 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.float()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct, test_loss\n",
    "\n",
    "accuracies = []\n",
    "losses = []\n",
    "model = NeuralNetwork(size=2048)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "for e in range(100):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    c, l = test(val_dataloader, model, loss_fn)\n",
    "    accuracies.append(c)\n",
    "    losses.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.plot(range(len(accuracies)), accuracies, color='blue')\n",
    "ax1.set_ylabel('accuracy (%)', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(len(losses)), losses, color='red')\n",
    "ax2.set_ylabel('loss', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.435747  [   64/19873]\n",
      "loss: 3.436984  [ 6464/19873]\n",
      "loss: 3.423784  [12864/19873]\n",
      "loss: 3.430506  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.424442 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.424414  [   64/19873]\n",
      "loss: 3.429589  [ 6464/19873]\n",
      "loss: 3.406697  [12864/19873]\n",
      "loss: 3.423506  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.4%, Avg loss: 3.417385 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.411875  [   64/19873]\n",
      "loss: 3.422109  [ 6464/19873]\n",
      "loss: 3.386972  [12864/19873]\n",
      "loss: 3.416448  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.0%, Avg loss: 3.409485 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.397501  [   64/19873]\n",
      "loss: 3.414029  [ 6464/19873]\n",
      "loss: 3.363566  [12864/19873]\n",
      "loss: 3.408613  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.8%, Avg loss: 3.400243 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.380370  [   64/19873]\n",
      "loss: 3.405009  [ 6464/19873]\n",
      "loss: 3.334330  [12864/19873]\n",
      "loss: 3.399773  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.2%, Avg loss: 3.389042 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.359496  [   64/19873]\n",
      "loss: 3.394744  [ 6464/19873]\n",
      "loss: 3.297369  [12864/19873]\n",
      "loss: 3.390066  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.4%, Avg loss: 3.375263 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.333663  [   64/19873]\n",
      "loss: 3.383018  [ 6464/19873]\n",
      "loss: 3.250484  [12864/19873]\n",
      "loss: 3.379443  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.0%, Avg loss: 3.358282 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.301541  [   64/19873]\n",
      "loss: 3.369969  [ 6464/19873]\n",
      "loss: 3.190190  [12864/19873]\n",
      "loss: 3.368025  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.8%, Avg loss: 3.337363 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.261523  [   64/19873]\n",
      "loss: 3.355958  [ 6464/19873]\n",
      "loss: 3.112663  [12864/19873]\n",
      "loss: 3.356130  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.5%, Avg loss: 3.311836 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.212601  [   64/19873]\n",
      "loss: 3.341092  [ 6464/19873]\n",
      "loss: 3.015175  [12864/19873]\n",
      "loss: 3.343928  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.4%, Avg loss: 3.281635 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 3.156607  [   64/19873]\n",
      "loss: 3.325607  [ 6464/19873]\n",
      "loss: 2.900555  [12864/19873]\n",
      "loss: 3.331762  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.7%, Avg loss: 3.248881 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 3.100356  [   64/19873]\n",
      "loss: 3.310010  [ 6464/19873]\n",
      "loss: 2.782646  [12864/19873]\n",
      "loss: 3.319878  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.1%, Avg loss: 3.217921 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 3.053644  [   64/19873]\n",
      "loss: 3.294385  [ 6464/19873]\n",
      "loss: 2.681378  [12864/19873]\n",
      "loss: 3.308574  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.3%, Avg loss: 3.192171 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 3.019547  [   64/19873]\n",
      "loss: 3.279061  [ 6464/19873]\n",
      "loss: 2.606560  [12864/19873]\n",
      "loss: 3.298075  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.5%, Avg loss: 3.171800 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.995025  [   64/19873]\n",
      "loss: 3.263980  [ 6464/19873]\n",
      "loss: 2.554442  [12864/19873]\n",
      "loss: 3.288427  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.9%, Avg loss: 3.155221 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.976243  [   64/19873]\n",
      "loss: 3.249198  [ 6464/19873]\n",
      "loss: 2.517262  [12864/19873]\n",
      "loss: 3.279341  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.2%, Avg loss: 3.140982 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.960443  [   64/19873]\n",
      "loss: 3.234368  [ 6464/19873]\n",
      "loss: 2.489216  [12864/19873]\n",
      "loss: 3.270833  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.4%, Avg loss: 3.128160 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.946062  [   64/19873]\n",
      "loss: 3.219443  [ 6464/19873]\n",
      "loss: 2.466645  [12864/19873]\n",
      "loss: 3.262808  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.5%, Avg loss: 3.115944 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.932242  [   64/19873]\n",
      "loss: 3.204178  [ 6464/19873]\n",
      "loss: 2.447334  [12864/19873]\n",
      "loss: 3.254930  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.7%, Avg loss: 3.104003 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.918388  [   64/19873]\n",
      "loss: 3.188229  [ 6464/19873]\n",
      "loss: 2.429913  [12864/19873]\n",
      "loss: 3.246969  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.0%, Avg loss: 3.091988 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.904379  [   64/19873]\n",
      "loss: 3.171500  [ 6464/19873]\n",
      "loss: 2.413379  [12864/19873]\n",
      "loss: 3.238777  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.2%, Avg loss: 3.079691 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.889898  [   64/19873]\n",
      "loss: 3.153472  [ 6464/19873]\n",
      "loss: 2.397386  [12864/19873]\n",
      "loss: 3.230499  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.3%, Avg loss: 3.066917 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.874798  [   64/19873]\n",
      "loss: 3.134028  [ 6464/19873]\n",
      "loss: 2.381616  [12864/19873]\n",
      "loss: 3.221828  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.5%, Avg loss: 3.053538 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.859104  [   64/19873]\n",
      "loss: 3.113075  [ 6464/19873]\n",
      "loss: 2.365963  [12864/19873]\n",
      "loss: 3.212746  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 3.039474 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.842795  [   64/19873]\n",
      "loss: 3.090544  [ 6464/19873]\n",
      "loss: 2.350269  [12864/19873]\n",
      "loss: 3.203292  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 3.024815 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.825566  [   64/19873]\n",
      "loss: 3.066665  [ 6464/19873]\n",
      "loss: 2.334367  [12864/19873]\n",
      "loss: 3.193300  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 3.009465 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.807546  [   64/19873]\n",
      "loss: 3.041312  [ 6464/19873]\n",
      "loss: 2.318373  [12864/19873]\n",
      "loss: 3.182673  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.8%, Avg loss: 2.993478 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.788725  [   64/19873]\n",
      "loss: 3.014620  [ 6464/19873]\n",
      "loss: 2.302418  [12864/19873]\n",
      "loss: 3.171532  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.0%, Avg loss: 2.976816 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.769231  [   64/19873]\n",
      "loss: 2.986561  [ 6464/19873]\n",
      "loss: 2.286453  [12864/19873]\n",
      "loss: 3.159926  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.9%, Avg loss: 2.959575 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.748878  [   64/19873]\n",
      "loss: 2.957257  [ 6464/19873]\n",
      "loss: 2.270566  [12864/19873]\n",
      "loss: 3.147902  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.0%, Avg loss: 2.941755 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.727619  [   64/19873]\n",
      "loss: 2.927091  [ 6464/19873]\n",
      "loss: 2.254864  [12864/19873]\n",
      "loss: 3.135457  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 2.923431 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.705460  [   64/19873]\n",
      "loss: 2.896220  [ 6464/19873]\n",
      "loss: 2.239366  [12864/19873]\n",
      "loss: 3.122570  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.8%, Avg loss: 2.904781 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.682389  [   64/19873]\n",
      "loss: 2.864832  [ 6464/19873]\n",
      "loss: 2.224101  [12864/19873]\n",
      "loss: 3.109170  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.0%, Avg loss: 2.885772 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.658504  [   64/19873]\n",
      "loss: 2.833104  [ 6464/19873]\n",
      "loss: 2.209030  [12864/19873]\n",
      "loss: 3.095066  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.1%, Avg loss: 2.866515 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.633898  [   64/19873]\n",
      "loss: 2.801257  [ 6464/19873]\n",
      "loss: 2.194093  [12864/19873]\n",
      "loss: 3.080351  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.4%, Avg loss: 2.846987 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.608611  [   64/19873]\n",
      "loss: 2.769652  [ 6464/19873]\n",
      "loss: 2.179307  [12864/19873]\n",
      "loss: 3.065254  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.2%, Avg loss: 2.827385 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.582777  [   64/19873]\n",
      "loss: 2.738472  [ 6464/19873]\n",
      "loss: 2.164609  [12864/19873]\n",
      "loss: 3.049722  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.5%, Avg loss: 2.807802 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.556555  [   64/19873]\n",
      "loss: 2.707896  [ 6464/19873]\n",
      "loss: 2.150168  [12864/19873]\n",
      "loss: 3.033706  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.8%, Avg loss: 2.788370 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.530036  [   64/19873]\n",
      "loss: 2.678265  [ 6464/19873]\n",
      "loss: 2.135878  [12864/19873]\n",
      "loss: 3.017173  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 2.769083 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.503131  [   64/19873]\n",
      "loss: 2.649528  [ 6464/19873]\n",
      "loss: 2.121723  [12864/19873]\n",
      "loss: 3.000148  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.9%, Avg loss: 2.749978 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.476052  [   64/19873]\n",
      "loss: 2.621646  [ 6464/19873]\n",
      "loss: 2.107623  [12864/19873]\n",
      "loss: 2.982771  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 17.0%, Avg loss: 2.731099 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.448774  [   64/19873]\n",
      "loss: 2.594715  [ 6464/19873]\n",
      "loss: 2.093524  [12864/19873]\n",
      "loss: 2.965055  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 17.7%, Avg loss: 2.712488 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.421456  [   64/19873]\n",
      "loss: 2.568472  [ 6464/19873]\n",
      "loss: 2.079258  [12864/19873]\n",
      "loss: 2.946767  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.1%, Avg loss: 2.694068 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.394282  [   64/19873]\n",
      "loss: 2.543114  [ 6464/19873]\n",
      "loss: 2.064756  [12864/19873]\n",
      "loss: 2.928093  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.5%, Avg loss: 2.675935 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.367260  [   64/19873]\n",
      "loss: 2.518657  [ 6464/19873]\n",
      "loss: 2.050091  [12864/19873]\n",
      "loss: 2.908989  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.7%, Avg loss: 2.658173 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.340421  [   64/19873]\n",
      "loss: 2.494872  [ 6464/19873]\n",
      "loss: 2.035129  [12864/19873]\n",
      "loss: 2.889508  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.1%, Avg loss: 2.640734 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.313695  [   64/19873]\n",
      "loss: 2.471828  [ 6464/19873]\n",
      "loss: 2.019938  [12864/19873]\n",
      "loss: 2.869658  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.4%, Avg loss: 2.623597 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.287265  [   64/19873]\n",
      "loss: 2.449543  [ 6464/19873]\n",
      "loss: 2.004514  [12864/19873]\n",
      "loss: 2.849471  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.2%, Avg loss: 2.606848 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.261180  [   64/19873]\n",
      "loss: 2.427897  [ 6464/19873]\n",
      "loss: 1.988893  [12864/19873]\n",
      "loss: 2.829144  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.4%, Avg loss: 2.590457 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.235534  [   64/19873]\n",
      "loss: 2.407012  [ 6464/19873]\n",
      "loss: 1.973148  [12864/19873]\n",
      "loss: 2.808642  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.6%, Avg loss: 2.574406 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 2.210368  [   64/19873]\n",
      "loss: 2.386666  [ 6464/19873]\n",
      "loss: 1.957326  [12864/19873]\n",
      "loss: 2.788117  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.7%, Avg loss: 2.558695 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.185746  [   64/19873]\n",
      "loss: 2.366866  [ 6464/19873]\n",
      "loss: 1.941457  [12864/19873]\n",
      "loss: 2.767655  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.8%, Avg loss: 2.543361 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 2.161652  [   64/19873]\n",
      "loss: 2.347686  [ 6464/19873]\n",
      "loss: 1.925550  [12864/19873]\n",
      "loss: 2.747214  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 20.2%, Avg loss: 2.528311 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.138196  [   64/19873]\n",
      "loss: 2.328927  [ 6464/19873]\n",
      "loss: 1.909659  [12864/19873]\n",
      "loss: 2.726967  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 21.0%, Avg loss: 2.513564 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 2.115358  [   64/19873]\n",
      "loss: 2.310638  [ 6464/19873]\n",
      "loss: 1.893889  [12864/19873]\n",
      "loss: 2.706970  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 21.5%, Avg loss: 2.499148 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.093118  [   64/19873]\n",
      "loss: 2.292660  [ 6464/19873]\n",
      "loss: 1.878209  [12864/19873]\n",
      "loss: 2.687260  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 22.0%, Avg loss: 2.485036 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.071528  [   64/19873]\n",
      "loss: 2.275282  [ 6464/19873]\n",
      "loss: 1.862666  [12864/19873]\n",
      "loss: 2.667834  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 22.5%, Avg loss: 2.471209 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 2.050644  [   64/19873]\n",
      "loss: 2.258087  [ 6464/19873]\n",
      "loss: 1.847255  [12864/19873]\n",
      "loss: 2.648695  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 22.8%, Avg loss: 2.457685 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 2.030367  [   64/19873]\n",
      "loss: 2.241218  [ 6464/19873]\n",
      "loss: 1.832108  [12864/19873]\n",
      "loss: 2.629835  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.0%, Avg loss: 2.444394 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 2.010677  [   64/19873]\n",
      "loss: 2.224586  [ 6464/19873]\n",
      "loss: 1.817143  [12864/19873]\n",
      "loss: 2.611409  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.3%, Avg loss: 2.431397 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.991668  [   64/19873]\n",
      "loss: 2.208247  [ 6464/19873]\n",
      "loss: 1.802384  [12864/19873]\n",
      "loss: 2.593270  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.7%, Avg loss: 2.418614 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1.973242  [   64/19873]\n",
      "loss: 2.191947  [ 6464/19873]\n",
      "loss: 1.787853  [12864/19873]\n",
      "loss: 2.575454  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.1%, Avg loss: 2.406089 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.955430  [   64/19873]\n",
      "loss: 2.175835  [ 6464/19873]\n",
      "loss: 1.773554  [12864/19873]\n",
      "loss: 2.557828  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.4%, Avg loss: 2.393846 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.938166  [   64/19873]\n",
      "loss: 2.159845  [ 6464/19873]\n",
      "loss: 1.759470  [12864/19873]\n",
      "loss: 2.540558  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.5%, Avg loss: 2.381806 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.921453  [   64/19873]\n",
      "loss: 2.143860  [ 6464/19873]\n",
      "loss: 1.745595  [12864/19873]\n",
      "loss: 2.523429  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.5%, Avg loss: 2.370017 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.905298  [   64/19873]\n",
      "loss: 2.128011  [ 6464/19873]\n",
      "loss: 1.731931  [12864/19873]\n",
      "loss: 2.506552  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.9%, Avg loss: 2.358394 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.889641  [   64/19873]\n",
      "loss: 2.112088  [ 6464/19873]\n",
      "loss: 1.718474  [12864/19873]\n",
      "loss: 2.489970  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.1%, Avg loss: 2.347089 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.874398  [   64/19873]\n",
      "loss: 2.096267  [ 6464/19873]\n",
      "loss: 1.705191  [12864/19873]\n",
      "loss: 2.473606  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.3%, Avg loss: 2.335949 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.859595  [   64/19873]\n",
      "loss: 2.080477  [ 6464/19873]\n",
      "loss: 1.692103  [12864/19873]\n",
      "loss: 2.457469  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.6%, Avg loss: 2.325034 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.845230  [   64/19873]\n",
      "loss: 2.064658  [ 6464/19873]\n",
      "loss: 1.679234  [12864/19873]\n",
      "loss: 2.441474  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.7%, Avg loss: 2.314271 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.831249  [   64/19873]\n",
      "loss: 2.048792  [ 6464/19873]\n",
      "loss: 1.666585  [12864/19873]\n",
      "loss: 2.425731  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.8%, Avg loss: 2.303757 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.817656  [   64/19873]\n",
      "loss: 2.032935  [ 6464/19873]\n",
      "loss: 1.654115  [12864/19873]\n",
      "loss: 2.410090  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.1%, Avg loss: 2.293439 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.804366  [   64/19873]\n",
      "loss: 2.017061  [ 6464/19873]\n",
      "loss: 1.641838  [12864/19873]\n",
      "loss: 2.394660  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.2%, Avg loss: 2.283302 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.791428  [   64/19873]\n",
      "loss: 2.001213  [ 6464/19873]\n",
      "loss: 1.629717  [12864/19873]\n",
      "loss: 2.379361  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.5%, Avg loss: 2.273373 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.778751  [   64/19873]\n",
      "loss: 1.985323  [ 6464/19873]\n",
      "loss: 1.617744  [12864/19873]\n",
      "loss: 2.364108  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.9%, Avg loss: 2.263652 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.766497  [   64/19873]\n",
      "loss: 1.969324  [ 6464/19873]\n",
      "loss: 1.605917  [12864/19873]\n",
      "loss: 2.349032  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 27.0%, Avg loss: 2.254139 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.754501  [   64/19873]\n",
      "loss: 1.953310  [ 6464/19873]\n",
      "loss: 1.594299  [12864/19873]\n",
      "loss: 2.334045  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 27.5%, Avg loss: 2.244827 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 1.742792  [   64/19873]\n",
      "loss: 1.937234  [ 6464/19873]\n",
      "loss: 1.582835  [12864/19873]\n",
      "loss: 2.319221  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 27.7%, Avg loss: 2.235639 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.731247  [   64/19873]\n",
      "loss: 1.921205  [ 6464/19873]\n",
      "loss: 1.571578  [12864/19873]\n",
      "loss: 2.304525  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.4%, Avg loss: 2.226640 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.719907  [   64/19873]\n",
      "loss: 1.905106  [ 6464/19873]\n",
      "loss: 1.560471  [12864/19873]\n",
      "loss: 2.289937  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.5%, Avg loss: 2.217893 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.708828  [   64/19873]\n",
      "loss: 1.889028  [ 6464/19873]\n",
      "loss: 1.549599  [12864/19873]\n",
      "loss: 2.275442  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.9%, Avg loss: 2.209240 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1.697914  [   64/19873]\n",
      "loss: 1.873014  [ 6464/19873]\n",
      "loss: 1.538898  [12864/19873]\n",
      "loss: 2.261028  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 29.2%, Avg loss: 2.200821 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.687141  [   64/19873]\n",
      "loss: 1.856918  [ 6464/19873]\n",
      "loss: 1.528336  [12864/19873]\n",
      "loss: 2.246813  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 29.5%, Avg loss: 2.192552 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 1.676650  [   64/19873]\n",
      "loss: 1.840930  [ 6464/19873]\n",
      "loss: 1.517974  [12864/19873]\n",
      "loss: 2.232686  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 29.6%, Avg loss: 2.184472 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1.666319  [   64/19873]\n",
      "loss: 1.824897  [ 6464/19873]\n",
      "loss: 1.507766  [12864/19873]\n",
      "loss: 2.218652  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.0%, Avg loss: 2.176571 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.656162  [   64/19873]\n",
      "loss: 1.808956  [ 6464/19873]\n",
      "loss: 1.497719  [12864/19873]\n",
      "loss: 2.204711  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.168796 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1.646108  [   64/19873]\n",
      "loss: 1.792944  [ 6464/19873]\n",
      "loss: 1.487823  [12864/19873]\n",
      "loss: 2.190853  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.5%, Avg loss: 2.161232 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.636199  [   64/19873]\n",
      "loss: 1.777005  [ 6464/19873]\n",
      "loss: 1.478114  [12864/19873]\n",
      "loss: 2.177066  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.6%, Avg loss: 2.153847 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.626377  [   64/19873]\n",
      "loss: 1.761114  [ 6464/19873]\n",
      "loss: 1.468595  [12864/19873]\n",
      "loss: 2.163434  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.8%, Avg loss: 2.146601 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 1.616681  [   64/19873]\n",
      "loss: 1.745208  [ 6464/19873]\n",
      "loss: 1.459191  [12864/19873]\n",
      "loss: 2.149889  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.3%, Avg loss: 2.139568 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 1.607141  [   64/19873]\n",
      "loss: 1.729413  [ 6464/19873]\n",
      "loss: 1.449952  [12864/19873]\n",
      "loss: 2.136418  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.6%, Avg loss: 2.132643 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 1.597645  [   64/19873]\n",
      "loss: 1.713666  [ 6464/19873]\n",
      "loss: 1.440857  [12864/19873]\n",
      "loss: 2.123090  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.7%, Avg loss: 2.125890 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 1.588262  [   64/19873]\n",
      "loss: 1.698008  [ 6464/19873]\n",
      "loss: 1.431887  [12864/19873]\n",
      "loss: 2.109876  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.119351 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 1.578969  [   64/19873]\n",
      "loss: 1.682447  [ 6464/19873]\n",
      "loss: 1.423060  [12864/19873]\n",
      "loss: 2.096859  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 2.112912 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 1.569713  [   64/19873]\n",
      "loss: 1.667018  [ 6464/19873]\n",
      "loss: 1.414347  [12864/19873]\n",
      "loss: 2.083835  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.5%, Avg loss: 2.106654 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 1.560619  [   64/19873]\n",
      "loss: 1.651729  [ 6464/19873]\n",
      "loss: 1.405790  [12864/19873]\n",
      "loss: 2.070986  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.100540 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 1.551573  [   64/19873]\n",
      "loss: 1.636538  [ 6464/19873]\n",
      "loss: 1.397381  [12864/19873]\n",
      "loss: 2.058181  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 2.094550 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 1.542595  [   64/19873]\n",
      "loss: 1.621482  [ 6464/19873]\n",
      "loss: 1.389077  [12864/19873]\n",
      "loss: 2.045567  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 2.088711 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 1.533710  [   64/19873]\n",
      "loss: 1.606539  [ 6464/19873]\n",
      "loss: 1.380915  [12864/19873]\n",
      "loss: 2.033046  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 2.083030 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 1.524928  [   64/19873]\n",
      "loss: 1.591791  [ 6464/19873]\n",
      "loss: 1.372871  [12864/19873]\n",
      "loss: 2.020605  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 2.077502 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies_2 = []\n",
    "losses_2 = []\n",
    "model = NeuralNetwork(size=2048)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "for e in range(100):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    c, l = test(val_dataloader, model, loss_fn)\n",
    "    accuracies_2.append(c)\n",
    "    losses_2.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if need to load for training here\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=512):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(71636, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork(size=2048)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.507256  [   64/19873]\n",
      "loss: 0.356133  [ 6464/19873]\n",
      "loss: 0.417749  [12864/19873]\n",
      "loss: 0.520651  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.925979 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.505383  [   64/19873]\n",
      "loss: 0.353872  [ 6464/19873]\n",
      "loss: 0.415168  [12864/19873]\n",
      "loss: 0.517003  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.926733 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.503498  [   64/19873]\n",
      "loss: 0.351658  [ 6464/19873]\n",
      "loss: 0.412612  [12864/19873]\n",
      "loss: 0.513393  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.927503 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.501624  [   64/19873]\n",
      "loss: 0.349443  [ 6464/19873]\n",
      "loss: 0.410064  [12864/19873]\n",
      "loss: 0.509786  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.928256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.499762  [   64/19873]\n",
      "loss: 0.347252  [ 6464/19873]\n",
      "loss: 0.407554  [12864/19873]\n",
      "loss: 0.506229  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.929060 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.497901  [   64/19873]\n",
      "loss: 0.345081  [ 6464/19873]\n",
      "loss: 0.405033  [12864/19873]\n",
      "loss: 0.502687  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.929794 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.496048  [   64/19873]\n",
      "loss: 0.342921  [ 6464/19873]\n",
      "loss: 0.402546  [12864/19873]\n",
      "loss: 0.499159  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.930601 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.494200  [   64/19873]\n",
      "loss: 0.340775  [ 6464/19873]\n",
      "loss: 0.400077  [12864/19873]\n",
      "loss: 0.495672  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.931366 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.492366  [   64/19873]\n",
      "loss: 0.338666  [ 6464/19873]\n",
      "loss: 0.397625  [12864/19873]\n",
      "loss: 0.492197  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.932200 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.490524  [   64/19873]\n",
      "loss: 0.336553  [ 6464/19873]\n",
      "loss: 0.395189  [12864/19873]\n",
      "loss: 0.488757  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.932984 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.488690  [   64/19873]\n",
      "loss: 0.334463  [ 6464/19873]\n",
      "loss: 0.392772  [12864/19873]\n",
      "loss: 0.485337  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.933799 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.486875  [   64/19873]\n",
      "loss: 0.332391  [ 6464/19873]\n",
      "loss: 0.390376  [12864/19873]\n",
      "loss: 0.481944  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.934629 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.485057  [   64/19873]\n",
      "loss: 0.330349  [ 6464/19873]\n",
      "loss: 0.387998  [12864/19873]\n",
      "loss: 0.478578  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.935458 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.483250  [   64/19873]\n",
      "loss: 0.328298  [ 6464/19873]\n",
      "loss: 0.385644  [12864/19873]\n",
      "loss: 0.475232  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.936246 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.481436  [   64/19873]\n",
      "loss: 0.326275  [ 6464/19873]\n",
      "loss: 0.383302  [12864/19873]\n",
      "loss: 0.471896  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.937128 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.479639  [   64/19873]\n",
      "loss: 0.324268  [ 6464/19873]\n",
      "loss: 0.380981  [12864/19873]\n",
      "loss: 0.468599  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.937939 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.477833  [   64/19873]\n",
      "loss: 0.322282  [ 6464/19873]\n",
      "loss: 0.378677  [12864/19873]\n",
      "loss: 0.465326  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.938759 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.476051  [   64/19873]\n",
      "loss: 0.320300  [ 6464/19873]\n",
      "loss: 0.376388  [12864/19873]\n",
      "loss: 0.462065  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.939628 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.474278  [   64/19873]\n",
      "loss: 0.318344  [ 6464/19873]\n",
      "loss: 0.374120  [12864/19873]\n",
      "loss: 0.458827  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.940575 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.472490  [   64/19873]\n",
      "loss: 0.316406  [ 6464/19873]\n",
      "loss: 0.371867  [12864/19873]\n",
      "loss: 0.455624  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.941389 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.470723  [   64/19873]\n",
      "loss: 0.314481  [ 6464/19873]\n",
      "loss: 0.369629  [12864/19873]\n",
      "loss: 0.452434  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.942299 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.468956  [   64/19873]\n",
      "loss: 0.312579  [ 6464/19873]\n",
      "loss: 0.367419  [12864/19873]\n",
      "loss: 0.449286  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.943143 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.467197  [   64/19873]\n",
      "loss: 0.310697  [ 6464/19873]\n",
      "loss: 0.365205  [12864/19873]\n",
      "loss: 0.446141  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.944066 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.465445  [   64/19873]\n",
      "loss: 0.308813  [ 6464/19873]\n",
      "loss: 0.363023  [12864/19873]\n",
      "loss: 0.443024  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.944951 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.463691  [   64/19873]\n",
      "loss: 0.306955  [ 6464/19873]\n",
      "loss: 0.360850  [12864/19873]\n",
      "loss: 0.439924  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.945813 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.461949  [   64/19873]\n",
      "loss: 0.305112  [ 6464/19873]\n",
      "loss: 0.358697  [12864/19873]\n",
      "loss: 0.436851  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.946737 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.460210  [   64/19873]\n",
      "loss: 0.303279  [ 6464/19873]\n",
      "loss: 0.356556  [12864/19873]\n",
      "loss: 0.433808  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.947663 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.458465  [   64/19873]\n",
      "loss: 0.301462  [ 6464/19873]\n",
      "loss: 0.354443  [12864/19873]\n",
      "loss: 0.430787  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.948548 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.456737  [   64/19873]\n",
      "loss: 0.299672  [ 6464/19873]\n",
      "loss: 0.352329  [12864/19873]\n",
      "loss: 0.427790  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.949488 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.455004  [   64/19873]\n",
      "loss: 0.297896  [ 6464/19873]\n",
      "loss: 0.350246  [12864/19873]\n",
      "loss: 0.424811  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.950422 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.453275  [   64/19873]\n",
      "loss: 0.296133  [ 6464/19873]\n",
      "loss: 0.348172  [12864/19873]\n",
      "loss: 0.421851  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.951327 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.451554  [   64/19873]\n",
      "loss: 0.294382  [ 6464/19873]\n",
      "loss: 0.346118  [12864/19873]\n",
      "loss: 0.418916  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.952324 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.449840  [   64/19873]\n",
      "loss: 0.292652  [ 6464/19873]\n",
      "loss: 0.344083  [12864/19873]\n",
      "loss: 0.416017  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.953249 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.448130  [   64/19873]\n",
      "loss: 0.290937  [ 6464/19873]\n",
      "loss: 0.342058  [12864/19873]\n",
      "loss: 0.413129  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.954213 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.446417  [   64/19873]\n",
      "loss: 0.289238  [ 6464/19873]\n",
      "loss: 0.340046  [12864/19873]\n",
      "loss: 0.410262  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.955218 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.444719  [   64/19873]\n",
      "loss: 0.287544  [ 6464/19873]\n",
      "loss: 0.338046  [12864/19873]\n",
      "loss: 0.407426  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.956193 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.443014  [   64/19873]\n",
      "loss: 0.285881  [ 6464/19873]\n",
      "loss: 0.336073  [12864/19873]\n",
      "loss: 0.404610  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.957143 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.441323  [   64/19873]\n",
      "loss: 0.284215  [ 6464/19873]\n",
      "loss: 0.334114  [12864/19873]\n",
      "loss: 0.401814  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.958162 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.439626  [   64/19873]\n",
      "loss: 0.282571  [ 6464/19873]\n",
      "loss: 0.332161  [12864/19873]\n",
      "loss: 0.399048  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.959094 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.437932  [   64/19873]\n",
      "loss: 0.280946  [ 6464/19873]\n",
      "loss: 0.330234  [12864/19873]\n",
      "loss: 0.396304  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 1.960093 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.436247  [   64/19873]\n",
      "loss: 0.279337  [ 6464/19873]\n",
      "loss: 0.328315  [12864/19873]\n",
      "loss: 0.393568  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 1.961094 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.434554  [   64/19873]\n",
      "loss: 0.277731  [ 6464/19873]\n",
      "loss: 0.326408  [12864/19873]\n",
      "loss: 0.390875  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 1.962107 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.432874  [   64/19873]\n",
      "loss: 0.276158  [ 6464/19873]\n",
      "loss: 0.324517  [12864/19873]\n",
      "loss: 0.388187  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 1.963064 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.431183  [   64/19873]\n",
      "loss: 0.274576  [ 6464/19873]\n",
      "loss: 0.322641  [12864/19873]\n",
      "loss: 0.385530  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 1.964080 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.429508  [   64/19873]\n",
      "loss: 0.273035  [ 6464/19873]\n",
      "loss: 0.320774  [12864/19873]\n",
      "loss: 0.382893  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.965065 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.427833  [   64/19873]\n",
      "loss: 0.271498  [ 6464/19873]\n",
      "loss: 0.318929  [12864/19873]\n",
      "loss: 0.380268  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 1.966049 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.426166  [   64/19873]\n",
      "loss: 0.269975  [ 6464/19873]\n",
      "loss: 0.317094  [12864/19873]\n",
      "loss: 0.377681  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 1.967029 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.424506  [   64/19873]\n",
      "loss: 0.268469  [ 6464/19873]\n",
      "loss: 0.315284  [12864/19873]\n",
      "loss: 0.375106  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.968020 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.422840  [   64/19873]\n",
      "loss: 0.266975  [ 6464/19873]\n",
      "loss: 0.313475  [12864/19873]\n",
      "loss: 0.372562  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 1.969072 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.421182  [   64/19873]\n",
      "loss: 0.265509  [ 6464/19873]\n",
      "loss: 0.311685  [12864/19873]\n",
      "loss: 0.370032  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 1.970028 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.419534  [   64/19873]\n",
      "loss: 0.264023  [ 6464/19873]\n",
      "loss: 0.309916  [12864/19873]\n",
      "loss: 0.367526  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 1.971090 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.417880  [   64/19873]\n",
      "loss: 0.262585  [ 6464/19873]\n",
      "loss: 0.308153  [12864/19873]\n",
      "loss: 0.365037  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 1.972137 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.416232  [   64/19873]\n",
      "loss: 0.261147  [ 6464/19873]\n",
      "loss: 0.306405  [12864/19873]\n",
      "loss: 0.362571  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 1.973195 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.414591  [   64/19873]\n",
      "loss: 0.259730  [ 6464/19873]\n",
      "loss: 0.304678  [12864/19873]\n",
      "loss: 0.360135  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 1.974176 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.412950  [   64/19873]\n",
      "loss: 0.258319  [ 6464/19873]\n",
      "loss: 0.302949  [12864/19873]\n",
      "loss: 0.357702  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 1.975288 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.411315  [   64/19873]\n",
      "loss: 0.256916  [ 6464/19873]\n",
      "loss: 0.301245  [12864/19873]\n",
      "loss: 0.355297  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 1.976368 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.409681  [   64/19873]\n",
      "loss: 0.255531  [ 6464/19873]\n",
      "loss: 0.299551  [12864/19873]\n",
      "loss: 0.352917  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 1.977403 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.408058  [   64/19873]\n",
      "loss: 0.254167  [ 6464/19873]\n",
      "loss: 0.297867  [12864/19873]\n",
      "loss: 0.350550  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 1.978410 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.406423  [   64/19873]\n",
      "loss: 0.252806  [ 6464/19873]\n",
      "loss: 0.296212  [12864/19873]\n",
      "loss: 0.348209  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.979537 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.404819  [   64/19873]\n",
      "loss: 0.251463  [ 6464/19873]\n",
      "loss: 0.294546  [12864/19873]\n",
      "loss: 0.345903  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.980556 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.403195  [   64/19873]\n",
      "loss: 0.250130  [ 6464/19873]\n",
      "loss: 0.292905  [12864/19873]\n",
      "loss: 0.343592  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.981633 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.401589  [   64/19873]\n",
      "loss: 0.248812  [ 6464/19873]\n",
      "loss: 0.291269  [12864/19873]\n",
      "loss: 0.341306  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.982713 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.399976  [   64/19873]\n",
      "loss: 0.247500  [ 6464/19873]\n",
      "loss: 0.289665  [12864/19873]\n",
      "loss: 0.339050  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.983788 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.398371  [   64/19873]\n",
      "loss: 0.246218  [ 6464/19873]\n",
      "loss: 0.288053  [12864/19873]\n",
      "loss: 0.336808  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.984909 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.396776  [   64/19873]\n",
      "loss: 0.244926  [ 6464/19873]\n",
      "loss: 0.286467  [12864/19873]\n",
      "loss: 0.334593  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.985987 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.395174  [   64/19873]\n",
      "loss: 0.243660  [ 6464/19873]\n",
      "loss: 0.284893  [12864/19873]\n",
      "loss: 0.332386  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.987058 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.393575  [   64/19873]\n",
      "loss: 0.242405  [ 6464/19873]\n",
      "loss: 0.283321  [12864/19873]\n",
      "loss: 0.330208  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.988162 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.391992  [   64/19873]\n",
      "loss: 0.241162  [ 6464/19873]\n",
      "loss: 0.281773  [12864/19873]\n",
      "loss: 0.328050  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.989211 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.390404  [   64/19873]\n",
      "loss: 0.239914  [ 6464/19873]\n",
      "loss: 0.280228  [12864/19873]\n",
      "loss: 0.325909  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 1.990358 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.388831  [   64/19873]\n",
      "loss: 0.238701  [ 6464/19873]\n",
      "loss: 0.278699  [12864/19873]\n",
      "loss: 0.323795  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 1.991454 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.387255  [   64/19873]\n",
      "loss: 0.237495  [ 6464/19873]\n",
      "loss: 0.277184  [12864/19873]\n",
      "loss: 0.321705  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 1.992557 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.385687  [   64/19873]\n",
      "loss: 0.236297  [ 6464/19873]\n",
      "loss: 0.275679  [12864/19873]\n",
      "loss: 0.319621  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.993697 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.384132  [   64/19873]\n",
      "loss: 0.235104  [ 6464/19873]\n",
      "loss: 0.274188  [12864/19873]\n",
      "loss: 0.317575  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.994743 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.382552  [   64/19873]\n",
      "loss: 0.233936  [ 6464/19873]\n",
      "loss: 0.272705  [12864/19873]\n",
      "loss: 0.315540  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.995886 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.381006  [   64/19873]\n",
      "loss: 0.232781  [ 6464/19873]\n",
      "loss: 0.271229  [12864/19873]\n",
      "loss: 0.313524  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.996983 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.379447  [   64/19873]\n",
      "loss: 0.231619  [ 6464/19873]\n",
      "loss: 0.269770  [12864/19873]\n",
      "loss: 0.311526  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.998107 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.377898  [   64/19873]\n",
      "loss: 0.230483  [ 6464/19873]\n",
      "loss: 0.268316  [12864/19873]\n",
      "loss: 0.309550  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.999222 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.376367  [   64/19873]\n",
      "loss: 0.229365  [ 6464/19873]\n",
      "loss: 0.266882  [12864/19873]\n",
      "loss: 0.307587  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.000354 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.374828  [   64/19873]\n",
      "loss: 0.228239  [ 6464/19873]\n",
      "loss: 0.265448  [12864/19873]\n",
      "loss: 0.305633  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.001433 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.373292  [   64/19873]\n",
      "loss: 0.227150  [ 6464/19873]\n",
      "loss: 0.264031  [12864/19873]\n",
      "loss: 0.303710  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.002568 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.371771  [   64/19873]\n",
      "loss: 0.226028  [ 6464/19873]\n",
      "loss: 0.262629  [12864/19873]\n",
      "loss: 0.301796  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.003751 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.370250  [   64/19873]\n",
      "loss: 0.224964  [ 6464/19873]\n",
      "loss: 0.261230  [12864/19873]\n",
      "loss: 0.299907  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.004888 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.368732  [   64/19873]\n",
      "loss: 0.223882  [ 6464/19873]\n",
      "loss: 0.259840  [12864/19873]\n",
      "loss: 0.298032  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.005999 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.367228  [   64/19873]\n",
      "loss: 0.222817  [ 6464/19873]\n",
      "loss: 0.258470  [12864/19873]\n",
      "loss: 0.296166  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.007095 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.365730  [   64/19873]\n",
      "loss: 0.221764  [ 6464/19873]\n",
      "loss: 0.257102  [12864/19873]\n",
      "loss: 0.294327  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.008280 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.364237  [   64/19873]\n",
      "loss: 0.220732  [ 6464/19873]\n",
      "loss: 0.255752  [12864/19873]\n",
      "loss: 0.292503  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.009383 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.362737  [   64/19873]\n",
      "loss: 0.219680  [ 6464/19873]\n",
      "loss: 0.254403  [12864/19873]\n",
      "loss: 0.290707  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.010543 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.361254  [   64/19873]\n",
      "loss: 0.218662  [ 6464/19873]\n",
      "loss: 0.253075  [12864/19873]\n",
      "loss: 0.288915  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.011652 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.359774  [   64/19873]\n",
      "loss: 0.217655  [ 6464/19873]\n",
      "loss: 0.251751  [12864/19873]\n",
      "loss: 0.287153  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.012754 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.358306  [   64/19873]\n",
      "loss: 0.216635  [ 6464/19873]\n",
      "loss: 0.250446  [12864/19873]\n",
      "loss: 0.285397  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.013894 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.356839  [   64/19873]\n",
      "loss: 0.215652  [ 6464/19873]\n",
      "loss: 0.249140  [12864/19873]\n",
      "loss: 0.283666  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.015023 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.355372  [   64/19873]\n",
      "loss: 0.214656  [ 6464/19873]\n",
      "loss: 0.247850  [12864/19873]\n",
      "loss: 0.281944  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.016217 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.353938  [   64/19873]\n",
      "loss: 0.213685  [ 6464/19873]\n",
      "loss: 0.246560  [12864/19873]\n",
      "loss: 0.280240  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.017307 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.352482  [   64/19873]\n",
      "loss: 0.212712  [ 6464/19873]\n",
      "loss: 0.245292  [12864/19873]\n",
      "loss: 0.278558  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.018521 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.351050  [   64/19873]\n",
      "loss: 0.211754  [ 6464/19873]\n",
      "loss: 0.244036  [12864/19873]\n",
      "loss: 0.276885  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.019671 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.349605  [   64/19873]\n",
      "loss: 0.210801  [ 6464/19873]\n",
      "loss: 0.242773  [12864/19873]\n",
      "loss: 0.275235  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.020780 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.348178  [   64/19873]\n",
      "loss: 0.209866  [ 6464/19873]\n",
      "loss: 0.241528  [12864/19873]\n",
      "loss: 0.273597  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.021895 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.346758  [   64/19873]\n",
      "loss: 0.208937  [ 6464/19873]\n",
      "loss: 0.240287  [12864/19873]\n",
      "loss: 0.271980  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.023071 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.345340  [   64/19873]\n",
      "loss: 0.208006  [ 6464/19873]\n",
      "loss: 0.239071  [12864/19873]\n",
      "loss: 0.270372  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.024242 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.343930  [   64/19873]\n",
      "loss: 0.207097  [ 6464/19873]\n",
      "loss: 0.237849  [12864/19873]\n",
      "loss: 0.268778  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.025405 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "for e in range(100):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    c, l = test(val_dataloader, model, loss_fn)\n",
    "    accuracies_2.append(c)\n",
    "    losses_2.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAGwCAYAAAAZn0mrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAYxBJREFUeJzt3Xd4U2X7B/BvZ8poC6WMlhYoIqMU2aMMkSkgIAgyVIYCigxBUF8LKEst+gqvIluRIUpRAUVBlkABARkC1hYBBaFg2dCWVWh7fn/cvzTdJG2a5yT5fq7rXOckOUnuY7C584z7cdE0TQMRERER6Y6r6gCIiIiIKHdM1IiIiIh0iokaERERkU4xUSMiIiLSKSZqRERERDrFRI2IiIhIp5ioEREREemUu+oAbC01NRWHDx9G+fLl4erKPJWIiMgepKen4+LFi6hfvz7c3Z0nfXGeK/1/hw8fRpMmTVSHQURERAWwf/9+NG7cWHUYNuN0iVr58uUByAcdEBCgOBoiIiIyR0JCApo0aZLxPe4snC5RM3Z3BgQEICgoSHE0REREZAlnG7bkXFdLREREZEeYqBERERHpFBM1IiIiIp1iokZERESkU0zUiIiIiHSKiRoRERGRTjFRIyIiItIpJmpEREREOsVEjYiIiEinmKgRERER6RQTNSIiIiKdYqJGREREpFNM1Kzp/Hngt99UR0FERFSkNA24eRNISwPi44EzZ4Dr11VH5ZiYqFnL3r1AWBjQqxeQnKw6GiIiIotpGrBvHxAVBRw/DqSnZ338wgXgiy+AqlUBb2+gZEmgUiWgShVgxgwlITs8d9UBOIzatYFSpYB//gHGjQM+/VR1RERERLlKSQEOHwbOnQN++km+wmrVAqZNk0TNyM8PmDABqF8f2LoVmDkTuHfP9Pjdu4C7u2kj6+N/Vmvx8QGWLgXatAE++wzo0QN44gnVURERkZM7dAjYtg24dk1uX7wIrF8PXLqU+/leXkCdOsAff8hzXnst6+OPPAJ07w689BJw+zbw0EOAm1vRXoMzY6JmTa1bA6++CsyaBQwdCsTFAaVLq46KiIjs1K1bMvbLx0c2c6WkAJs3Azt2yFdSbsqUAcqXl6+uI0eAK1eAZs2kCzMwELh/H1iyBJg3T1rOqlcHBg6UET4uLta4uiI2f75s//wjt2vXBt5+G+jc+cHP/eUX+Q8TFib/cRRy0TRNUxqBjZ07dw7BwcGIj49HUFCQ9d/g7l1pI/7zT2D8eODDD63/HkRE5HDWrQMmTpTxXiVKALGx8nvfOE6sRg1pzbp/H/j7b6BlS6BuXRlXduoU8O+/0kp2/rx8FZ06ZXrtrl2l5cvFRRK+pk2BDh0ADw8ll1ogFn9///CDNPVVqya3ly0D/vtf6fOtXTvv5yUmAg0ayPMuXmSiZmtFnqgBwMaNkrF7ekrCFhJSNO9DRER248IFYM0amS2Zng789Zfsvb2lm3Hbttyf5+EhyZmlypaVdoPBg4H+/QsVui4Yv7/j4uJQsWLFjPsNBgMMBoN5L+LnJ8nakCF5n9OvH/Dww5Lkffed8kSNXZ9F4fHHgfbtZeTlhx8Cc+eqjoiIiGxg40bg9deBGzeAjh2lJevzz6UbMvMg/Lw0bgw89xyQmiotYI0bSzfklSvAgQMyE9PNTbosN26U9wHkdrVq0lpWsaKc362bY46+CQ0NzXJ78uTJmDJlSv5PSksDvvlG+pLDw/M+b8kSaa5csQJ4553CB2sFbFErKtu3A23bAsWKSYGZsmWL7r2IiMimNE1ayDRNKjLFxsqwprzGgxk1awbUrCnHgYGAq6u0sNWsCVSoIHPQOHsydwVqUYuJkcTs7l2pJfLVV0CXLrmfe/Kk9Cfv2iUD8qZMYYuaQ3vsMaBhQ5lu8+mnMr+ZiIhw7ZokI5kHx9+/D5w+DRgMQOXK5r2OpklylJgoI0xcXKRnq3x5efzkSfmOrVVL7jtwQGYp+vtLo4mHh3SA3LwptcpLlpSuyLt3ZTxYcjJw7Ji0YHXpIr+3NQ34+WdgzBgZP5ab0aOlJW3SJODECel6HDJEErMyZQrzX44AwNvbGz7mzqyoUUP+Edy4AaxeDQwaBERHA9la5ZCWBjzzDDB1qiRpOsIWtaK0dCnw/PPS1338uJ1MkyEiKhonTgD/+Y8Mmtc06Zbz85Pt998lQQIksVm8WFqbAPmOjY6W8V3798u4qwMH5PETJ7K+h4uLDDE6fTprPbDCKl0a8PWVGZiJiab3cnOT4cihoVJK84UXTOPBNE02V5aWtwqrfH+3by99ygsXZr3/xg35kDPXGUlPlw/QzU36rtu2LXDshcEWtaLUqxcwcqT8rNu3L/9+cSIiB6JpMlj+9m2ZqbhkCTBihJSNMLp2zVTbC5D6XXfvym/cPXukBSo9XZKyO3dM5/35p+nYw0O+dy9dkve8fh1YuVIec3eXemC//y4NJjVrSqvZlSvSynbpknR6eHrK7MmkJEmqiheXRM/LS6ozxMdLy5pxiaTixSUhmzpVksy8uLjw97nuaFrWf4RGPj7STZrZvHkyw+Pbb5VOCmSiVpS8vYHevYHly4Evv2SiRkQOR9OAs2dljFaZMsDOndKNuWqVJEiA3H/1qhx37Aj873/S/XjlilTGv37dVA3h66+lVteJE1lby6pXlxEljz4q1RWaNJGGjnr1JFEz2r4d+P57ea3evWXc140bkjD5+uaMPyFB4vP0zPsa79+XBpXixeX1goLkzzvp3IQJUoEhOFj6saOipLDcxo3yeESE1DJZvlwy9LCwrM8vV86UrSvERK2o9ekj/wjWrQM++YQ/r4jIIfz0kzQ47N+fd4V7g0G+/65elaRq0iRg8mTTn8Fy5XIOFerbF2jXTl7XuGxy5cpS98v4vGefzTuuNm1ky6xUqbzPDwjI+zEjDw8uNGOXLl4EBgyQbNzXV5p2N26UAnKA3H/2rNoYzcAxakXtzh35uXbnjvwMrFev6N+TiKiQfvxRJrx5eUkrWe3acnzggMx2NLaQAdLF+PDDcn9YmKnVaeJEeSwuTrodHbFUBNmOzb+/dYItakWtWDHJ3tetkyrJTNSISGeSk4Hp06WB4a+/ZAZk9ppf2YfvAMCLL8p8qbp15U9dXjjqg6jgmKjZQrdukqht3Ai89ZbqaIiIAMgA+WXLZDblyZNZH3N3lyTM3V26HX//XZY1atJEuiBLlwaaN1cTN5EzYaJmC8YpvcYiPsWLq42HiJxWSoqML5szR+qBGfn5AU8/LYP2u3eX8WOWLAJOREWDiZothITImh7nz0uZDkW1WIjIed29C4waBXzxhalb09VVCrP27Cl7f3+1MRJRTkzUbMHFBWjdWpauiI5mokZENnH3rsyeXLpUykucPy/3ly8v3ZrDh5u/CgARqaG8XvK8edLg5OUlKy7t2pX3uTt2mAoIZt4yFz/UrUcflX10tNo4iMjhxccDw4bJhPPWraXY7Pnz0r25aZNMGoiMZJJGZA+UtqitWgWMHSvJWosWsqJD584ylbtSpbyfd/x41rETdrHeecuWsj94UEpkZ16mgojICi5dkkXB586V9SsB6c7s0gV47jmZfVmypNoYicgyShO1WbNkodqhQ+X2Rx/Jr7358+XXXl7Klcu/gGFmKSkpSMm0XESysYKirdWsKZMIbt2Sctu1aqmJg4gchqZJ69n+/VLh/8MPTd2bzZsD778vP4JZZ5vIfinr+rx3T9ZY69gx6/0dO8oab/mpX1+qSbdrJ8uF5CcyMhK+vr4ZW2j2Mti24uYmgQNy4UREBZCcLMNApk6VYSOVK8tszVdflSStZk1g7VoZRtKyJZM0InunrEXtyhXpASxfPuv95ctLdevcBAQAixbJWLaUFJm91K6d/NEyDgHLLiIiAuPGjcu4ff78eXXJWsOGwC+/SPfnc8+piYGI7M6//8rQkNWrZWhI5vVk3N1lZZyHHpKkbeJE83sciEj/lM/6zP5rT9Py/gVYo4ZsRuHh0uz/4Yd5J2oGgwEGgyHjdlJSUiEjLoSGDWXPFjUiyoOmAVu2yLZ/P3D5shSjTU01nVOpEtC4MfDkk0CvXizNSOTIlCVq/v7SG5i99ezSpZytbPlp1gxYscK6sRUZY6J2+DCQni5FjIjIqaWkyCzMSpXkT8InnwBjxuQ8r1UrKafRtq2spUlEzkFZoubpKXnLli1SbNFoyxb5lWiuw4elS9QuVK8OeHjIhIL4eM6NJ3JiKSnAf/8rk6iuXgUMBvm7aJyt2b+/jNmtXFnqZVevrjRcIlJEadfnuHHAgAFAo0bSjbloEXD2rPxqBICICBkcu3y53P7oI6BKFaB2bZmMsGKFjNlYvVrVFVjIw0P+2sbGykATJmpETkfTpJziyy+bakC6uEjiZpygPmyYjEnjRAAiUpqo9e0rvySnTZOm/7AwYMMGU/6SkCCJm9G9e8Brr0nyVqyYJGzr10uNILtRq5YkaseOSdE4InIa587JDM19++R2+fLAzJlA794yYSA9XVrWgoLUxklE+qF8MsGIEbLlZunSrLffeEM2u2accRoXpzYOIrKpK1eA9u2lYLfBALzwAvDuu0Dp0vJ4SIja+IhIn5Qnak7HmKgdO6Y2DiKyGU0DXnpJkrTgYGDnThnGQUT0IEzUbM24IoGxGBIHoRA5rP37gR9+kLFoa9ZIzbPvv2eSRkTmY6Jma9Wryxz8GzekNondTFklogfRNGD3blkxZd8+YOPGrMVp33vPtEAJEZE5mKjZmpeXlBA/eVK6P5moETmEY8ekpMbRo1nv79VLfp89/TSTNCKyHBM1FWrVkkQtLk6qVxKR3UpKksXP58yR4xIlgO7dpRj3Y4/J8k5ERAXFRE2F0FBg3TrO/CSyU5oG/POPjEGbNs30v3LLllLXsVw5peERkQNhoqYCZ34S2a20NKkBmbnQdmAgMHu2rKrizr+qRGRF/JOiQuaZn0RkF9LSgK1bpb7j6tWyVnGDBrKqyhtvyDJPRETWxkRNhZo1ZX/pkizNUKaM2niIKFeaBvz8M/D557Ls07//mh5bsQLo109dbETkHJioqVCypKyTdeaMtKq1aqU6IiLK5s4dYORIYMkS032lSskszl69uAIcEdkGEzVVQkOZqBHpwL//Al98IWsLly0LFC8OHDgAbN4sDd6ursDw4ZKcNWsmjxMR2QoTNVVCQ4GffuI4NSIbundPVgjYv1/WGD53TlrG7t7N/fxKlYBFi4DHH7dtnERERkzUVKldW/axsWrjIHIC9+8DH30EzJolC4IAMkszLU2OGzeWBdNPn5Yuz0aNZJLAY4/JpAEiIlWYqKliLNHBFjWiIpGaCqSkyFJOr74KxMTI/RUqAFWrAnv2yO169WSiQLFiykIlIsoTEzVVjCU6EhKA69eB0qXVxkPkILZuBd58EzhyxNRiBsjk6g8+AJ57DvDwkKWeYmOBTp2YpBGRfjFRU8XHBwgOBuLjpVWtRQvVERHZvWXLgCFDciZoTz0lC6L7+5vur1dPNiIiPXNVHYBTCwuTffZVnInIIocOAePGAc8/L0nagAEyqToxEbh8WSYEZE7SiMgJzJ8vi+36+MgWHi6T+PKyZg3QoYNM/zaev2mT7eLNAxM1lRo0kP2hQ2rjILJT6enA+PEyGeB//5MCtcOHS8tapUryt9bFRXWURKREUBAwYwZw8KBsbdvKOm95TeLbuVMStQ0b5Hu5TRugWzfg8GHbxp0Nuz5VathQ9kzUiCx25YqMRVu8WG737i1dnH37MjkjIkiSldm770or2759psoLmX30Udbb770HfP898MMPQP36RRbmgzBRU8nYohYbK4WcvLzUxkNkBy5flgkDw4YBt25JUrZsmXR3EpHjS05ORlJSUsZtg8EAg8GQ/5PS0oBvvpE/GuHh5r1RejqQnAz4+RUi2sJj16dKlSrJSOfUVFPtACICIJOhb97Met/06VJe45ln5O9t3brA2rVM0oicSWhoKHx9fTO2yMjIvE+OiZFlGw0GGRexdq2pPNaDzJwpf2j69LFO4AXEFjWVXFyk+3PzZlmzpnFj1RERKaNpUtvs119lxYD58wFvb+DLL6Wx+ZNP5AcxILdHjpThJ+78K0bkVOLi4lCxYsWM2/m2ptWoIbV6btwAVq8GBg2SwokPStZWrgSmTJGuz3LlrBF2gfFPnGrh4ZKo7dola9oQOSFNA0aPBubOzXr/3btAx45Z74uMlLFpROScvL294ePjY97Jnp5AtWpy3KiRNIp8/DGwcGHez1m1Sur8fPONLFmiGLs+VWvTRvY7dsi3FZGT0TRJvObOlUbmnj2BF18EVqyQchve3lKQduhQmXzFJI2ICkzTZMmSvKxcCQweDHz1FfDEEzYLKz9sUVOtaVPpO79wATh+HKhZU3VEREXu/n1gwQLgxAmZHLBqldy/YIEkaUbPPgt89pmM6WUXJxFZZMIEoHNnKS6fnAxERUmjyMaN8nhEBHD+PLB8udxeuRIYOFBa3Jo1My0MXKwY4Our5BIAJmrqeXlJ9+eOHbIxUSMHd+YM0KOHDBvJbObMrEmakaurbEREFrl4UWYaJSRIovXII5KkdeggjyckAGfPms5fuFAm940cKZvRoEHA0qU2DT0zJmp60LatKcsfPlx1NERFZu9eSdIuXZIJz/36SXfnwIGcS0NEVmYsspiX7MnXjh1FFUmhMFHTg65dgbfflkkFt28DxYurjojI6g4elCGZKSmyxua6ddIjQUREeWOHgh7Uqyc11e7ckUqeRA7m1CkpRZSSAjz+uExyZpJGRPRgTNT0wMUF6N5djr/7TmkoRNakacDLLwMPPQScPi0zOJculfqTRET0YEzU9KJ3b9l/+61UQiayc5cvA/37y0xOQGpGzpkjKwsQEZF5mKjpRatWQEiITCFes0Z1NEQFdvcusGmTrGG8apXM2FyyRCZgDRyoOjoiIvvCRE0vXF2lyB4g32pEdujvv6XCTKdOUp6oZk1ZEsr4T5uIiCzDRE1PBg2S8Wrbt8uAHiI7cuwY0K6d1Enz9wdeeEHKcTRqpDoyIiL7xURNTypXlppqgKlSMpEd2L9f6jafOQM8/DAQEyMljEqVUh0ZEZF9Y6KmN88/L/slS4C0NLWxED2ApgE//SRdnYmJQPPmwC+/cMIAEZG1MFHTm549gdKlpWnip59UR0OUq5QUYNo0Sci6dAGuX5cWtU2bgLJlVUdHROQ4mKjpTfHipla1uXPVxkKUi1u3gPbtgcmTZSkoT09g3DhgyxbWRyMisjYmanr08suy37gR+OsvtbEQ/b/r14ExY2Qm5+7dssbxl19Kl+fMmUCJEqojJCJyPEzU9KhaNRn0AwDz56uNhQjAiRNAWBgwezZw7pxMEti4EXjmGcDLS3V0RESOi4maXo0cKfvPP5eF2olsSNOAhARZo/Pll6Ue87//AtWrA99/D5w9CzRrpjpKIiLH5646AMpD585AlSrAP/8AUVFSlIrIRkaOzNmYW6sWEB3NyQJERLbEFjW9cnMzjVWbO1eaOIiKWFoa8O67WZO01q2BtWuBgweZpBER2RoTNT174QXAYAB++03W4SEqAleuyGSAbt2AwEBg0iS5f9o06e7cvh3o0UMmJBMRkW2x61PP/P2Bfv2AZcukVY2DgsjKfvsNaNMGSEoy3efrC8yYAbz0kqxoRkRE6rBFTe+Mkwq+/lqKVhFZycmTwFNPSZIWFgb873+yNueFC8Dw4UzSiIj0gIma3jVuLNu9e7J4IpEVfPYZUKeOaW3OXbuAsWOl0ZblNoiI9IOJmj0wtqotWMD1P8kimgasWCE5/s2b8s/ntdeAYcNkGagOHYCtW7l4OhGRXjFRswd9+wJlykjxqh9/VB0N6dz167JPTZVlngYMAIYOlWovrVrJxAEAmDJF1uasVElVpERE9CBM1OyBlxcwZIgcf/aZ2lhIt5KTgUGDAD8/oG5dScCmTzc9fvWqjEErVkxK802ezHFoRER6x0TNXhgXav/pJ+DiRbWxkO688YZ0Xy5fLrd//11WFvD3B+bMkSGOH34oFV9iY6WRloiI9I/lOexFzZpA06ZST+3LL4Fx41RHRDpw/778c/jvf+X2Qw9JK1pqKlCxItC8uWlywPjx6uIkIqKCYYuaPRk8WPZLl3KlAieXlgasWgWEhJgaW//zH+Cvv4D+/WVcWtu2nMFJRGTvmKjZk759ZaWCmBjgyBHV0ZAicXGy7ma/fsD589Ll+fzzwNSpqiMjIiJrY6JmT0qXBp58Uo6XLVMbCylx4ADQvr0Uqy1dWiYEJCQAn38uOTwRETkWJmr2ZtAg2UdFsaaak1m+XIYpJiTISgInT0qJDXZvEhE5LiZq9qZDB6m/cPEiEB2tOhqykdOngREjZGhi376yUHqZMqqjIiKiosZEzd54eAC9eslxVJTaWMgmjh+X7s5bt4BHHwW++krKbhARkeNjomaPjEWwVq+W+gzksFJTgR49gFOnZIbn8uWAK/+vJSJyGvyTb49atwbKlQOuXZOFGslhLVsG/PmndHPu2wdUrqw6IiIisiXlidq8edJS4OUFNGwI7Npl3vN++QVwdwfq1SvS8PTJ3R14+mk5XrVKbSxUZBYuBF55RY4nTJDcnIiIzDR/PvDII4CPj2zh4bK6T36ioyUZ8fICqlYFFiywTaz5UJqorVoFjB0LTJwIHD4sC0Z37ixrj+cnMREYOBBo184mYeqTsftz7VogJUVtLGR1v/0GDB8O3L4t49NGjFAdERGRnQkKAmbMAA4elK1tWylxFRub+/mnTwNdukgycviw/EJ+5RUZZqSQi6apK3HftCnQoIEkvUa1asmYnMjIvJ/Xrx/w8MOAmxvw3XeW1X49d+4cgoODER8fj6CgoAJGrgPp6fKPMCFBfiF06qQ6IrKiTp2ATZuAPn2AlSs5Lo2IyCrf335+subekCE5H/vPf4B164Bjx0z3DR8OHD0K7N1bsPezAmV//u/dAw4dAjp2zHp/x47Anj15P2/JEuDvv6XQpzlSUlKQlJSUsSUnJxc8aD1xdTUVv/3uO6WhkHXt2CFJmru7/GBhkkZEZJKcnJzlez3FnF6ltDSplHDrlnSB5mbv3pxJyeOPS2ucwol7yr4CrlyR/27ly2e9v3x54MKF3J9z8iTw5puyCLW7mcvJR0ZGwtfXN2MLDQ0tXOB60qOH7L//XlrYyO5pGhARIccvvihDJIiIyCQ0NDTL93pkfl1wMTFAyZKydMvw4TJcKK884MKF3JOS1FRJWhRR/lvdxSXrbU3LeR8gSd0zz8h6htWrm//6ERERSExMzNji4uIKF7CetGkjAyQvXAD271cdDVnBl1/K7M7ixYFJk1RHQ0SkP3FxcVm+1yOMv25zU6OGjI/atw94+WVZ3Se/PCC3pCS3+23IzHYp6/P3lzFm2VvPLl3KmdACQHKytD4ePgyMGiX3pafLf0N3d2DzZhknmJ3BYIAh0yKISUlJVrwKxTw9ZeBjVJT8SmjWTHVEVECaBmzYAIweLbcnTAACAtTGRESkR97e3vDx8THvZE9PoFo1OW7USBZM/vhjmVafXYUKuScl7u5Kl4JR1qLm6SkzYLdsyXr/li1A8+Y5z/fxkRbMI0dM2/DhpmS5adOij1mXevaU/dq1psyf7Iqmyb/lrl2BGzeAxo1lTCsREVmZpuVdKSE8PGdSsnmzJHgeHkUfWx6UtagBwLhxwIAB8t8gPBxYtEhKcwwfLo9HRADnz5uqsYeFZX1+uXJS6iT7/U6lUyfJek+elMqotWqpjogscOgQMH68lO5xdQXGjJHWNHPHYBIRUR4mTJCaX8HB0i0XFSWztTZulMczJxmAJB9z5khyMmyYTC5YvFim3iuk9Ougb1/g6lVg2jSpMhEWJt0/xurrCQkPrqnm9Hx8ZKzapk3A+vVM1OxEYiJw8aL8Dbl8WRKzTz8FBg9WHRkRkYO4eFFagxISAF9fKX67cSPQoYM8nj3JCAmRJOTVV4G5c4HAQGD2bNP62oooraOmgsPUUcts9mxpimnTBti2TXU0lI99++Sjyjz3o04d4McfgUqV1MVFRKR3Dvn9bQblsz7JCrp0kf3u3YAjTZZwMKtWAS1aZE3SiheX9TyZpBERUW6YqDmCatVkqYb794Gff1YdDeUiJgZ47jmZqfz008C//wLXrkmre/36qqMjIiK9YqLmKDp3lv2GDWrjoFxt3y41E1u1knGpAQFA6dJKZ3wTEZEdYKLmKIzdnz/9xDIdOpSQIPt69aR+IBERkTmYqDmK1q2BYsVkqnFMjOpoKBtjosYitkREZAkmao7Cy8u0NAO7P3Xn339lHxioNg4iIrIvTNQcSebuT9IVtqgREVFBMFFzJMYJBb/8ImsRkW4YEzW2qBERkSWYqDmSkBCgZk0gLS3nemWkTEqKrMABsEWNiIgsw0TN0bD7U3cuXJC9pyfg56c2FiIisi9M1BzN44/LfssWlunQiczj01xc1MZCRET2hYmao2nZUppuzp0DTpxQHY3TW78eCA+XY3Z7EhGRpZioOZrixSVZAzhOTbGrV4HevU23ixVTFwsREdknJmqOqH172W/dqjYOJzdlCnD3rul28+bKQiEiIjvlrjoAKgIdOgATJpgWmHTnx2xrCQnAokVybJzX0aqVuniIiMg+8RvcEdWvL9MLr10DDhwwDZIim5k1C7h3D2jRAujUSXU0RERkr9j16Yjc3EzLSXGcms3FxACzZ8txRITaWIiIyL4xUXNUHTrInuPUbCo1FRg8WFrTunUzlbUjIiIqCCZqjso4oWDvXiA5WW0sTmThQuC334BSpWSMGuumERFRYTBRc1RVq8qWmgrs3Kk6Gqdw+TIwaZIcv/suUKGC2niIiMj+MVFzZMZWNY5Ts4kJE4AbN4B69YCXXlIdDREROQImao6M49Rs5ttvgcWL5XjOHJnPQUREVFhM1BxZ27YySCo2Fvj3X9XROKTUVCAyEnjmGVladeRIKclBRERkDUzUHJmfH9CwoRyzVc3q0tKA556TLs/794FnnwU+/lh1VERE5EgKlailpFgrDCoy7P60Ok0DoqKAsDBg1SrAwwNYuhT44gt2eRIRkXVZlKht2iQ1oh56SL6cihcHvL2B1q1llht713Qo87qfmqY2Fgewdaused+/P/Dnn/LvPyoKGDSIpTiIiMj6zErUvvsOqFFDvoxcXYHXXwfWrJHEbfFiSdS2bpVqEMOHS5kC0okWLYBixWTxydhY1dHYtd27gc6dgT17AIMBmDoVOHcOeOop1ZEREZGjMmutz/feAz78EHjiCUnUsuvTR/bnz8sYneXLgfHjrRkmFZjBIKuBb94M/Pyz9NeRxe7ckX/nqalAjx7A3LlAYKDqqIiIyNGZlajt32/ei1WsCHzwQWHCoSLRvr0pURszRnU0dmnLFmmUDAwEVqwASpRQHRERETmDQs/6vHkTSEqyRihUZIzj1HbskOmJZLG1a2XfqxeTNCIisp0CJ2pxcUCjRoCPD1C6NFCnDnDwoDVDI6upWxcoU0bW/DxwQHU0dic1FfjhBznmeDQiIrKlAidqL70EjBolLWpXr8oX2KBB1gyNrMbVVYrfAizTUQALFsi/cX9/mfFJRERkK2Ynak8+KZMFjC5fBrp3lxIdpUoBXboAFy8WQYRkHZnLdJBZzp6Vgravvy63J08G3M0a1UlERGQdZidqzz4LtGkDzJ4t5bhGjQJq1wb69ZNxO506AWPHFmGkVDjGRG3vXmkGpXxduCCNkF9+Cdy9Kz9ERoxQHRUREZktMhJo3FgKXpYrJ1P2jx9/8PO+/FKGDBUvDgQEAM8/L90qipidqPXpI7M/Y2OBpk2lPNfmzbI3Vn+YNKkoQ6VCqVoVCAmRAVc7d6qORveGDQP+/lv+k23dCqxbl3tpGiIi0qnoaFmAed8+mbqfmgp07AjcupX3c3bvBgYOBIYMkYTnm29kbPfQobaLOxuLOnJKlQIWLpTrGDRIVieaPl2STrID7dsDn34qZTq6dFEdjW79+KNsHh6yDw1VHREREVls48ast5cskZa1Q4eARx/N/Tn79gFVqgCvvCK3Q0JkUL7C2mMWtRFcvy7XV6eO7L29gfr1gfXriyo8siqOUzPL9OmyHzuWSRoRkd4kJycjKSkpY0sxd+HxxETZ+/nlfU7z5rLkzIYNMs7r4kXg22+l4r8iZidqq1ZJQdsnngAqVwZ++gmYMgX4/ntJNPv04WQC3WvTRva//84PKw9Hj0oXv4cH8NprqqMhIqLsQkND4evrm7FFRkY++EmaBowbJ1P381uhp3lzGaPWty/g6QlUqCDdiZ98YrX4LWV2ovaf/wCffy6DrH/+GXjrLbm/Zk3pBm7fHggPL6owySrKlgXq1ZPjbduUhqJXc+fKvmdPaSEnIiJ9iYuLQ2JiYsYWERHx4CeNGiWNFCtXPujFpdvz7bel63DjRuD0aVnIXBGzE7XkZFmYHQAeegi4fTvr4y++KF27pHPs/szTnDkyhA9Q+v8kERHlw9vbGz4+PhmbwWDI/wmjR8uMsO3bgaCg/M+NjJRZkq+/DjzyCPD448C8edJSlZBgvYuwgNmJ2qBB0u35zDNAkybAgAE5z2ELhB0wJmpbtkhTMAEAFi+W/5cBaS029hITEZGdMtYSW7NGepFCQh78nNu3c07xd3MzvZ4CZs/6nDVLvrz+/BMYPFhmuJIdatlS+t3j44G//gIeflh1RMrs2iUTBpKTgZMn5b5XXwWmTlUaFhERWcPIkcBXX8lgem9vGbsFAL6+QLFichwRIdX8ly+X2926SX2m+fOlNS0hQb4omjQBAgOVXIZF5Tm6dZON7FiJEjJYcscOGWzopInapk3yb9m4Rr2bG/DmmzLj08VFbWxERGQF8+fL/rHHst6/ZIm0OAGSiJ09a3ps8GD59T5nDjB+vEwkaNsWeP/9oo83D2YlalFRsgKBOeLj5ZpbtChMWFSk2rWTRG3rVqccjHXqlPx7vn9feoKHDAEaNACqV1cdGRERWY05XZVLl+a8b/Ro01gYHTBrjNr8+TK78/33gWPHcj6emCglR555BmjYELh2zdphklUZx6lt2wakpamNRYFXXgFu3ACaNZOCtv36MUkjIiJ9MitRi44GPvxQvtfDwgAfH+kxq1NHJlCUKSOtElWqAH/8we5R3WvUSD7E69eBw4dVR2NT27dLgWZ3d2DZMuBBk4WIiIhUMnuMWteusl29KktI/fMPcOcO4O8vqxPUr8+1EO2Gu7vMDPn+e+n+bNRIdUQ2kZ4uM64B6fFlKxoREemdRZMJAGk9e/LJogiFbKp9e1Oi9uabqqOxiago09Jnb7+tOhoiIqIHYxuYs2rXTva7d0vTqINLSQEmTJDjN9+URRqIiIj0jomas6pZU2rCpKQAe/aojqbIzZkDnDkj69WOHas6GiIiIvMwUXNWLi5Os5zU2bPAtGlyPH06ULy42niIiIjMxUTNmWVeTspBJSRI2ZikJCA8HBg4UHVERERE5rM4UduxowiiIDWMidqhQ8ClS2pjKQIXLwL16gG//CKtaMuWmZZsIyIisgcWJ2qdOgEPPQS8846sQkB2LCAAqFtXjh2wVe2ddyT/rFFDhuE56WpZRERkxyxO1P79FxgzRhajDwmRNUu//hq4d68owqMi16mT7DdtUhuHlcXHAwsXyvG8eaZ8lIiIyJ5YnKj5+ckSPL/9Bhw8KK0VI0dK48wrrwBHjxZFmFRkMidq6elqY7Gib7+VtTxbtpT1dImIiOxRoSYT1KsnNalGjgRu3QI+/1zW+mzVCoiNtVKEVLSaNwdKlpQ+wiNHVEdjNT/+KPtevdTGQUREVBgFStTu35cWiy5dgMqVpTFmzhwZvH36NBAcDDz9tLVDpSLh6Wkqfrtxo9pYrCQxEdi5U467dlUbCxERUWFYnKiNHi3dnMa1Eg8fBvbuBYYOBUqUkCRtxgzgzz+LIlwqEsbuTwdJ1D7+GEhNlZq+1aqpjoaIiKjgLE7U4uKATz6RSQUffQSEheU8JzAQ2L7dCtGRbTz+uOz37JHmKDulacC77wKTJ8vtMWPUxkNERE5o2TJg/XrT7TfeAEqVkqFGZ85Y/HIWJ2o//wz07y89Znlxdwdat7Y4FlIlJESaR9PSgG3bVEdTIJomrb2TJsntiROl1ZeIiMim3nsPKFZMjvfulbFhH3wA+PsDr75q8ctZnKhFRsqkgew+/xx4/32L3x/z5kme4OUlExF27cr73N27gRYtgDJl5L9BzZrA//5n+XtSLuy8+/Pdd4G5c2VlrLlzpYYaERGRzcXHm8bdfPcd0Ls38OKLkkDll+TkweJEbeFCSZCyq10bWLDAstdatUoWyJ44Uca6tWoFdO4sazPmpkQJYNQoGSh+7Ji0nkyaBCxaZOlVUA6ZEzVNUxuLhS5fln//gCT+I0aojYeIiJxYyZLA1atyvHmzaRUgLy/gzh2LX87iRO3CBZlMkF3ZsrKuoiVmzQKGDJGJCLVqyZi34GBg/vzcz69fX7pda9cGqlQBnntOhlcVIEGl7Fq3BgwGyZLtbCbI7NnA7dvSIvvSS6qjISIip9ahgyQ2Q4cCJ04ATzwh98fGSvJiIYsTteBgWTsxu19+kUkE5rp3T5aY7Ngx6/0dO8qYdnMcPizn5jceLiUlBUlJSRlbcnKy+UE6k+LFTf8h7aj78949U2IfESFdn0RERMrMnQuEh0t3z+rVMl4LkKSnf3+LX87d0icMHSrdlffvmyq+//yzTGoYP97817lyRcauly+f9f7y5aXVLj9BQXL9qanAlCkSU14iIyMxdepU8wNzZp06STPtxo0FGvCowk8/SQtzQADw5JOqoyEiIqdXqpRMIMiugLmIxS1qb7wh3ZUjRgBVq8o2erQsHxURYXkA2VtANO3BrSK7dsnyVQsWSHfpypV5nxsREYHExMSMLS4uzvIgnYVxnFp0tPQl6pymAUuXyvGzz8psYyIiIqU2bpTZj0Zz58pSTs88A1y/bvHLWZyoubjI7M7Ll4F9+2Rtz2vXgLfftux1/P0BN7ecrWeXLuVsZcsuJASoUwcYNkwafqZMyftcg8EAHx+fjM3b29uyQJ1JzZpApUpASookazqWlgb06ycTagBgwACl4RAREYnXXweSkuQ4Jka6G7t0AU6dAsaNs/jlCrzWZ8mSQOPGUvDWYLD8+Z6eMvh7y5as92/ZIjXhzKVpkleQFbi4yLRbwLRYpk6tWgV8/bW0okVGAo88ojoiIiIiyFqaoaFyvHq1rGX43ntSluCnnyx+uQJ1Fh04AHzzjUwQvHcv62Nr1pj/OuPGSUtIo0Yy7m7RInlNY6HSiAjg/Hlg+XK5PXeuNPgYy4Ps3g18+KF0vZKVdOsmNVh+/FH62HU4Oj8tzVQnbfJk4M031cZDRESUwdPTNHxo61Zg4EA59vMztbRZwOJELSpK3rNjR2n96tgROHlSujB79rTstfr2lYHg06ZJaY+wMGDDBlnoHZD7MtdUS0+X5O30aWlJeeghWVeUJRmsqG1bqSZ89qw02eqwqWr1aqmj5+vLJJ2IiHSmZUtpiWrRAti/X7qAACnVERRk8ctZ3PX53nuyGsCPP0rS+PHH8qXZp4+0dllqxAjgn3+k+/LQIeDRR02PLV0K7Nhhuj16NPDHH8CtW7Ik5W+/AS+/DLgWuAOXcihWzFSc74cf1MaSi/R0U2va2LGSrBEREenGnDnSmvTtt1I/qmJFuf+nn0yT9ixgcYrz99+m2m0GgyRNLi4yqJ8rBDiIrl1lr8Nxat9/Lw193t5cdJ2IiHSoUiX5/jx6VMpkGP3vf1Kh3UIWd336+QHGmrEVK0oLV506wI0bdlHRgcxhTNR+/VWm4ZYrpzae/6dp0k0OSDmY0qXVxkNERJSrtDQpS3DsmLRm1aolxT7d3Cx+KYsTtVatZGxanTrS3TlmDLBtm9zXrp3F7096FBgoU3IPHZJBg4MHq44IqanS7X7kiKz5aif1eImIyNn89ZeU4zh/HqhRQ1oZTpyQpZ3Wr5cB9hawuOtzzhypXwXIwP7XXgMuXgSeegpYvNjSVyPdMraq6WSc2gcfyAxPQErUGFfkICIi0pVXXpFkLD5eBtMfPiwT9EJC5DELuWiappl7cmoq8OWXshB6hQoWv5cunDt3DsHBwYiPj0dQAWZfOI1Dh6RuSsmSst5XQYrlWVHTpjJ55u23pcCxDquGEBFREbKb7+8SJWRFgDp1st5/9KjMBL1506KXs6hFzd1dZlmywKwTqF9fFtC8eVP5KgV37siPEgB4/nkmaUREZIbISKnM7+0tY6179ACOH3/w81JSgIkTpVaYwSCtY59/bv77GgymwfyZ3bwp5TIsZHHXZ9Om0opHDs7VVTfdnwcPSmtuQICpxh4REVG+oqOBkSOldWvLFvki6dhRylXkp08f4OefZTzX8eOyoLix0r45unYFXnxRJuRpmmz79kk1/+7dLb4MiycTjBghy1adOyfjzUuUyPq4DuujUkF17w58+qnUxJg9W1lT1p49sm/enK1pRERkpo0bs95eskRa1rIXbc3+nOhoWZfTz0/uq1LFsvedPRsYNEiWXPLwkPvu35dZnx99ZNlroQCJWt++ss88Hs7FRRJGFxeZkUoOon17ycTj401j1mxM00zl3CxZA5aIiBxTcnIykjItxWQwGGAwZxx1YqLsjQlYbtatk++6Dz4AvvhCvgO7dwemT5eC8OYoVUoaOP76S8pzaJqs/VmtmnnPz8biRO306QK9D9kjLy+pbvz117KIq4JEbfVqWdPVYAB69bL52xMRkc6EGhc8/3+TJ0/GlClT8n+SpsmyTi1bynqVeTl1Sr50vLyAtWtlMt2IEcC1a/mPUxs3Lv/3z7zM0qxZ+Z+bjcWJGscIOZmePU2J2nvv2fStNQ2YMEGO33yT//aIiAiIi4tDReOyTIB5rWmjRgG//y5JWH7S06V78MsvTWsUzpoF9O4NzJ2bd6uauYP3CzB+x+JEbfny/B83LhJPDqJLF5mlcvy4NOHWqmWzt967Fzh5UlqeX3vNZm9LREQ65u3tDR8fH/OfMHq0dGnu3PngRdEDAmTZpcwLSdeqJS0H584BDz+c+/O2bzc/HgtZnKhlX1/x/n1ZOsrTEyhenImaw/HxATp0kGrKa9bIlGUbMf4o6NVLyrkRERGZTdMkSVu7VroeQ0Ie/JwWLYBvvpFSGsYvnhMnpBKCotptFpfnuH4963bzpjS2tGwpM1jJAfXsKfs1a2z2lidPSsszwOSfiIgKYORIYMUK4KuvpJbahQuy3bljOiciIuuXzDPPyNI3zz8PxMVJK9zrrwMvvGD+ZAIrszhRy83DDwMzZuRsbSMH0b27/Jr47TfgzJkif7v792VJsps35cdNmzZF/pZERORo5s+XmZ6PPSZdmsZt1SrTOQkJsryTUcmSUnPtxg2ZQPfss0C3blJyQxGLuz7z4uYG/PuvtV6NdKVsWaBVK6kts3YtMHZskb7djz8Cf/whP2q++UZyRCIiIouYs0Lm0qU576tZU5I1nbA4UVu3LuttTZOEdM4caf0gB/XUU5KorVlT5Inap5/KfuhQ+fFDRETkrCxO1Hr0yHrbxUUaXNq2BWbOtFJUpD89e0rf9u7d0sdfoUKRvM2xY6Zi0kOHFslbEBER2Q2LO5XS07NuaWnyvf3VV2z9cGjBwbLQq6YB335bJG+RmirjNzVNlkorYBFnIiIih8HRP2Q+4/phmQdiWkl6umkNW19fYN48q78FERGR3bE4UevdW2Z4Zvff/wJPP22NkEi3jB/w7t1S+M+K5syR9XLd3GRsZ3CwVV+eiIjILlmcqEVHy/KP2XXqJOVGyIEFBUnBPECmY1pJYiJgXKZt1qyc4yCJiIiclcWJ2s2bsgpBdh4eQKbF7MlRWbH7U9OAd98FSpWS4snVq8vat0RERCQsTtTCwnL/jo6KArItaE+OqHdvKWz266/AP/8U6qVGjQImTZLjUqWkNqG71Sr7ERER2T+LvxbfekvWXvz7bynJAQA//yzLR1mxN4z0qkIFoHVrWYD266+BN94o0Mvs2iUTBlxcgAULZLanh4eVYyUiIrJzFreode8OfPcd8Ndf0k01fryMK9+6lWOLnEa/frKPiirQ02/fBl5+WY6HDZPZnkzSiIiIcnLRNHPWWHAc586dQ3BwMOLj4xEUFKQ6HPt05Yq0rKWlASdOyGKvFhgwQNbJLVcOiI0F/P2LKE4iInIYzvr9bXGL2oEDMjwpu19/BQ4etEZIpHv+/kD79nK8cqVFT/3xR0nS3Nyk55RJGhERUd4sTtRGjgTi43Pef/68PEZO4plnZL9ihXkL3wJISZFVqADgtddkqBsRERHlzeJELS4OaNAg5/3168tj5CSeegooXhw4eRLYv9+sp6xcCZw6JUuNGWd7EhERUd4sTtQMBuDixZz3JySwtIJTKVlSFmoHgC++eODpmga8/74cjxkjTyciIqL8WZyodegARERINXmjGzeACRPkMXIiAwbIPioKuHcvz9PWrAFKlwb+/BMoVgwYOtRG8REREdk5ixO1mTNljFrlykCbNrKFhAAXLshj5ETatZPZn1evAhs35npKVJTU3UtMlBUt3noLKFPGxnESERHZKYsTtYoVgd9/Bz74QFYiaNgQ+PhjICaGC2k7HXd3oH9/OV6xIsfDd+7IpAFAaqUlJUlrLBEREZmnQKPKSpSQL14iDBgA/O9/wLp10gdeqlTGQ/Pny2zgSpWA2bNlfCMRERGZr8DD/+PigLNncw5N6t69sCGRXalXD6hdWyrXfvttxgC0tDTgo4/klLffZpJGRERUEBYnaqdOyWS/mBhZp9FYQsvFRfZpadYMj3TPxUVa1d58E1i+PCNRW79exjKWKQM8+6ziGImIiOyUxWPUxoyRyQMXL0oZrdhYYOdOoFEjYMeOIoiQ9O/ZZwFXV1lp/eRJALLgOgAMGQJ4eSmMjYiIyI5ZnKjt3QtMmwaULSvfza6uQMuWQGQk8MorRREi6V5QEPD443K8ZAn++gvYtEka2156SW1oRERE9sziRC0tzVSs1N8f+PdfOa5cGTh+3JqhkV0ZMkT2S5di0bxUAEDnzkDVqgpjIiIisnMWj1ELC5PyHFWrAk2bSpkOT09g0SJ+KTu1bt2kmTUhAWcXbQTQFSNGqA6KiIjIvlncojZpEpCeLsfvvAOcOQO0agVs2CAlGMhJeXpmrFTQ99Zi1KgBdOqkOCYiIiI7Z3GLmnEoEiAtaHFxwLVrskSQceYnOad/O72AwFmz0BU/IuXli3BzK686JCIiIrtmcYtabvz8mKQ5u5s3gcdG1sY+NIUHUtH7zoMXaiciIqL8WSVRI1q0SCpzrCklkwrcly02FdkjIiKiAmGiRoWWkgLMnCnHtaf1lQJ7f/4J/PKL2sCIiIjsHBM1KrQ1a6RMS8WKQL8XfUwLtS9YoDYwIiIiO8dEjQpt+XLZDxny/2t6Dh8ud3zzDXD5srK4iIiI7B0TNSqUhARg82Y5/v/qHLKeWKNGwL17wJIlymIjIiKyd0zUqFC+/Vbq6oWHA9WqZXrg5Zdlv3ChqfAeERGRrURGAo0bA97eQLlyQI8eli2h9MsvgLs7UK9eUUVoFiZqVCgbNsi+V69sD/TtC/j6AqdOAVu22DwuIiJyctHRwMiRwL598j2Umgp07AjcuvXg5yYmAgMHAu3aFX2cD8BEjQrs9m1g+3Y57tw524MlSgCDBsnx/Pk2jYuIiBxXcnIykpKSMraUlJTcT9y4ERg8GKhdG6hbV4binD0LHDr04Dd56SXgmWeku0gxJmpUYNu3S2mOypWBWrVyOeGll2T/ww9AfLxNYyMiIscUGhoKX1/fjC0yMtK8JyYmyt7PL//zliwB/v4bmDy5cIFaicVLSBEZGbs9u3TJY2WK0FCgdWtpfl60CJg+3abxERGR44mLi0PFihUzbhsMhgc/SdOAceOAli2BsLC8zzt5EnjzTWDXLhmfpgNsUaMC0bSsiVqeRo6U/cKFwN27RR4XERE5Nm9vb/j4+GRsZiVqo0YBv/8OrFyZ9zlpadLdOXUqUL269QIuJCZqVCDHjwP//AN4egJt2uRzYo8eQFCQ1FOLirJRdERERP9v9Ghg3ToZrxMUlPd5ycnAwYOS1Lm7yzZtGnD0qBxv22a7mDNhokYFYmxNe+wxmTeQJw8PU6vaxx9z/U8iIrINTZOka80aSbJCQvI/38cHiIkBjhwxbcOHAzVqyHHTpkUfcy6YqFGBGH9YdOpkxsnDhgHFisk/9F27ijIsIiIiMXIksGIF8NVXUkvtwgXZ7twxnRMRIWU4AMDVVcavZd7KlQO8vOQ431aJosNEjSymaVKWBgBatDDjCWXKAM89J8cff1xkcREREWWYP19mej72GBAQYNpWrTKdk5AgJTt0zEXTnKsv6ty5cwgODkZ8fDyC8uurpjz9/besQuDpCSQl/f/6ng/yxx9AnTryi+Xvv4EqVYo6TCIiciDO+v3NFjWymLE1rX59M5M0QJqN27WT5aTmzi2y2IiIiByJ8kRt3jwZ3+flBTRsmP8QpjVrgA4dgLJlZcxfeDiwaZPtYiXx66+yb9bMwieOGSP7Tz+V2TVERESUL6WJ2qpVwNixwMSJwOHDQKtWshRRXt3FO3dKorZhg6wA0aYN0K2bPJdsQ9NMS3danKg98YTMnklMlAK4RERElC+lY9SaNgUaNMi6FGStWlJ6y9wVIWrXlvW/334798dTUlKyrAN2/vx5hIaGOl0ft7Xs3i0JdfHiMgbTx8fCF1i8GBg6FKhYURZs9/QskjiJiMixcIyajd27J61iHTtmvb9jR2DPHvNeIz1detDyW7YrMjIyy5pgoaGhBQ/ayd2/D8yaJcf9+hUgSQNk9mdAAHD+vEyZJiIiojwpS9SuXJHVGsqXz3p/+fJS5sQcM2cCt24BffrkfU5ERAQSExMztri4uIIH7cRu3JCWtLVr5bZxvXWLGQzS3w0AH3wg2TYRERHlSvlkguyLeWtaHgt8Z7NyJTBlioxzK1cu7/MMBkOWNcG8vb0LFa8zunNHhpf9+itQurTUD2zSpBAv+NJL0hx37Bjw449Wi5OIiMjRKEvU/P0BN7ecrWeXLuVsZctu1SpgyBDg66+B9u2LLkZndukS8OabwOnTwIgR0h1dujSwYwfw7LOFfHFfX+Dll+X4/fcLGyoREZHDUpaoeXpKOQ7jDEKjLVuA5s3zft7KlcDgwTK86YknijREpzZmjORQNWsCS5dKndpvvwUeecSKb+DpKRngL79Y6UWJiIgci9Kuz3HjgM8+Az7/XHrBXn1VSnMMHy6PZ16CC5AkbeBAGZvWrJlp2a7ERDXxO6pz54CoKDm+d0/2U6cCbdta8U0CAkwfLlvViIiIcqU0UevbF/joI2DaNKBePamTtmEDULmyPJ59Ca6FC4HUVFlnNfOyXcY6qmQds2dnvd20qXSDWt3rr8uAxB9+AGJji+ANiIiI7BvX+qQskpKA4GDZ//CD1Etr2FCGlRWJXr1kyYn+/Vmug4iI8uSs39/KZ32Svnz6qSRptWoBXbpId2eRJWkAMGmS7KOigD//LMI3IiIisj9M1CjDtWumFSHGjZMJBEWufn2ge3epy/LOOzZ4QyIiIvvBRI0yTJ4MXL0qy3INGmTDNzau/7VyJXDihA3fmIiISN+YqBEAWSnCuE767NmAh4cN37xhQ6BrV1mlgK1qREREGZioEQApkXLvHtCokZXLcJhr8mTZf/klcPKkggCIiIj0h4kaIS0NWLBAjkeMUBREo0Yye4GtakRERBmYqBE2bZKlokqXltp2ykyZIvsVK4C4OIWBEBER6QMTNcK8ebJ//nmpm6ZM48ZAz57SqvbWWwoDISIi0gcmak7u0iVZDQIwLd2l1PTpslrBmjXAgQOqoyEiIlKKiZqT27tXSpiFhQEPP6w6GkhtkAED5HjCBLWxEBERKcZEzcn9+qvsmzZVG0cWU6dKfZCtW4Ft21RHQ0REpAwTNSe3b5/sdZWoVakCvPSSHE+YIE1+REREToiJmhNLSwOio+W4WTO1seQwcaLMbPj1V+Dbb1VHQ0REpAQTNScVGwuUKycTLEuWBEJDVUeUTYUKwOuvy/F//gPcvas2HiIiIgWYqDmZAwdkDfSwMFmE3cMDGD0acHNTHVkuXn8dCAyUIm+zZ6uOhoiIyOaYqDmRu3eB/v2BH36Q22XKAGfPAu+9pzauPJUoYQru3XellggREZETYaLmRGbNAv7+W46bNQNWrZIeRl0bMEAWbU9KMq0HSkRE5CSYqDk4TQMGDwZatJCqFwDwxRdSP61dO6WhmcfVVTJMAFi0SAbXEREROQkmag7uq6+AZcuAPXuAe/eASpWAfv1UR2WhRx8FnnpKZj6MH686GiIiIpthouYANA24fz/n/T/+CIwZk/W+kSMBd3fbxGVVH3wgMx82bQJ++kl1NERERDbBRM3OaZq0kPn7y+zNVq2AX36RfKZbN+DqVaBuXeC//wV699bJep4F8dBDwCuvyPGYMUBKitp4iIiIbICJmp3SNKlaMWUK8PXXMtZ+zhxg926gZUugVy8577nnpNvztdeAb74BfHyUhl04b70lsx9OngQ+/FB1NEREpGeRkUDjxoC3txQO7dEDOH48/+esWQN06ACULStfmOHh0vKhEBM1O3PuHPD00/JvrmpVYNo0ub9ECaBiRdN5t27JIusLF0qBf4fg62uaWPDOO5KpEhER5SY6Wsb77NsHbNkCpKYCHTvKF2Redu6URG3DBuDQIaBNG+meOnzYdnFn46JpzrWQ4rlz5xAcHIz4+HgEBQWpDscix44BbdsCFy7IbQ8P6dbs0QOIiJAJkocOAS+/DDRqJMtk2tklPpimyXTV7dvlf55161RHRERENmD8/o6Li0PFTC0TBoMBBoPhwS9w+bK0ckRHyyQ1c9WuDfTtC7z9dgGiLjy2qNmJxETgySclSQsLky7O5GRZaWDiREnSACk5tn8/MG+eAyZpAODiAsydK1nqDz+YqvcSEZFTCA0Nha+vb8YWGRlp3hMTE2Xv52f+m6Wny5etJc+xMnuc/+d07tyRVrOTJ4HgYGDbNuk+d1q1agHjxgHvvy8zKNq0kQVLiYjI4eXWovZAmibfGy1bSmuHuWbOlK7SPn0KEKl1sEVN565cATp1AnbskPGQ333n5Ema0VtvAZUrA2fOyDERETkFb29v+Pj4ZGxmJWqjRgG//w6sXGn+G61cKTP2Vq2SLlNFmKjpXP/+MrbRxwdYvx5o0EB1RDpRooTMlACAjz+WwaJERETZjR4t45m3bzd/TNCqVcCQIVJWoX37oo3vAZio6diRI8DWrVKgdvduqZFGmTz+ODBwoDRpDxnC2mpERGSiadKStmaNjBkKCTHveStXytqLX30FPPFEkYZoDiZqOmasRNG7N1CnjtpYdGvWLOkLjouTmjlERESAlOZYsUISLm9vmY134YIM/DaKiJAf/EYrV8rtmTOBZs1MzzFORFCAiZoOpaQAM2bI4ulAzmWgKJMyZYBPPpHj994DYmLUxkNERPowf74kWI89BgQEmLZVq0znJCQAZ8+abi9cKPXWRo7M+hyFX8Sc9akzV65IIeS//pLbkyZJUk/56NNHfjGtWwcMGAD8+itgzuBSIiJyXOaUiV26NOvtHTuKIpJCYYuazkycKElauXLARx+ZVh6gfLi4AIsWyYKnR4/KLB0iIiIHwERNRw4eBD79VI5Xr5aWVhcXtTHZjfLlJVkDgA8+kJXpiYiI7BwTNZ1IT5fJKZomC6m3bKk6IjvUs6fM1ElPl8GgycmqIyIiIioUJmo6sXq1DK0qWVIahKiAPv5YCuGeOgWMHas6GiIiokJhoqYTn38u+zFjZIIJFZCPD7BsmfQZf/458OWXqiMiIiIqMCZqOpCQAGzeLMeDBqmNxSG0bg28/bYcDx8ui6QSERHZISZqimmazOxMT5eyHA8/rDoiB/HWW1I75+ZNKd9x967qiIiIiCzGRE2xWbOABQvk+I031MbiUNzcpNvT31/W4nrtNdURERERWYyJmkJnz0rDDyA103r0UBmNAwoMNC3vMHeu6ZiIiMhOMFFTKCJClhx79FHglVdUR+OgOnWS5R0AYNgwKVZHRERkJ5ioKXLpEvDNN3I8axYL2xapqVOBbt1kEdWePYGLF1VHREREZBYmaoosWwbcvw80aQI0bKg6Ggfn6gqsWAHUrAmcOwf06gXcu6c6KiIiogdiomZjmgYkJQGffCK3hw1TG4/T8PEBvvtO9r/8wr5mIiKyC0zUbOjmTaBaNcDXF4iPlwL6/furjsqJ1KgBrFwp/cwLF8oEAyIiIh1jomZDu3bJykZGS5YAJUqoi8cpdekCvPeeHL/yCvDDD2rjISIiygcTNRvatUv25coBP/8MtGmjNh6n9Z//AEOGSJXhfv2AAwdUR0RERJQrJmo2ZEzUIiOBtm3VxuLUXFyA+fOBxx8Hbt8GunYFTp9WHRUREVEOTNRs5O5dYP9+OW7VSm0sBMDDQ+qj1KsntVIef5xlO4iISHeYqNnI4cNSEaJ8eZlQQDrg7Q2sXy+zOk6eBDp0AK5dUx0VERFRBiZqNnL0qOwbNGBxW10JDAS2bgUqVABiYoDOnYHkZNVRERERAWCiZjO//y77Rx5RGwflolo1Sdb8/KR/uls3GbtGRESkGBM1G4mJkX2dOmrjoDzUrg1s2iTdodHRbFkjIiJdYKJmA5rGRM0uNGoEbNwoqxfs3Al07AjcuKE6KiIicmJM1Gzg3DkgMRFwd5flJknHmjeXInelSwP79kkdlStXVEdFREROiomaDRhb02rUADw91cZCZmjUCNixAyhbVqbrtmkDXLigOioiInJCTNRswFjotmFDtXGQBR55RMaqBQQAf/wBtGwJ/PWX6qiIiMjJMFGzga1bZd++vdo4yEK1aslYtZAQ4O+/gfBw4NdfVUdFREROhIlaEbt+HTh0SI7btVMbCxVAtWrAnj3SHHrlinSDciF3IiKyEeWJ2rx50mDh5SXfhcZuwtwkJADPPCNjvVxdgbFjbRZmgW3fLrM+a9WS2qpkhypUkDFrnToBd+4APXoACxeqjoqIiJyA0kRt1SpJtiZOlDHbrVpJ+aqzZ3M/PyVFxndPnAjUrWvTUAvMuL5n69Zq46BCKlkSWLcOeOEFID0dGD4cmDRJsnAiIgJu3ZIftZGRwDvvqI7GYShN1GbNAoYMAYYOlRanjz4CgoOB+fNzP79KFeDjj4GBAwFfX1tGWnDG8ecsy+EAPDyAzz4DpkyR2+++K61r16+rjIqIyPY0TcburlgBjBwp6yP6+srwkAkT5MuaP2Stwl3VG9+7J2O33nwz6/0dO8qQIGtJSUlBSkpKxu1kG1ebNyZqXIjdQbi4AJMnA5UqSavaunXSZ//NN5zWS0SO69Yt4MABYO9e2fbtAy5fznleYCDQooXUpLx/nzWprEBZonblCpCWBpQvn/X+8uWtW7IqMjISU6dOtd4LWsD4gwNgouZwnn9e+t979wZOn5Y/Sh9+CIwaJckcEZG9Mn55GROyvXtlweq0tKzneXhIS1p4uGkLDlYTc24iI4E1a4A//wSKFZO/0++/LwPd8xMdDYwbB8TGSuL5xhvyw1wRZYmaUfbvNE2z7vdcREQExo0bl3H7/PnzCA0Ntd4b5OPSJeDmTZn4UKWKTd6SbKlBA+C33yRp++474JVXgPXrgc8/58wRIrIfN28CBw8+uLWsYsWsSVn9+jITUK+io6VbtnFjIDVVBrh37AjExQElSuT+nNOngS5dgGHDpFv3l1+AESNkgHyvXraN//8pS9T8/QE3t5ytZ5cu5WxlKwyDwQCDwZBxOykpyXov/gDGbs/gYCBTCORISpWSX2xz5wKvvy4Lu9epI7NCe/dWHR0RUVaZW8uMW0xMztYyT8+crWVBQWpiLqiNG7PeXrIEKFdOxl09+mjuz1mwQIa2fPSR3K5VS5LYDz90vkTN01OG9GzZAvTsabp/yxbgySdVRWVd7PZ0Ei4u0uXZrh3w3HPSyvb00zLR4JNP7O+PGxE5jqQkGVv266+m1rLc1i8OCsrZWqbTFobk5OQsjS7ZG2TylJgoez+/vM/Zu1da3TJ7/HFg8WIZc+fhUYCIC0dp1+e4ccCAAbK0Yng4sGiRlOYwdgVHRADnzwPLl5uec+SI7G/elJbZI0ck6bNRb6ZFOJHAydSqJf+TT58OzJgh3aE//yyzROvXl2WoFPxPTkROIi1NxlXt2yeJ2a+/Sjdf9tmXxpYSY1LWrJld/aDMPnxp8uTJmGKcjZ8XTZOko2VLICws7/MuXMh98HxqqiS4AQEFC7oQlCZqffsCV68C06ZJMduwMGDDBqByZXk8ISFnTbX69U3Hhw4BX30l5//zj83CNtvu3bKvVUttHGRDnp6SqPXtC7z4oiRu48fLY4MHS9M7EZE1nD8vxTp//VWSs4MHZXZmdpUrSzLWtKnuW8vMERcXh4oVK2bcNqs1bdQomRBh/GLOT26D53O730aUTyYYMUK23CxdmvM+eynLcuGCjGMEgO7d1cZCCoSFyR+EhQtl+Y0//pB/0N27Z+3rJyIyx40bkojt3y9dmfv3A//+m/M8b2+gSRNJyoybNQd+64C3tzd8fHzMf8Lo0VJKaefOB7ccVqiQ++B5d3egTBnLg7UC5Ymao1q9WgrYN2kiS2SRE3J1BV5+WbaICOkOHTtWlt/Q80wpIlLr7l3g6NGsSdnx4znPc3OTH4VNmphazGrWlPtJWnZGjwbWrpUVE8z5Mg4Pz7me8+bNMkZL0dAVJmpF5PvvZd+nj9o4SCfefhv44gvpy583T8ZKEBGlpkqdr4MHTUnZ0aMycD27qlUlKTNu9esDxYvbPmZ7MXKkjI/6/ntpaTS2lPn6Sl01IOdg+OHDgTlz5G/0sGEyfGXxYmDlSjXXACZqRULTZPwcADz2mNJQSC+KFQOmTpX10iZOlMrdTZuqjoqIbCklRQb7//abaTt6VFrQsvP3l78RjRtLUta4sdxH5jOuR5n9i3jJEhkzDOQcDB8SIoPlX31Vyi4FBgKzZysrzQEALppmL6O+rOPcuXMIDg5GfHw8gopolkt8vJRhcXOT2ans5SIAMiPrySelKG7p0sDXXwPt26uOioiKwq1bMng9c1IWG5t7S1nJktI6lrm1rHJlrnKSjS2+v/WILWpF4PBh2YeGMkmjTNzcpPm8Y0eZodWhg9RamzZNiuQSkX1KTJRaUZmTsj//lIHK2fn5SSHZBg0kOWvQQGo4ubraPGyyD0zUioAxUctcSoQIgIyT2L5dJhV8+qnUWvvuO6BrV+C116RaNn9FE+nX5cvyRz5zUmasbp5d+fJSr8yYmDVoIN0t/H+cLMBErQgYi/IyUaNceXnJMiVjxgCTJwPffgv8+KNsjRpJwtarl0wHJyI10tKAEydkDNnRo/KH/ehRGdOUm8qVc7aUKSiOSo6H3wRFIDZW9nXrqo2DdK5WLRmnduIE8L//SZ21gweBfv3kj/6rrwJDhsj4FSIqOleuSK3DmBgZV3b0qBznNsgfkK7KzC1l9esrq7FFjo+TCaxM0+R79fZtWULqoYes/hbkqC5fltIdc+aY1uIrVUqmiA8cmP+yJ0T0YLdvyy9pY1Jm3GcvcGpUooSMH61bV7Z69eQ2fzwp4ayTCZioWVliony3AjLphyVuyGJ37gDLlgGzZgEnT5ruDwsDnnkG6N8fqFJFWXhEunfrlgzmj4sDjh2T7Y8/ZCxZXl95ISGShBkTs3r15Jc2B/nrBhM1J1HUH/Sff0qPlq+vrPhBVGBpaVIhe+lSqeuTeVp/8+bAU08BXbpIJXIOTiZndO2aKRHLnJSdOZP3c8qWNSVkYWGyr12brWR2wFkTNY5RszLjOFOOIaVCc3OT8h09egDXr8u6ZCtXyqzRPXtke+01aV3r1EmKOrZuLWvVETmKO3dkHMmJE7IdP246vno17+eVLSs1kmrVkq12bUnKypWzXexEVsBEzcqMa+QGBqqNgxxM6dKyqsHQofKP7JtvpJVtxw7gn39kFumCBXJujRqStBkTN/5qIL1LS5NWsMxJmHHLXDU+N8HBkogZkzLjnoP7yUEwUbMytqhRkQsMlNIeY8bIWJxt22TbsUNmqx0/LtvChXJ+9eqSsBkTt4oVVUZPziopCTh9Gjh1Kuf2zz/AvXt5P7dUKfkBUr26aatRQ2ZflihhqysgUoKJmpUZW9SYqJFNlCgBdOsmGyBdpLt2SdIWHS2FOY0tE59+KudUqyYJW+PGUmIgLIxLaFDh3b0r6+edPSuJV/ZkzDiTOS9eXsDDD2dNxoxbmTIch0lOi4malRlb1Nj1SUqULg107y4bIDNadu2SpG3HDknc/vpLtsWL5Rx3d0nWGjSQmW7GMT2BgfxyJKFpUsLi7FnTZkzKjMeXLj34dfz9gapVc9+CgznDkigXTNSsjF2fpCulSmVtcUtMBHbvlu3QIVn+5upVqbpuXFLDyMdHZpTWqiUtHZm/VP39mcQ5sqtXgSVLJLk3dk3eufPg55UoIQlXSIhsmf/NhITIvykisggTNSvjZALSNV9f4IknZAOkpSQ+XpK2Q4ek1tSxY1JvKikJ2L9ftuxKlMj6BRwUJGPfAgNN+2LFbHttlL+UFOl+vHxZWr/y2hsM0gKbnaurfLaVKskWHGw6Nt4uXZoJPJGVMVGzops3gXPn5JgtamQXXFxMX7Q9e5ruT0mR7tG4OCkOmHms0fnzMokhJka2vJQuLf8jlClj2vz8cr/t5yetLSVK8Is+N5omn8nNm0Bysml/44aMS7x2TfaZt+z3mdMillnlysC4cTKL0tg16eFRJJdHRHljomZFH38sfwsfekj+rhHZLYNB6k7Vrp3zsbt3pZSCMXE7fVqaks+fN21375oSBEu4ugLe3rL5+Mjm7S3FSA0GGXBuMOQ8zu+2m5skf66uss/r+EGPZz/WNClCfP++zFjMvM/tvsyP3bkj2+3bshmPM99361bWpCwtrfCfq7u7dFuXKyd1xsqWNR2XKyfJ9YoV8hl+/TX/kBHpABM1K7l6FfjgAzmePl2+G4gckpeXlEaoUSP3xzVNWnrOn5cB6Neuyf8gxn1ux9euAenpsiUmyka5K15cEteSJWUMop+fJFjGLb/b3t4PHrD/9NM2uQwiMg8TNSuJj5eC8CEhQN++qqMhUsjFxZQYmLuQvKZJK1JysoyNS0rKenzzpnT9Zd7u3s39OPvttDR5/fR02Vvr2MVFugI9PWWf+fhB9xUrJgmXcZ/bsTEZM7YoliwpXcP8FUjkVJioWUm9ekBsrPQAcYY5kYVcXCQJKVGCS2AREWXClMKK3N1lTDYRERGRNTBRIyIiItIpJmpEREREOsVEjYiIiEinmKgRERER6RQTNSIiIiKdYqJGREREpFNM1IiIiIh0iokaERERkU4xUSMiIiLSKSZqRERERDrFRI2IiIhIp5ioEREREekUEzUiIiIinXJXHYCtpaenAwASEhIUR0JERETmMn5vG7/HnYXTJWoXL14EADRp0kRxJERERGSpixcvolKlSqrDsBkXTdM01UHYUmpqKg4fPozy5cvD1dW6Pb/JyckIDQ1FXFwcvL29rfraesDrs2+8PvvG67N/jn6NRX196enpuHjxIurXrw93d+dpZ3K6RK0oJSUlwdfXF4mJifDx8VEdjtXx+uwbr8++8frsn6Nfo6NfnyqcTEBERESkU0zUiIiIiHSKiZoVGQwGTJ48GQaDQXUoRYLXZ994ffaN12f/HP0aHf36VOEYNSIiIiKdYosaERERkU4xUSMiIiLSKSZqRERERDrFRI2IiIhIp5ioWcm8efMQEhICLy8vNGzYELt27VIdUoFMmTIFLi4uWbYKFSpkPK5pGqZMmYLAwEAUK1YMjz32GGJjYxVGnL+dO3eiW7duCAwMhIuLC7777rssj5tzPSkpKRg9ejT8/f1RokQJdO/eHefOnbPhVeTtQdc3ePDgHJ9ns2bNspyj5+uLjIxE48aN4e3tjXLlyqFHjx44fvx4lnPs+TM05/rs+TOcP38+HnnkEfj4+MDHxwfh4eH46aefMh63588OePD12fNnl5vIyEi4uLhg7NixGffZ+2doD5ioWcGqVaswduxYTJw4EYcPH0arVq3QuXNnnD17VnVoBVK7dm0kJCRkbDExMRmPffDBB5g1axbmzJmDAwcOoEKFCujQoQOSk5MVRpy3W7duoW7dupgzZ06uj5tzPWPHjsXatWsRFRWF3bt34+bNm+jatSvS0tJsdRl5etD1AUCnTp2yfJ4bNmzI8riery86OhojR47Evn37sGXLFqSmpqJjx464detWxjn2/Bmac32A/X6GQUFBmDFjBg4ePIiDBw+ibdu2ePLJJzO+yO35swMefH2A/X522R04cACLFi3CI488kuV+e/8M7YJGhdakSRNt+PDhWe6rWbOm9uabbyqKqOAmT56s1a1bN9fH0tPTtQoVKmgzZszIuO/u3buar6+vtmDBAhtFWHAAtLVr12bcNud6bty4oXl4eGhRUVEZ55w/f15zdXXVNm7caLPYzZH9+jRN0wYNGqQ9+eSTeT7Hnq5P0zTt0qVLGgAtOjpa0zTH+wyzX5+mOd5nWLp0ae2zzz5zuM/OyHh9muY4n11ycrL28MMPa1u2bNFat26tjRkzRtM0x/v/T6/YolZI9+7dw6FDh9CxY8cs93fs2BF79uxRFFXhnDx5EoGBgQgJCUG/fv1w6tQpAMDp06dx4cKFLNdqMBjQunVru7xWc67n0KFDuH//fpZzAgMDERYWZjfXvGPHDpQrVw7Vq1fHsGHDcOnSpYzH7O36EhMTAQB+fn4AHO8zzH59Ro7wGaalpSEqKgq3bt1CeHi4w3122a/PyBE+u5EjR+KJJ55A+/bts9zvaJ+hXjnP8vNF5MqVK0hLS0P58uWz3F++fHlcuHBBUVQF17RpUyxfvhzVq1fHxYsX8c4776B58+aIjY3NuJ7crvXMmTMqwi0Uc67nwoUL8PT0ROnSpXOcYw+fb+fOnfH000+jcuXKOH36NN566y20bdsWhw4dgsFgsKvr0zQN48aNQ8uWLREWFgbAsT7D3K4PsP/PMCYmBuHh4bh79y5KliyJtWvXIjQ0NONL2t4/u7yuD7D/zw4AoqKi8Ntvv+HAgQM5HnOk///0jImalbi4uGS5rWlajvvsQefOnTOO69Spg/DwcDz00ENYtmxZxiBYR7lWo4Jcj71cc9++fTOOw8LC0KhRI1SuXBnr16/HU089lefz9Hh9o0aNwu+//47du3fneMwRPsO8rs/eP8MaNWrgyJEjuHHjBlavXo1BgwYhOjo643F7/+zyur7Q0FC7/+zi4+MxZswYbN68GV5eXnmeZ++fod6x67OQ/P394ebmluOXwaVLl3L8yrBHJUqUQJ06dXDy5MmM2Z+Ocq3mXE+FChVw7949XL9+Pc9z7ElAQAAqV66MkydPArCf6xs9ejTWrVuH7du3IygoKON+R/kM87q+3NjbZ+jp6Ylq1aqhUaNGiIyMRN26dfHxxx87zGeX1/Xlxt4+u0OHDuHSpUto2LAh3N3d4e7ujujoaMyePRvu7u4ZMdr7Z6h3TNQKydPTEw0bNsSWLVuy3L9lyxY0b95cUVTWk5KSgmPHjiEgIAAhISGoUKFClmu9d+8eoqOj7fJazbmehg0bwsPDI8s5CQkJ+OOPP+zymq9evYr4+HgEBAQA0P/1aZqGUaNGYc2aNdi2bRtCQkKyPG7vn+GDri839vYZZqdpGlJSUuz+s8uL8fpyY2+fXbt27RATE4MjR45kbI0aNcKzzz6LI0eOoGrVqg75GeqOjScvOKSoqCjNw8NDW7x4sRYXF6eNHTtWK1GihPbPP/+oDs1i48eP13bs2KGdOnVK27dvn9a1a1fN29s741pmzJih+fr6amvWrNFiYmK0/v37awEBAVpSUpLiyHOXnJysHT58WDt8+LAGQJs1a5Z2+PBh7cyZM5qmmXc9w4cP14KCgrStW7dqv/32m9a2bVutbt26WmpqqqrLypDf9SUnJ2vjx4/X9uzZo50+fVrbvn27Fh4erlWsWNFuru/ll1/WfH19tR07dmgJCQkZ2+3btzPOsefP8EHXZ++fYUREhLZz507t9OnT2u+//65NmDBBc3V11TZv3qxpmn1/dpqW//XZ+2eXl8yzPjXN/j9De8BEzUrmzp2rVa5cWfP09NQaNGiQZXq9Penbt68WEBCgeXh4aIGBgdpTTz2lxcbGZjyenp6uTZ48WatQoYJmMBi0Rx99VIuJiVEYcf62b9+uAcixDRo0SNM0867nzp072qhRozQ/Pz+tWLFiWteuXbWzZ88quJqc8ru+27dvax07dtTKli2reXh4aJUqVdIGDRqUI3Y9X19u1wZAW7JkScY59vwZPuj67P0zfOGFFzL+LpYtW1Zr165dRpKmafb92Wla/tdn759dXrInavb+GdoDF03TNNu13xERERGRuThGjYiIiEinmKgRERER6RQTNSIiIiKdYqJGREREpFNM1IiIiIh0iokaERERkU4xUSMiIiLSKSZqRERERDrFRI2InN6OHTvg4uKCGzduqA6FiCgLJmpEREREOsVEjYiIiEinmKgRkXKapuGDDz5A1apVUaxYMdStWxfffvstAFO35Pr161G3bl14eXmhadOmiImJyfIaq1evRu3atWEwGFClShXMnDkzy+MpKSl44403EBwcDIPBgIcffhiLFy/Ocs6hQ4fQqFEjFC9eHM2bN8fx48eL9sKJiB6AiRoRKTdp0iQsWbIE8+fPR2xsLF599VU899xziI6Ozjjn9ddfx4cffogDBw6gXLly6N69O+7fvw9AEqw+ffqgX79+iImJwZQpU/DWW29h6dKlGc8fOHAgoqKiMHv2bBw7dgwLFixAyZIls8QxceJEzJw5EwcPHoS7uzteeOEFm1w/EVFeXDRN01QHQUTO69atW/D398e2bdsQHh6ecf/QoUNx+/ZtvPjii2jTpg2ioqLQt29fAMC1a9cQFBSEpUuXok+fPnj22Wdx+fJlbN68OeP5b7zxBtavX4/Y2FicOHECNWrUwJYtW9C+ffscMezYsQNt2rTB1q1b0a5dOwDAhg0b8MQTT+DOnTvw8vIq4v8KRES5Y4saESkVFxeHu3fvokOHDihZsmTGtnz5cvz9998Z52VO4vz8/FCjRg0cO3YMAHDs2DG0aNEiy+u2aNECJ0+eRFpaGo4cOQI3Nze0bt0631geeeSRjOOAgAAAwKVLlwp9jUREBeWuOgAicm7p6ekAgPXr16NixYpZHjMYDFmStexcXFwAyBg347FR5s6CYsWKmRWLh4dHjtc2xkdEpAJb1IhIqdDQUBgMBpw9exbVqlXLsgUHB2ect2/fvozj69ev48SJE6hZs2bGa+zevTvL6+7ZswfVq1eHm5sb6tSpg/T09Cxj3oiI7AFb1IhIKW9vb7z22mt49dVXkZ6ejpYtWyIpKQl79uxByZIlUblyZQDAtGnTUKZMGZQvXx4TJ06Ev78/evToAQAYP348GjdujOnTp6Nv377Yu3cv5syZg3nz5gEAqlSpgkGDBuGFF17A7NmzUbduXZw5cwaXLl1Cnz59VF06EdEDMVEjIuWmT5+OcuXKITIyEqdOnUKpUqXQoEEDTJgwIaPrccaMGRgzZgxOnjyJunXrYt26dfD09AQANGjQAF9//TXefvttTJ8+HQEBAZg2bRoGDx6c8R7z58/HhAkTMGLECFy9ehWVKlXChAkTVFwuEZHZOOuTiHTNOCPz+vXrKFWqlOpwiIhsimPUiIiIiHSKiRoRERGRTrHrk4iIiEin2KJGREREpFNM1IiIiIh0iokaERERkU4xUSMiIiLSKSZqRERERDrFRI2IiIhIp5ioEREREekUEzUiIiIinfo/sHyFkPREZDIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.plot(range(len(accuracies_2)), accuracies_2, color='blue')\n",
    "ax1.set_ylabel('accuracy (%)', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(len(losses_2)), losses_2, color='red')\n",
    "ax2.set_ylabel('loss', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.435303  [   64/19873]\n",
      "loss: 3.433383  [ 6464/19873]\n",
      "loss: 3.425956  [12864/19873]\n",
      "loss: 3.436632  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.0%, Avg loss: 3.429720 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.421176  [   64/19873]\n",
      "loss: 3.429466  [ 6464/19873]\n",
      "loss: 3.411180  [12864/19873]\n",
      "loss: 3.428868  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.5%, Avg loss: 3.422830 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.406147  [   64/19873]\n",
      "loss: 3.419096  [ 6464/19873]\n",
      "loss: 3.394997  [12864/19873]\n",
      "loss: 3.422561  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.6%, Avg loss: 3.415558 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.395527  [   64/19873]\n",
      "loss: 3.410601  [ 6464/19873]\n",
      "loss: 3.370490  [12864/19873]\n",
      "loss: 3.415933  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.9%, Avg loss: 3.407149 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.380742  [   64/19873]\n",
      "loss: 3.402539  [ 6464/19873]\n",
      "loss: 3.353793  [12864/19873]\n",
      "loss: 3.408164  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.3%, Avg loss: 3.396933 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.364247  [   64/19873]\n",
      "loss: 3.395374  [ 6464/19873]\n",
      "loss: 3.320056  [12864/19873]\n",
      "loss: 3.401972  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.384807 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.341234  [   64/19873]\n",
      "loss: 3.381009  [ 6464/19873]\n",
      "loss: 3.295383  [12864/19873]\n",
      "loss: 3.385710  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.3%, Avg loss: 3.370329 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.317808  [   64/19873]\n",
      "loss: 3.368081  [ 6464/19873]\n",
      "loss: 3.240517  [12864/19873]\n",
      "loss: 3.373111  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.9%, Avg loss: 3.352362 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.285914  [   64/19873]\n",
      "loss: 3.356534  [ 6464/19873]\n",
      "loss: 3.169676  [12864/19873]\n",
      "loss: 3.360544  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.3%, Avg loss: 3.331250 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.247414  [   64/19873]\n",
      "loss: 3.341382  [ 6464/19873]\n",
      "loss: 3.095937  [12864/19873]\n",
      "loss: 3.346057  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.6%, Avg loss: 3.305726 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 3.190787  [   64/19873]\n",
      "loss: 3.320016  [ 6464/19873]\n",
      "loss: 2.990844  [12864/19873]\n",
      "loss: 3.336720  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.9%, Avg loss: 3.274809 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 3.130690  [   64/19873]\n",
      "loss: 3.319453  [ 6464/19873]\n",
      "loss: 2.910291  [12864/19873]\n",
      "loss: 3.333508  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.3%, Avg loss: 3.243438 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 3.093174  [   64/19873]\n",
      "loss: 3.298279  [ 6464/19873]\n",
      "loss: 2.769455  [12864/19873]\n",
      "loss: 3.315609  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.8%, Avg loss: 3.213024 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 3.038980  [   64/19873]\n",
      "loss: 3.287221  [ 6464/19873]\n",
      "loss: 2.682013  [12864/19873]\n",
      "loss: 3.310664  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.3%, Avg loss: 3.187288 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 3.011700  [   64/19873]\n",
      "loss: 3.269957  [ 6464/19873]\n",
      "loss: 2.608154  [12864/19873]\n",
      "loss: 3.298039  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 3.168189 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.984171  [   64/19873]\n",
      "loss: 3.271544  [ 6464/19873]\n",
      "loss: 2.555408  [12864/19873]\n",
      "loss: 3.295652  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.6%, Avg loss: 3.152353 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.966114  [   64/19873]\n",
      "loss: 3.263028  [ 6464/19873]\n",
      "loss: 2.505580  [12864/19873]\n",
      "loss: 3.295261  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.9%, Avg loss: 3.139694 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.946092  [   64/19873]\n",
      "loss: 3.251665  [ 6464/19873]\n",
      "loss: 2.475087  [12864/19873]\n",
      "loss: 3.292845  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.6%, Avg loss: 3.127079 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.931578  [   64/19873]\n",
      "loss: 3.219734  [ 6464/19873]\n",
      "loss: 2.484648  [12864/19873]\n",
      "loss: 3.277603  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.0%, Avg loss: 3.115566 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.932744  [   64/19873]\n",
      "loss: 3.217957  [ 6464/19873]\n",
      "loss: 2.447702  [12864/19873]\n",
      "loss: 3.272774  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.5%, Avg loss: 3.105189 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.922554  [   64/19873]\n",
      "loss: 3.187448  [ 6464/19873]\n",
      "loss: 2.459144  [12864/19873]\n",
      "loss: 3.253635  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.7%, Avg loss: 3.094009 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.905022  [   64/19873]\n",
      "loss: 3.198759  [ 6464/19873]\n",
      "loss: 2.452143  [12864/19873]\n",
      "loss: 3.275882  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.8%, Avg loss: 3.083102 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.903258  [   64/19873]\n",
      "loss: 3.154045  [ 6464/19873]\n",
      "loss: 2.395744  [12864/19873]\n",
      "loss: 3.247340  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.7%, Avg loss: 3.070593 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.893383  [   64/19873]\n",
      "loss: 3.176259  [ 6464/19873]\n",
      "loss: 2.410132  [12864/19873]\n",
      "loss: 3.252085  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.9%, Avg loss: 3.058961 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.883545  [   64/19873]\n",
      "loss: 3.122593  [ 6464/19873]\n",
      "loss: 2.375305  [12864/19873]\n",
      "loss: 3.235960  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.4%, Avg loss: 3.046358 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.850118  [   64/19873]\n",
      "loss: 3.136980  [ 6464/19873]\n",
      "loss: 2.360550  [12864/19873]\n",
      "loss: 3.217020  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.3%, Avg loss: 3.032693 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.863816  [   64/19873]\n",
      "loss: 3.073217  [ 6464/19873]\n",
      "loss: 2.355459  [12864/19873]\n",
      "loss: 3.207226  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.5%, Avg loss: 3.018373 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.825307  [   64/19873]\n",
      "loss: 3.050124  [ 6464/19873]\n",
      "loss: 2.320811  [12864/19873]\n",
      "loss: 3.232358  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.8%, Avg loss: 3.004747 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.796802  [   64/19873]\n",
      "loss: 3.037454  [ 6464/19873]\n",
      "loss: 2.321666  [12864/19873]\n",
      "loss: 3.210397  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.9%, Avg loss: 2.988190 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.793423  [   64/19873]\n",
      "loss: 3.017641  [ 6464/19873]\n",
      "loss: 2.345078  [12864/19873]\n",
      "loss: 3.206713  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.2%, Avg loss: 2.973689 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.803565  [   64/19873]\n",
      "loss: 2.973118  [ 6464/19873]\n",
      "loss: 2.300521  [12864/19873]\n",
      "loss: 3.187179  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.4%, Avg loss: 2.956198 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.717770  [   64/19873]\n",
      "loss: 2.965879  [ 6464/19873]\n",
      "loss: 2.282946  [12864/19873]\n",
      "loss: 3.145153  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.7%, Avg loss: 2.940132 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.716528  [   64/19873]\n",
      "loss: 2.934612  [ 6464/19873]\n",
      "loss: 2.252134  [12864/19873]\n",
      "loss: 3.154399  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.8%, Avg loss: 2.923950 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.699731  [   64/19873]\n",
      "loss: 2.887890  [ 6464/19873]\n",
      "loss: 2.259753  [12864/19873]\n",
      "loss: 3.142485  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.9%, Avg loss: 2.904261 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.666926  [   64/19873]\n",
      "loss: 2.874518  [ 6464/19873]\n",
      "loss: 2.210109  [12864/19873]\n",
      "loss: 3.128900  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.2%, Avg loss: 2.885989 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.613228  [   64/19873]\n",
      "loss: 2.833631  [ 6464/19873]\n",
      "loss: 2.209918  [12864/19873]\n",
      "loss: 3.090231  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.4%, Avg loss: 2.868398 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.643148  [   64/19873]\n",
      "loss: 2.819476  [ 6464/19873]\n",
      "loss: 2.210443  [12864/19873]\n",
      "loss: 3.096502  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.6%, Avg loss: 2.849350 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.621670  [   64/19873]\n",
      "loss: 2.815476  [ 6464/19873]\n",
      "loss: 2.168428  [12864/19873]\n",
      "loss: 3.076034  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 2.831191 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.571206  [   64/19873]\n",
      "loss: 2.789398  [ 6464/19873]\n",
      "loss: 2.136971  [12864/19873]\n",
      "loss: 3.081536  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.8%, Avg loss: 2.811930 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.600019  [   64/19873]\n",
      "loss: 2.721188  [ 6464/19873]\n",
      "loss: 2.162649  [12864/19873]\n",
      "loss: 2.996585  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.1%, Avg loss: 2.791603 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.539439  [   64/19873]\n",
      "loss: 2.758739  [ 6464/19873]\n",
      "loss: 2.166703  [12864/19873]\n",
      "loss: 3.034481  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.3%, Avg loss: 2.773415 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.470910  [   64/19873]\n",
      "loss: 2.629654  [ 6464/19873]\n",
      "loss: 2.111783  [12864/19873]\n",
      "loss: 3.045852  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.9%, Avg loss: 2.755659 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.584340  [   64/19873]\n",
      "loss: 2.653347  [ 6464/19873]\n",
      "loss: 2.094525  [12864/19873]\n",
      "loss: 2.936937  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 17.4%, Avg loss: 2.735146 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.465574  [   64/19873]\n",
      "loss: 2.641146  [ 6464/19873]\n",
      "loss: 2.100055  [12864/19873]\n",
      "loss: 2.990763  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 17.3%, Avg loss: 2.719423 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.467986  [   64/19873]\n",
      "loss: 2.581521  [ 6464/19873]\n",
      "loss: 2.082347  [12864/19873]\n",
      "loss: 2.957105  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 17.5%, Avg loss: 2.698485 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.409384  [   64/19873]\n",
      "loss: 2.558115  [ 6464/19873]\n",
      "loss: 2.083901  [12864/19873]\n",
      "loss: 2.910581  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.0%, Avg loss: 2.680787 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.422258  [   64/19873]\n",
      "loss: 2.550656  [ 6464/19873]\n",
      "loss: 1.994004  [12864/19873]\n",
      "loss: 2.956402  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.3%, Avg loss: 2.663420 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.283886  [   64/19873]\n",
      "loss: 2.528315  [ 6464/19873]\n",
      "loss: 2.070432  [12864/19873]\n",
      "loss: 2.878716  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.3%, Avg loss: 2.648510 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.334109  [   64/19873]\n",
      "loss: 2.498611  [ 6464/19873]\n",
      "loss: 2.036412  [12864/19873]\n",
      "loss: 2.915639  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.5%, Avg loss: 2.628202 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.430035  [   64/19873]\n",
      "loss: 2.504264  [ 6464/19873]\n",
      "loss: 2.011130  [12864/19873]\n",
      "loss: 2.878916  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 2.613108 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 2.268857  [   64/19873]\n",
      "loss: 2.419435  [ 6464/19873]\n",
      "loss: 1.969747  [12864/19873]\n",
      "loss: 2.876119  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.2%, Avg loss: 2.595380 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.240798  [   64/19873]\n",
      "loss: 2.429831  [ 6464/19873]\n",
      "loss: 1.963517  [12864/19873]\n",
      "loss: 2.864640  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.5%, Avg loss: 2.578703 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 2.212887  [   64/19873]\n",
      "loss: 2.390513  [ 6464/19873]\n",
      "loss: 2.005246  [12864/19873]\n",
      "loss: 2.836380  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.9%, Avg loss: 2.561573 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.272862  [   64/19873]\n",
      "loss: 2.425971  [ 6464/19873]\n",
      "loss: 1.929882  [12864/19873]\n",
      "loss: 2.752852  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 20.5%, Avg loss: 2.546619 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 2.190085  [   64/19873]\n",
      "loss: 2.412265  [ 6464/19873]\n",
      "loss: 1.922190  [12864/19873]\n",
      "loss: 2.748745  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 20.8%, Avg loss: 2.529417 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.125372  [   64/19873]\n",
      "loss: 2.368660  [ 6464/19873]\n",
      "loss: 1.924127  [12864/19873]\n",
      "loss: 2.776886  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 21.1%, Avg loss: 2.516365 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.094884  [   64/19873]\n",
      "loss: 2.326725  [ 6464/19873]\n",
      "loss: 1.907171  [12864/19873]\n",
      "loss: 2.681341  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 21.4%, Avg loss: 2.502509 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 2.091086  [   64/19873]\n",
      "loss: 2.331502  [ 6464/19873]\n",
      "loss: 1.866216  [12864/19873]\n",
      "loss: 2.763723  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 22.0%, Avg loss: 2.485784 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 2.091828  [   64/19873]\n",
      "loss: 2.272460  [ 6464/19873]\n",
      "loss: 1.826729  [12864/19873]\n",
      "loss: 2.750047  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 22.1%, Avg loss: 2.471797 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 2.110376  [   64/19873]\n",
      "loss: 2.319203  [ 6464/19873]\n",
      "loss: 1.897441  [12864/19873]\n",
      "loss: 2.647849  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 22.3%, Avg loss: 2.458948 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 2.079826  [   64/19873]\n",
      "loss: 2.259358  [ 6464/19873]\n",
      "loss: 1.907268  [12864/19873]\n",
      "loss: 2.694066  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.3%, Avg loss: 2.445553 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 2.070531  [   64/19873]\n",
      "loss: 2.321712  [ 6464/19873]\n",
      "loss: 1.870445  [12864/19873]\n",
      "loss: 2.644352  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.1%, Avg loss: 2.434299 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.974220  [   64/19873]\n",
      "loss: 2.262561  [ 6464/19873]\n",
      "loss: 1.815862  [12864/19873]\n",
      "loss: 2.716800  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.5%, Avg loss: 2.423703 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.966478  [   64/19873]\n",
      "loss: 2.191677  [ 6464/19873]\n",
      "loss: 1.741750  [12864/19873]\n",
      "loss: 2.667395  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 2.407624 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.995752  [   64/19873]\n",
      "loss: 2.211467  [ 6464/19873]\n",
      "loss: 1.771068  [12864/19873]\n",
      "loss: 2.636520  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 2.395801 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.928159  [   64/19873]\n",
      "loss: 2.216326  [ 6464/19873]\n",
      "loss: 1.773091  [12864/19873]\n",
      "loss: 2.563038  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.2%, Avg loss: 2.381169 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.905897  [   64/19873]\n",
      "loss: 2.121317  [ 6464/19873]\n",
      "loss: 1.843947  [12864/19873]\n",
      "loss: 2.562106  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.5%, Avg loss: 2.370221 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.924825  [   64/19873]\n",
      "loss: 2.172984  [ 6464/19873]\n",
      "loss: 1.744552  [12864/19873]\n",
      "loss: 2.568248  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.9%, Avg loss: 2.360708 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.954252  [   64/19873]\n",
      "loss: 2.101141  [ 6464/19873]\n",
      "loss: 1.727708  [12864/19873]\n",
      "loss: 2.518382  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.4%, Avg loss: 2.343865 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.938911  [   64/19873]\n",
      "loss: 2.166534  [ 6464/19873]\n",
      "loss: 1.796974  [12864/19873]\n",
      "loss: 2.523363  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.4%, Avg loss: 2.334183 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.947743  [   64/19873]\n",
      "loss: 2.134845  [ 6464/19873]\n",
      "loss: 1.730815  [12864/19873]\n",
      "loss: 2.542279  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.7%, Avg loss: 2.324772 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.931748  [   64/19873]\n",
      "loss: 2.108595  [ 6464/19873]\n",
      "loss: 1.690398  [12864/19873]\n",
      "loss: 2.474205  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.2%, Avg loss: 2.315487 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.818860  [   64/19873]\n",
      "loss: 2.063519  [ 6464/19873]\n",
      "loss: 1.728021  [12864/19873]\n",
      "loss: 2.506834  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.4%, Avg loss: 2.299814 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.835712  [   64/19873]\n",
      "loss: 2.009931  [ 6464/19873]\n",
      "loss: 1.668350  [12864/19873]\n",
      "loss: 2.396073  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 2.290869 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.913054  [   64/19873]\n",
      "loss: 2.052933  [ 6464/19873]\n",
      "loss: 1.670374  [12864/19873]\n",
      "loss: 2.392363  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.8%, Avg loss: 2.278922 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.814734  [   64/19873]\n",
      "loss: 2.032506  [ 6464/19873]\n",
      "loss: 1.661774  [12864/19873]\n",
      "loss: 2.461719  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 27.2%, Avg loss: 2.270433 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.863153  [   64/19873]\n",
      "loss: 2.123239  [ 6464/19873]\n",
      "loss: 1.657315  [12864/19873]\n",
      "loss: 2.420398  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 27.2%, Avg loss: 2.259337 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 1.835639  [   64/19873]\n",
      "loss: 2.032495  [ 6464/19873]\n",
      "loss: 1.667662  [12864/19873]\n",
      "loss: 2.350670  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 27.8%, Avg loss: 2.253398 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.701149  [   64/19873]\n",
      "loss: 1.981713  [ 6464/19873]\n",
      "loss: 1.646642  [12864/19873]\n",
      "loss: 2.396535  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.2%, Avg loss: 2.240113 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.801584  [   64/19873]\n",
      "loss: 2.018407  [ 6464/19873]\n",
      "loss: 1.612653  [12864/19873]\n",
      "loss: 2.262798  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.5%, Avg loss: 2.233845 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.743808  [   64/19873]\n",
      "loss: 2.003913  [ 6464/19873]\n",
      "loss: 1.584492  [12864/19873]\n",
      "loss: 2.335727  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.5%, Avg loss: 2.220297 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1.732970  [   64/19873]\n",
      "loss: 1.863067  [ 6464/19873]\n",
      "loss: 1.610538  [12864/19873]\n",
      "loss: 2.364657  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.8%, Avg loss: 2.210589 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.781963  [   64/19873]\n",
      "loss: 2.003650  [ 6464/19873]\n",
      "loss: 1.620235  [12864/19873]\n",
      "loss: 2.299322  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.7%, Avg loss: 2.208408 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 1.748018  [   64/19873]\n",
      "loss: 1.930883  [ 6464/19873]\n",
      "loss: 1.540378  [12864/19873]\n",
      "loss: 2.301876  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 29.3%, Avg loss: 2.198759 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1.661351  [   64/19873]\n",
      "loss: 1.921940  [ 6464/19873]\n",
      "loss: 1.584822  [12864/19873]\n",
      "loss: 2.236001  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 29.5%, Avg loss: 2.181399 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.697485  [   64/19873]\n",
      "loss: 1.871634  [ 6464/19873]\n",
      "loss: 1.550197  [12864/19873]\n",
      "loss: 2.265516  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 29.6%, Avg loss: 2.185659 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1.740355  [   64/19873]\n",
      "loss: 1.867178  [ 6464/19873]\n",
      "loss: 1.570935  [12864/19873]\n",
      "loss: 2.248105  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.178717 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.660490  [   64/19873]\n",
      "loss: 1.940453  [ 6464/19873]\n",
      "loss: 1.545779  [12864/19873]\n",
      "loss: 2.238877  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.2%, Avg loss: 2.167894 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.663184  [   64/19873]\n",
      "loss: 1.844988  [ 6464/19873]\n",
      "loss: 1.555760  [12864/19873]\n",
      "loss: 2.192686  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.6%, Avg loss: 2.147909 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 1.648780  [   64/19873]\n",
      "loss: 1.845898  [ 6464/19873]\n",
      "loss: 1.508896  [12864/19873]\n",
      "loss: 2.211290  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.7%, Avg loss: 2.150726 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 1.738052  [   64/19873]\n",
      "loss: 1.804199  [ 6464/19873]\n",
      "loss: 1.525149  [12864/19873]\n",
      "loss: 2.204281  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.9%, Avg loss: 2.144012 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 1.613749  [   64/19873]\n",
      "loss: 1.730989  [ 6464/19873]\n",
      "loss: 1.502622  [12864/19873]\n",
      "loss: 2.199765  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.4%, Avg loss: 2.134980 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 1.618451  [   64/19873]\n",
      "loss: 1.830508  [ 6464/19873]\n",
      "loss: 1.549737  [12864/19873]\n",
      "loss: 2.242154  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.7%, Avg loss: 2.123661 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 1.659502  [   64/19873]\n",
      "loss: 1.787352  [ 6464/19873]\n",
      "loss: 1.438630  [12864/19873]\n",
      "loss: 2.176654  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.8%, Avg loss: 2.115735 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 1.687409  [   64/19873]\n",
      "loss: 1.759031  [ 6464/19873]\n",
      "loss: 1.418063  [12864/19873]\n",
      "loss: 2.128254  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 2.118218 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 1.648623  [   64/19873]\n",
      "loss: 1.770069  [ 6464/19873]\n",
      "loss: 1.493962  [12864/19873]\n",
      "loss: 2.130601  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.111763 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 1.629756  [   64/19873]\n",
      "loss: 1.801179  [ 6464/19873]\n",
      "loss: 1.533820  [12864/19873]\n",
      "loss: 2.145813  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 2.097606 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 1.616085  [   64/19873]\n",
      "loss: 1.696256  [ 6464/19873]\n",
      "loss: 1.432214  [12864/19873]\n",
      "loss: 2.139893  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 2.093371 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 1.573698  [   64/19873]\n",
      "loss: 1.689984  [ 6464/19873]\n",
      "loss: 1.478355  [12864/19873]\n",
      "loss: 2.164626  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.088574 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 1.608140  [   64/19873]\n",
      "loss: 1.716438  [ 6464/19873]\n",
      "loss: 1.427154  [12864/19873]\n",
      "loss: 2.102035  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 2.083086 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=256):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(71636, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "accuracies_3 = []\n",
    "losses_3 = []\n",
    "model_3 = NeuralNetwork(size=2048)\n",
    "optimizer = torch.optim.SGD(model_3.parameters(), lr=0.001)\n",
    "for e in range(100):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model_3, loss_fn, optimizer)\n",
    "    c, l = test(val_dataloader, model_3, loss_fn)\n",
    "    accuracies_3.append(c)\n",
    "    losses_3.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.535783  [   64/19873]\n",
      "loss: 1.684313  [ 6464/19873]\n",
      "loss: 1.421311  [12864/19873]\n",
      "loss: 2.104614  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 2.078802 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.609567  [   64/19873]\n",
      "loss: 1.612348  [ 6464/19873]\n",
      "loss: 1.478102  [12864/19873]\n",
      "loss: 2.135336  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 2.069864 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.584914  [   64/19873]\n",
      "loss: 1.676722  [ 6464/19873]\n",
      "loss: 1.467627  [12864/19873]\n",
      "loss: 2.050126  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 2.062798 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.592776  [   64/19873]\n",
      "loss: 1.643939  [ 6464/19873]\n",
      "loss: 1.349442  [12864/19873]\n",
      "loss: 2.036195  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 2.056390 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.629171  [   64/19873]\n",
      "loss: 1.609333  [ 6464/19873]\n",
      "loss: 1.364653  [12864/19873]\n",
      "loss: 2.014068  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 2.049933 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.429232  [   64/19873]\n",
      "loss: 1.635134  [ 6464/19873]\n",
      "loss: 1.414754  [12864/19873]\n",
      "loss: 2.123071  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 2.049309 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.614153  [   64/19873]\n",
      "loss: 1.519696  [ 6464/19873]\n",
      "loss: 1.377197  [12864/19873]\n",
      "loss: 1.998920  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 2.045309 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.477462  [   64/19873]\n",
      "loss: 1.566067  [ 6464/19873]\n",
      "loss: 1.395988  [12864/19873]\n",
      "loss: 1.973678  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 34.5%, Avg loss: 2.039913 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.453810  [   64/19873]\n",
      "loss: 1.494848  [ 6464/19873]\n",
      "loss: 1.343061  [12864/19873]\n",
      "loss: 1.976322  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 2.038051 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.505513  [   64/19873]\n",
      "loss: 1.629802  [ 6464/19873]\n",
      "loss: 1.314268  [12864/19873]\n",
      "loss: 1.923026  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 2.028474 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.478117  [   64/19873]\n",
      "loss: 1.543783  [ 6464/19873]\n",
      "loss: 1.412516  [12864/19873]\n",
      "loss: 1.841643  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 2.019984 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.474367  [   64/19873]\n",
      "loss: 1.515721  [ 6464/19873]\n",
      "loss: 1.325802  [12864/19873]\n",
      "loss: 2.011839  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 35.9%, Avg loss: 2.016675 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.496548  [   64/19873]\n",
      "loss: 1.487654  [ 6464/19873]\n",
      "loss: 1.356983  [12864/19873]\n",
      "loss: 1.904954  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 36.1%, Avg loss: 2.009001 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.452275  [   64/19873]\n",
      "loss: 1.460729  [ 6464/19873]\n",
      "loss: 1.355989  [12864/19873]\n",
      "loss: 1.925017  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 36.1%, Avg loss: 2.013749 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.521505  [   64/19873]\n",
      "loss: 1.519567  [ 6464/19873]\n",
      "loss: 1.370309  [12864/19873]\n",
      "loss: 1.867404  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 36.4%, Avg loss: 2.005257 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.485364  [   64/19873]\n",
      "loss: 1.481838  [ 6464/19873]\n",
      "loss: 1.435406  [12864/19873]\n",
      "loss: 1.914582  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 2.009918 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.470670  [   64/19873]\n",
      "loss: 1.543714  [ 6464/19873]\n",
      "loss: 1.346164  [12864/19873]\n",
      "loss: 1.932693  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 2.002399 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.480554  [   64/19873]\n",
      "loss: 1.412783  [ 6464/19873]\n",
      "loss: 1.256199  [12864/19873]\n",
      "loss: 1.878521  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 37.0%, Avg loss: 1.997214 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.443839  [   64/19873]\n",
      "loss: 1.454470  [ 6464/19873]\n",
      "loss: 1.298263  [12864/19873]\n",
      "loss: 1.829491  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 37.0%, Avg loss: 1.990086 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.425685  [   64/19873]\n",
      "loss: 1.419467  [ 6464/19873]\n",
      "loss: 1.271085  [12864/19873]\n",
      "loss: 1.897172  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 37.1%, Avg loss: 1.983476 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.435633  [   64/19873]\n",
      "loss: 1.364518  [ 6464/19873]\n",
      "loss: 1.332855  [12864/19873]\n",
      "loss: 1.857509  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 1.983549 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.469479  [   64/19873]\n",
      "loss: 1.369224  [ 6464/19873]\n",
      "loss: 1.352342  [12864/19873]\n",
      "loss: 1.812300  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 1.985075 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.382895  [   64/19873]\n",
      "loss: 1.387044  [ 6464/19873]\n",
      "loss: 1.329480  [12864/19873]\n",
      "loss: 1.857356  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 1.973963 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.442739  [   64/19873]\n",
      "loss: 1.376990  [ 6464/19873]\n",
      "loss: 1.246400  [12864/19873]\n",
      "loss: 1.790350  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 1.977367 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.374490  [   64/19873]\n",
      "loss: 1.348751  [ 6464/19873]\n",
      "loss: 1.263562  [12864/19873]\n",
      "loss: 1.792852  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 37.9%, Avg loss: 1.972105 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.401312  [   64/19873]\n",
      "loss: 1.316164  [ 6464/19873]\n",
      "loss: 1.249997  [12864/19873]\n",
      "loss: 1.823828  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 1.972001 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.383014  [   64/19873]\n",
      "loss: 1.407952  [ 6464/19873]\n",
      "loss: 1.282823  [12864/19873]\n",
      "loss: 1.764791  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.975087 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.355516  [   64/19873]\n",
      "loss: 1.324441  [ 6464/19873]\n",
      "loss: 1.215277  [12864/19873]\n",
      "loss: 1.734602  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 1.965849 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.390640  [   64/19873]\n",
      "loss: 1.272241  [ 6464/19873]\n",
      "loss: 1.191029  [12864/19873]\n",
      "loss: 1.754807  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 1.966091 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.337889  [   64/19873]\n",
      "loss: 1.364944  [ 6464/19873]\n",
      "loss: 1.237789  [12864/19873]\n",
      "loss: 1.747847  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 1.957802 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.373920  [   64/19873]\n",
      "loss: 1.395615  [ 6464/19873]\n",
      "loss: 1.218597  [12864/19873]\n",
      "loss: 1.747076  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 1.957319 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.330260  [   64/19873]\n",
      "loss: 1.263306  [ 6464/19873]\n",
      "loss: 1.203855  [12864/19873]\n",
      "loss: 1.787635  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 1.952865 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.411108  [   64/19873]\n",
      "loss: 1.360397  [ 6464/19873]\n",
      "loss: 1.262981  [12864/19873]\n",
      "loss: 1.776653  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 1.954188 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.325716  [   64/19873]\n",
      "loss: 1.275327  [ 6464/19873]\n",
      "loss: 1.202351  [12864/19873]\n",
      "loss: 1.746609  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 1.948372 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.275699  [   64/19873]\n",
      "loss: 1.226395  [ 6464/19873]\n",
      "loss: 1.224593  [12864/19873]\n",
      "loss: 1.721356  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 1.945862 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.291586  [   64/19873]\n",
      "loss: 1.225622  [ 6464/19873]\n",
      "loss: 1.256153  [12864/19873]\n",
      "loss: 1.740114  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 1.942251 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.295910  [   64/19873]\n",
      "loss: 1.204651  [ 6464/19873]\n",
      "loss: 1.223407  [12864/19873]\n",
      "loss: 1.688648  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 1.927760 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.305194  [   64/19873]\n",
      "loss: 1.280364  [ 6464/19873]\n",
      "loss: 1.228604  [12864/19873]\n",
      "loss: 1.702982  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 1.928406 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.358913  [   64/19873]\n",
      "loss: 1.292421  [ 6464/19873]\n",
      "loss: 1.154543  [12864/19873]\n",
      "loss: 1.672992  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 39.9%, Avg loss: 1.930959 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.331725  [   64/19873]\n",
      "loss: 1.214675  [ 6464/19873]\n",
      "loss: 1.171909  [12864/19873]\n",
      "loss: 1.728228  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 40.1%, Avg loss: 1.926119 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.273730  [   64/19873]\n",
      "loss: 1.247876  [ 6464/19873]\n",
      "loss: 1.194289  [12864/19873]\n",
      "loss: 1.659854  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 40.4%, Avg loss: 1.932644 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.301911  [   64/19873]\n",
      "loss: 1.203891  [ 6464/19873]\n",
      "loss: 1.255034  [12864/19873]\n",
      "loss: 1.577772  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 40.5%, Avg loss: 1.922621 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.206347  [   64/19873]\n",
      "loss: 1.250116  [ 6464/19873]\n",
      "loss: 1.183711  [12864/19873]\n",
      "loss: 1.636029  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 40.9%, Avg loss: 1.925671 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.214080  [   64/19873]\n",
      "loss: 1.165539  [ 6464/19873]\n",
      "loss: 1.220495  [12864/19873]\n",
      "loss: 1.645726  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 41.2%, Avg loss: 1.925643 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.236542  [   64/19873]\n",
      "loss: 1.299439  [ 6464/19873]\n",
      "loss: 1.102040  [12864/19873]\n",
      "loss: 1.711915  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 1.921587 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.210986  [   64/19873]\n",
      "loss: 1.131748  [ 6464/19873]\n",
      "loss: 1.171865  [12864/19873]\n",
      "loss: 1.609436  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 41.2%, Avg loss: 1.918013 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.202776  [   64/19873]\n",
      "loss: 1.262403  [ 6464/19873]\n",
      "loss: 1.127802  [12864/19873]\n",
      "loss: 1.660203  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 41.2%, Avg loss: 1.916793 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.278515  [   64/19873]\n",
      "loss: 1.205889  [ 6464/19873]\n",
      "loss: 1.094072  [12864/19873]\n",
      "loss: 1.562886  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 41.5%, Avg loss: 1.919365 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.207466  [   64/19873]\n",
      "loss: 1.162068  [ 6464/19873]\n",
      "loss: 1.156550  [12864/19873]\n",
      "loss: 1.563859  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 1.913361 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.203120  [   64/19873]\n",
      "loss: 1.224286  [ 6464/19873]\n",
      "loss: 1.115304  [12864/19873]\n",
      "loss: 1.567171  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 41.5%, Avg loss: 1.906146 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1.214256  [   64/19873]\n",
      "loss: 1.144811  [ 6464/19873]\n",
      "loss: 1.152331  [12864/19873]\n",
      "loss: 1.545613  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 1.908680 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 1.227653  [   64/19873]\n",
      "loss: 1.090759  [ 6464/19873]\n",
      "loss: 1.135738  [12864/19873]\n",
      "loss: 1.554649  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 41.8%, Avg loss: 1.908236 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 1.228782  [   64/19873]\n",
      "loss: 1.082943  [ 6464/19873]\n",
      "loss: 1.099358  [12864/19873]\n",
      "loss: 1.603329  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 41.9%, Avg loss: 1.909876 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 1.235447  [   64/19873]\n",
      "loss: 1.090941  [ 6464/19873]\n",
      "loss: 1.124535  [12864/19873]\n",
      "loss: 1.603365  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 41.9%, Avg loss: 1.902058 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 1.170432  [   64/19873]\n",
      "loss: 1.057985  [ 6464/19873]\n",
      "loss: 1.063154  [12864/19873]\n",
      "loss: 1.576117  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 41.8%, Avg loss: 1.906540 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 1.184615  [   64/19873]\n",
      "loss: 1.155178  [ 6464/19873]\n",
      "loss: 1.166093  [12864/19873]\n",
      "loss: 1.540213  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.0%, Avg loss: 1.901268 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 1.243362  [   64/19873]\n",
      "loss: 1.081444  [ 6464/19873]\n",
      "loss: 1.135483  [12864/19873]\n",
      "loss: 1.519343  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.2%, Avg loss: 1.896250 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1.235023  [   64/19873]\n",
      "loss: 1.135888  [ 6464/19873]\n",
      "loss: 1.138226  [12864/19873]\n",
      "loss: 1.552997  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.1%, Avg loss: 1.898794 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 1.229189  [   64/19873]\n",
      "loss: 1.112838  [ 6464/19873]\n",
      "loss: 1.084921  [12864/19873]\n",
      "loss: 1.500813  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.2%, Avg loss: 1.898583 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 1.082762  [   64/19873]\n",
      "loss: 1.107365  [ 6464/19873]\n",
      "loss: 1.021735  [12864/19873]\n",
      "loss: 1.508177  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.2%, Avg loss: 1.904121 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.075777  [   64/19873]\n",
      "loss: 1.039344  [ 6464/19873]\n",
      "loss: 1.154962  [12864/19873]\n",
      "loss: 1.482751  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.2%, Avg loss: 1.894921 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1.127590  [   64/19873]\n",
      "loss: 1.080240  [ 6464/19873]\n",
      "loss: 1.082203  [12864/19873]\n",
      "loss: 1.518229  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.2%, Avg loss: 1.896609 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.157824  [   64/19873]\n",
      "loss: 1.082340  [ 6464/19873]\n",
      "loss: 1.098831  [12864/19873]\n",
      "loss: 1.451897  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.1%, Avg loss: 1.895178 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.060610  [   64/19873]\n",
      "loss: 1.019445  [ 6464/19873]\n",
      "loss: 1.063000  [12864/19873]\n",
      "loss: 1.444841  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.3%, Avg loss: 1.896219 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.116089  [   64/19873]\n",
      "loss: 1.015561  [ 6464/19873]\n",
      "loss: 1.047065  [12864/19873]\n",
      "loss: 1.553854  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.3%, Avg loss: 1.894153 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.187774  [   64/19873]\n",
      "loss: 0.987160  [ 6464/19873]\n",
      "loss: 1.053425  [12864/19873]\n",
      "loss: 1.416138  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.4%, Avg loss: 1.890730 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.194909  [   64/19873]\n",
      "loss: 1.052289  [ 6464/19873]\n",
      "loss: 1.085345  [12864/19873]\n",
      "loss: 1.517441  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.3%, Avg loss: 1.887204 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.110998  [   64/19873]\n",
      "loss: 0.984649  [ 6464/19873]\n",
      "loss: 1.047128  [12864/19873]\n",
      "loss: 1.448231  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.7%, Avg loss: 1.893198 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.158316  [   64/19873]\n",
      "loss: 0.972389  [ 6464/19873]\n",
      "loss: 1.001850  [12864/19873]\n",
      "loss: 1.570697  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.6%, Avg loss: 1.889252 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.122924  [   64/19873]\n",
      "loss: 0.959186  [ 6464/19873]\n",
      "loss: 1.059629  [12864/19873]\n",
      "loss: 1.381585  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.881059 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.115024  [   64/19873]\n",
      "loss: 1.031832  [ 6464/19873]\n",
      "loss: 0.988406  [12864/19873]\n",
      "loss: 1.415732  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.5%, Avg loss: 1.885458 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.052644  [   64/19873]\n",
      "loss: 0.978531  [ 6464/19873]\n",
      "loss: 0.993034  [12864/19873]\n",
      "loss: 1.448784  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 1.889344 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.037181  [   64/19873]\n",
      "loss: 0.961133  [ 6464/19873]\n",
      "loss: 1.106063  [12864/19873]\n",
      "loss: 1.363179  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 1.883285 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.045710  [   64/19873]\n",
      "loss: 0.963710  [ 6464/19873]\n",
      "loss: 0.988142  [12864/19873]\n",
      "loss: 1.435473  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 1.879879 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.018458  [   64/19873]\n",
      "loss: 0.991653  [ 6464/19873]\n",
      "loss: 1.031168  [12864/19873]\n",
      "loss: 1.432968  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 1.879519 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.036943  [   64/19873]\n",
      "loss: 0.940058  [ 6464/19873]\n",
      "loss: 1.031020  [12864/19873]\n",
      "loss: 1.442426  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 1.876496 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.042622  [   64/19873]\n",
      "loss: 0.973290  [ 6464/19873]\n",
      "loss: 0.922959  [12864/19873]\n",
      "loss: 1.382292  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 1.875254 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.979031  [   64/19873]\n",
      "loss: 0.941788  [ 6464/19873]\n",
      "loss: 0.983811  [12864/19873]\n",
      "loss: 1.404713  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 1.879071 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.987135  [   64/19873]\n",
      "loss: 0.923926  [ 6464/19873]\n",
      "loss: 0.954803  [12864/19873]\n",
      "loss: 1.383454  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 1.875722 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.064123  [   64/19873]\n",
      "loss: 0.865902  [ 6464/19873]\n",
      "loss: 1.009872  [12864/19873]\n",
      "loss: 1.323792  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.872380 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.037172  [   64/19873]\n",
      "loss: 0.888515  [ 6464/19873]\n",
      "loss: 0.999834  [12864/19873]\n",
      "loss: 1.432793  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 1.866943 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.971866  [   64/19873]\n",
      "loss: 0.906017  [ 6464/19873]\n",
      "loss: 0.942676  [12864/19873]\n",
      "loss: 1.337381  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.874146 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.111949  [   64/19873]\n",
      "loss: 0.884060  [ 6464/19873]\n",
      "loss: 0.926968  [12864/19873]\n",
      "loss: 1.339125  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 43.9%, Avg loss: 1.874160 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 1.164527  [   64/19873]\n",
      "loss: 0.879202  [ 6464/19873]\n",
      "loss: 0.979061  [12864/19873]\n",
      "loss: 1.343262  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 43.9%, Avg loss: 1.884528 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.989338  [   64/19873]\n",
      "loss: 0.891860  [ 6464/19873]\n",
      "loss: 0.950876  [12864/19873]\n",
      "loss: 1.346985  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 44.2%, Avg loss: 1.868985 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.033519  [   64/19873]\n",
      "loss: 0.925648  [ 6464/19873]\n",
      "loss: 0.928984  [12864/19873]\n",
      "loss: 1.324363  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 44.3%, Avg loss: 1.866132 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1.090330  [   64/19873]\n",
      "loss: 0.893820  [ 6464/19873]\n",
      "loss: 0.933925  [12864/19873]\n",
      "loss: 1.302675  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 44.1%, Avg loss: 1.869856 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.065222  [   64/19873]\n",
      "loss: 0.877905  [ 6464/19873]\n",
      "loss: 0.889954  [12864/19873]\n",
      "loss: 1.346922  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 44.1%, Avg loss: 1.870070 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.040926  [   64/19873]\n",
      "loss: 0.902897  [ 6464/19873]\n",
      "loss: 0.899713  [12864/19873]\n",
      "loss: 1.295712  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 44.5%, Avg loss: 1.867221 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.967725  [   64/19873]\n",
      "loss: 0.818250  [ 6464/19873]\n",
      "loss: 0.930239  [12864/19873]\n",
      "loss: 1.359636  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 44.5%, Avg loss: 1.874347 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.986100  [   64/19873]\n",
      "loss: 0.858546  [ 6464/19873]\n",
      "loss: 0.951899  [12864/19873]\n",
      "loss: 1.332807  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 44.8%, Avg loss: 1.871810 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.932137  [   64/19873]\n",
      "loss: 0.839697  [ 6464/19873]\n",
      "loss: 0.939308  [12864/19873]\n",
      "loss: 1.277972  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 44.8%, Avg loss: 1.863221 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.962396  [   64/19873]\n",
      "loss: 0.918713  [ 6464/19873]\n",
      "loss: 0.951303  [12864/19873]\n",
      "loss: 1.336983  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 44.8%, Avg loss: 1.863817 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.968039  [   64/19873]\n",
      "loss: 0.872676  [ 6464/19873]\n",
      "loss: 0.964770  [12864/19873]\n",
      "loss: 1.301489  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 1.867449 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 1.030325  [   64/19873]\n",
      "loss: 0.816845  [ 6464/19873]\n",
      "loss: 0.917938  [12864/19873]\n",
      "loss: 1.260971  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 1.872254 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.975350  [   64/19873]\n",
      "loss: 0.787286  [ 6464/19873]\n",
      "loss: 0.839276  [12864/19873]\n",
      "loss: 1.209123  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 1.866006 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.960812  [   64/19873]\n",
      "loss: 0.769598  [ 6464/19873]\n",
      "loss: 0.889244  [12864/19873]\n",
      "loss: 1.226518  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.4%, Avg loss: 1.868773 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 1.003316  [   64/19873]\n",
      "loss: 0.787013  [ 6464/19873]\n",
      "loss: 0.884484  [12864/19873]\n",
      "loss: 1.326300  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 1.862848 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.964880  [   64/19873]\n",
      "loss: 0.850857  [ 6464/19873]\n",
      "loss: 0.899384  [12864/19873]\n",
      "loss: 1.263699  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 1.866090 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.952359  [   64/19873]\n",
      "loss: 0.799211  [ 6464/19873]\n",
      "loss: 0.858194  [12864/19873]\n",
      "loss: 1.269633  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 1.862687 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model_3.parameters(), lr=0.001)\n",
    "for e in range(100):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model_3, loss_fn, optimizer)\n",
    "    c, l = test(val_dataloader, model_3, loss_fn)\n",
    "    accuracies_3.append(c)\n",
    "    losses_3.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.plot(range(len(accuracies_3)), accuracies_3, color='blue')\n",
    "ax1.set_ylabel('accuracy (%)', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(len(losses_3)), losses_3, color='red')\n",
    "ax2.set_ylabel('loss', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 266636])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=200000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(train['snip'])\n",
    "X_val_tfidf = tfidf.transform(val['snip'])\n",
    "\n",
    "x_train = [X_train_tfidf, x_train_cv]\n",
    "x_val = [X_val_tfidf, x_val_cv]\n",
    "# Combine TF-IDF and CountVectorizer features\n",
    "from scipy.sparse import hstack\n",
    "x_train_combined = hstack(x_train)\n",
    "x_val_combined = hstack(x_val)\n",
    "\n",
    "training_data = torch.tensor(x_train_combined.toarray(), dtype=torch.float32)\n",
    "val_data = torch.tensor(x_val_combined.toarray(), dtype=torch.float32)\n",
    "train_tensor = torch.utils.data.TensorDataset(training_data, training_labels)\n",
    "val_tensor = torch.utils.data.TensorDataset(val_data, val_labels)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_tensor, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_tensor, batch_size=batch_size)\n",
    "\n",
    "for X, y in val_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.433814  [   64/19873]\n",
      "loss: 3.403761  [ 6464/19873]\n",
      "loss: 3.288266  [12864/19873]\n",
      "loss: 3.335662  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg loss: 3.287629 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.203996  [   64/19873]\n",
      "loss: 3.273270  [ 6464/19873]\n",
      "loss: 2.503496  [12864/19873]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mtrain_fxn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_new_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     c, l \u001b[38;5;241m=\u001b[39m test(val_dataloader, model_new_param, loss_fn)\n\u001b[1;32m     26\u001b[0m     accuracies_new_param\u001b[38;5;241m.\u001b[39mappend(c)\n",
      "Cell \u001b[0;32mIn[37], line 14\u001b[0m, in \u001b[0;36mtrain_fxn\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=512):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(266636, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model_new_param = NeuralNetwork(size=2048)\n",
    "optimizer = torch.optim.SGD(model_new_param.parameters(), lr=0.01)\n",
    "accuracies_new_param = []\n",
    "losses_new_param = []\n",
    "for e in range(100):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model_new_param, loss_fn, optimizer)\n",
    "    c, l = test(val_dataloader, model_new_param, loss_fn)\n",
    "    accuracies_new_param.append(c)\n",
    "    losses_new_param.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
