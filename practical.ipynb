{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONE\n",
    "Step 1: Preprocess the data - use the same function across the board\n",
    "Clearing out the capitalization\n",
    "Clearing out em dashes, symbols\n",
    "Clearing out names\n",
    "\n",
    "TO DO\n",
    "Step 2: Implementing the features\n",
    "- Goal: Train a logistic regression on 2 feature representations\n",
    "\n",
    "Step 3: Implementing the regression based on that data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Imports and Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kate/anaconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "val = pd.read_csv('data/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for cleaning text\n",
    "def clean_html(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # Remove HTML tags\n",
    "    clean = re.sub(r'<.*?>', '', str(text))\n",
    "    # Remove extra whitespaces\n",
    "    clean = re.sub(r'\\s+', ' ', clean).strip()\n",
    "    # Replace HTML entities\n",
    "    clean = re.sub(r'&amp;', '&', clean)\n",
    "    clean = re.sub(r'&lt;', '<', clean)\n",
    "    clean = re.sub(r'&gt;', '>', clean)\n",
    "    clean = re.sub(r'&quot;|&#34;', '\"', clean)\n",
    "    clean = re.sub(r'&apos;|&#39;', \"'\", clean)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    snip   channel\n",
      "0      first of all, it feels like covid again but in...  FOXNEWSW\n",
      "1      to be a software drivenrganization where softw...     CSPAN\n",
      "2      you discuss the power of ai to revolutionize t...    CSPAN2\n",
      "3      ai bots like chatgpt and google's bard gained ...   BBCNEWS\n",
      "4      . >> i could sleep ten hours ai night if i was...  FOXNEWSW\n",
      "...                                                  ...       ...\n",
      "19868  cardiovascular science, but they're also pione...  FOXNEWSW\n",
      "19869  i of ai in different fields. have of ai in dif...   BBCNEWS\n",
      "19870  weighing down on the major averages, both tech...      KTVU\n",
      "19871  i also think crypto ai that legislation be fro...    CSPAN2\n",
      "19872  as we have worked to monitor the adoption iden...    CSPAN2\n",
      "\n",
      "[19873 rows x 2 columns]\n",
      "                                                   snip    channel\n",
      "0     . ♪ >> there's a kyu cho right have things tha...  BLOOMBERG\n",
      "1     he says the ai tool helped create a new fronti...       KPIX\n",
      "2     . >> the all new godaddy arrow put your busine...       CNNW\n",
      "3     in some cases they are powered by generative a...      CSPAN\n",
      "4     this was a ivotal it comes to ai. this was a p...    BBCNEWS\n",
      "...                                                 ...        ...\n",
      "3034  however, the ai trade is only one part of the ...       CNBC\n",
      "3035  oz but also was highlighted as a product by cr...     CSPAN2\n",
      "3036  the all new godaddy airo helps you get your bu...       CNBC\n",
      "3037  we are going to be way ahead on ai. we have to...       CNBC\n",
      "3038  his fourth management role after spells at der...    BBCNEWS\n",
      "\n",
      "[3039 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# use the clean_html function to clean the training data\n",
    "train['snip'] = train['snip'].apply(clean_html)\n",
    "val['snip'] = val['snip'].apply(clean_html)\n",
    "\n",
    "print(train)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metric equation\n",
    "def eval(y_pred, y_true):\n",
    "    correct = (y_pred == y_true)   # Boolean array: True if correct, False if wrong\n",
    "    accuracy = correct.sum() / len(y_true)  # Correct / Total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    snip   channel  \\\n",
      "0      first of all, it feels like covid again but in...  FOXNEWSW   \n",
      "1      to be a software drivenrganization where softw...     CSPAN   \n",
      "2      you discuss the power of ai to revolutionize t...    CSPAN2   \n",
      "3      ai bots like chatgpt and google's bard gained ...   BBCNEWS   \n",
      "4      . >> i could sleep ten hours ai night if i was...  FOXNEWSW   \n",
      "...                                                  ...       ...   \n",
      "19868  cardiovascular science, but they're also pione...  FOXNEWSW   \n",
      "19869  i of ai in different fields. have of ai in dif...   BBCNEWS   \n",
      "19870  weighing down on the major averages, both tech...      KTVU   \n",
      "19871  i also think crypto ai that legislation be fro...    CSPAN2   \n",
      "19872  as we have worked to monitor the adoption iden...    CSPAN2   \n",
      "\n",
      "       word_complexity  \n",
      "0             0.057407  \n",
      "1             0.079768  \n",
      "2             0.076151  \n",
      "3             0.077301  \n",
      "4             0.075782  \n",
      "...                ...  \n",
      "19868         0.080295  \n",
      "19869         0.093252  \n",
      "19870         0.080397  \n",
      "19871         0.079097  \n",
      "19872         0.080723  \n",
      "\n",
      "[19873 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(train['snip'])\n",
    "\n",
    "# Calculate word complexity per snip\n",
    "word_complexity = X_tfidf.sum(axis=1) / (X_tfidf != 0).sum(axis=1)\n",
    "word_complexity = np.array(word_complexity).flatten()\n",
    "\n",
    "# Add it to the train dataframe\n",
    "train['word_complexity'] = word_complexity\n",
    "\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   snip    channel  \\\n",
      "0     . ♪ >> there's a kyu cho right have things tha...  BLOOMBERG   \n",
      "1     he says the ai tool helped create a new fronti...       KPIX   \n",
      "2     . >> the all new godaddy arrow put your busine...       CNNW   \n",
      "3     in some cases they are powered by generative a...      CSPAN   \n",
      "4     this was a ivotal it comes to ai. this was a p...    BBCNEWS   \n",
      "...                                                 ...        ...   \n",
      "3034  however, the ai trade is only one part of the ...       CNBC   \n",
      "3035  oz but also was highlighted as a product by cr...     CSPAN2   \n",
      "3036  the all new godaddy airo helps you get your bu...       CNBC   \n",
      "3037  we are going to be way ahead on ai. we have to...       CNBC   \n",
      "3038  his fourth management role after spells at der...    BBCNEWS   \n",
      "\n",
      "      word_complexity  \n",
      "0            0.088216  \n",
      "1            0.081808  \n",
      "2            0.074722  \n",
      "3            0.082919  \n",
      "4            0.083567  \n",
      "...               ...  \n",
      "3034         0.079930  \n",
      "3035         0.081606  \n",
      "3036         0.080997  \n",
      "3037         0.078856  \n",
      "3038         0.075032  \n",
      "\n",
      "[3039 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Transform validation snips using the same TF-IDF vectorizer\n",
    "x_val_tfidf = vectorizer.transform(val['snip'])\n",
    "\n",
    "# Compute complexity\n",
    "val_word_complexity = x_val_tfidf.sum(axis=1) / (x_val_tfidf != 0).sum(axis=1)\n",
    "val_word_complexity = np.array(val_word_complexity).flatten()\n",
    "\n",
    "# Add to val DataFrame\n",
    "val['word_complexity'] = val_word_complexity\n",
    "\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = features, y = labels\n",
    "x_train = train[['word_complexity']]  # Needs to be 2D\n",
    "y_train = train['channel']\n",
    "\n",
    "x_val = val[['word_complexity']]\n",
    "y_val = val['channel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNNW' 'CNNW' 'CNNW' ... 'CNNW' 'CNNW' 'CNNW']\n",
      "0.07535373478117802\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "val_preds = model.predict(x_val)\n",
    "print(val_preds)\n",
    "\n",
    "# Evaluate\n",
    "val_accuracy = eval(val_preds, y_val)\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now do the min/max training to create new average values of scores\n",
    "\n",
    "scaled = current - min/ max-min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0.057407\n",
      "1        0.079768\n",
      "2        0.076151\n",
      "3        0.077301\n",
      "4        0.075782\n",
      "           ...   \n",
      "19868    0.080295\n",
      "19869    0.093252\n",
      "19870    0.080397\n",
      "19871    0.079097\n",
      "19872    0.080723\n",
      "Name: word_complexity, Length: 19873, dtype: float64\n",
      "0.035929255415410526 0.6822632439493239\n"
     ]
    }
   ],
   "source": [
    "min = 1\n",
    "max = 0\n",
    "\n",
    "complexity = train['word_complexity']\n",
    "print(complexity)\n",
    "\n",
    "for i in range(len(complexity)):\n",
    "    if complexity[i] <= min:\n",
    "        min = complexity[i]\n",
    "    if complexity[i] >= max:\n",
    "        max = complexity[i]\n",
    "    else:\n",
    "        min = min\n",
    "        max = max\n",
    "\n",
    "print(min,max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complexity is 2D\n",
    "def scale(complexity):\n",
    "    x_train = (train[['word_complexity']] - min )/ (max-min)\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_complexity = train[\"word_complexity\"].min()\n",
    "max_complexity = train[\"word_complexity\"].max()\n",
    "train[\"word_complexity_scaled\"] = (train[\"word_complexity\"] - min_complexity) / (max_complexity - min_complexity)\n",
    "val[\"word_complexity_scaled\"] = (val[\"word_complexity\"] - min_complexity) / (max_complexity - min_complexity)\n",
    "\n",
    "# X = features, y = labels\n",
    "x_train = train[['word_complexity_scaled']]\n",
    "y_train = train['channel']\n",
    "\n",
    "x_val = val[['word_complexity_scaled']]\n",
    "y_val = val['channel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNNW' 'CNNW' 'CNNW' ... 'CNNW' 'CNNW' 'CNNW']\n",
      "0.07666995722277065\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "val_preds = model.predict(x_val)\n",
    "print(val_preds)\n",
    "\n",
    "# Evaluate\n",
    "val_accuracy = eval(val_preds, y_val)\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FOXNEWSW', 'CSPAN', 'CSPAN2', 'BBCNEWS', 'GBN', 'KPIX', 'KGO', 'KNTV', '1TV', 'KRON', 'CSPAN3', 'SFGTV', 'RUSSIA24', 'KSTS', 'BLOOMBERG', 'MSNBCW', 'PRESSTV', 'KTVU', 'CNNW', 'FBC', 'CNBC', 'RUSSIA1', 'KDTV', 'DW', 'KQED', 'NTV', 'BELARUSTV', 'ALJAZ', 'RT', 'LINKTV', 'COM']\n"
     ]
    }
   ],
   "source": [
    "# create list of all of the channels\n",
    "channels = []\n",
    "\n",
    "for i in range(len(train)):\n",
    "    if train['channel'][i] not in channels:\n",
    "        channels.append(train['channel'][i])\n",
    "    else:\n",
    "        channels = channels\n",
    "\n",
    "# \n",
    "\n",
    "print(channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "train_corpus = list(train['snip'].values)\n",
    "val_corpus = list(val['snip'].values)\n",
    "nlp_sentiment = pipeline(\"sentiment-analysis\")\n",
    "train[\"Sentiment\"] = nlp_sentiment(train_corpus)\n",
    "val[\"Sentiment\"] = nlp_sentiment(val_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snip</th>\n",
       "      <th>channel</th>\n",
       "      <th>word_complexity</th>\n",
       "      <th>word_complexity_scaled</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Label</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first of all, it feels like covid again but in...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.057407</td>\n",
       "      <td>0.033231</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9903525710105...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.990353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to be a software drivenrganization where softw...</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>0.079768</td>\n",
       "      <td>0.067826</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9933253526687...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.993325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you discuss the power of ai to revolutionize t...</td>\n",
       "      <td>CSPAN2</td>\n",
       "      <td>0.076151</td>\n",
       "      <td>0.062230</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9971946477890...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.997195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai bots like chatgpt and google's bard gained ...</td>\n",
       "      <td>BBCNEWS</td>\n",
       "      <td>0.077301</td>\n",
       "      <td>0.064009</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9900817275047...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.990082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>. &gt;&gt; i could sleep ten hours ai night if i was...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.075782</td>\n",
       "      <td>0.061660</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.8428422808647...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.842842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                snip   channel  \\\n",
       "0  first of all, it feels like covid again but in...  FOXNEWSW   \n",
       "1  to be a software drivenrganization where softw...     CSPAN   \n",
       "2  you discuss the power of ai to revolutionize t...    CSPAN2   \n",
       "3  ai bots like chatgpt and google's bard gained ...   BBCNEWS   \n",
       "4  . >> i could sleep ten hours ai night if i was...  FOXNEWSW   \n",
       "\n",
       "   word_complexity  word_complexity_scaled  \\\n",
       "0         0.057407                0.033231   \n",
       "1         0.079768                0.067826   \n",
       "2         0.076151                0.062230   \n",
       "3         0.077301                0.064009   \n",
       "4         0.075782                0.061660   \n",
       "\n",
       "                                           Sentiment Sentiment_Label  \\\n",
       "0  {'label': 'NEGATIVE', 'score': 0.9903525710105...        NEGATIVE   \n",
       "1  {'label': 'POSITIVE', 'score': 0.9933253526687...        POSITIVE   \n",
       "2  {'label': 'NEGATIVE', 'score': 0.9971946477890...        NEGATIVE   \n",
       "3  {'label': 'POSITIVE', 'score': 0.9900817275047...        POSITIVE   \n",
       "4  {'label': 'NEGATIVE', 'score': 0.8428422808647...        NEGATIVE   \n",
       "\n",
       "   Sentiment_Score  \n",
       "0         0.990353  \n",
       "1         0.993325  \n",
       "2         0.997195  \n",
       "3         0.990082  \n",
       "4         0.842842  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Sentiment_Label'] = [x.get('label') for x in train['Sentiment']]\n",
    "train['Sentiment_Score'] = [x.get('score') for x in train['Sentiment']]\n",
    "val['Sentiment_Label'] = [x.get('label') for x in val['Sentiment']]\n",
    "val['Sentiment_Score'] = [x.get('score') for x in val['Sentiment']]\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snip</th>\n",
       "      <th>channel</th>\n",
       "      <th>word_complexity</th>\n",
       "      <th>word_complexity_scaled</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Label</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first of all, it feels like covid again but in...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.057407</td>\n",
       "      <td>0.033231</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9903525710105...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.990353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to be a software drivenrganization where softw...</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>0.079768</td>\n",
       "      <td>0.067826</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9933253526687...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.993325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you discuss the power of ai to revolutionize t...</td>\n",
       "      <td>CSPAN2</td>\n",
       "      <td>0.076151</td>\n",
       "      <td>0.062230</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9971946477890...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.997195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai bots like chatgpt and google's bard gained ...</td>\n",
       "      <td>BBCNEWS</td>\n",
       "      <td>0.077301</td>\n",
       "      <td>0.064009</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9900817275047...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.990082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>. &gt;&gt; i could sleep ten hours ai night if i was...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.075782</td>\n",
       "      <td>0.061660</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.8428422808647...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.842842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                snip   channel  \\\n",
       "0  first of all, it feels like covid again but in...  FOXNEWSW   \n",
       "1  to be a software drivenrganization where softw...     CSPAN   \n",
       "2  you discuss the power of ai to revolutionize t...    CSPAN2   \n",
       "3  ai bots like chatgpt and google's bard gained ...   BBCNEWS   \n",
       "4  . >> i could sleep ten hours ai night if i was...  FOXNEWSW   \n",
       "\n",
       "   word_complexity  word_complexity_scaled  \\\n",
       "0         0.057407                0.033231   \n",
       "1         0.079768                0.067826   \n",
       "2         0.076151                0.062230   \n",
       "3         0.077301                0.064009   \n",
       "4         0.075782                0.061660   \n",
       "\n",
       "                                           Sentiment Sentiment_Label  \\\n",
       "0  {'label': 'NEGATIVE', 'score': 0.9903525710105...        NEGATIVE   \n",
       "1  {'label': 'POSITIVE', 'score': 0.9933253526687...        POSITIVE   \n",
       "2  {'label': 'NEGATIVE', 'score': 0.9971946477890...        NEGATIVE   \n",
       "3  {'label': 'POSITIVE', 'score': 0.9900817275047...        POSITIVE   \n",
       "4  {'label': 'NEGATIVE', 'score': 0.8428422808647...        NEGATIVE   \n",
       "\n",
       "   Sentiment_Score  \n",
       "0        -0.990353  \n",
       "1         0.993325  \n",
       "2        -0.997195  \n",
       "3         0.990082  \n",
       "4        -0.842842  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Sentiment_Score\"] = np.where(\n",
    "    train[\"Sentiment_Label\"] == \"NEGATIVE\", -(train[\"Sentiment_Score\"]), train[\"Sentiment_Score\"]\n",
    ")\n",
    "\n",
    "val[\"Sentiment_Score\"] = np.where(\n",
    "    val[\"Sentiment_Label\"] == \"NEGATIVE\", -(val[\"Sentiment_Score\"]), val[\"Sentiment_Score\"]\n",
    ")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snip</th>\n",
       "      <th>channel</th>\n",
       "      <th>word_complexity</th>\n",
       "      <th>word_complexity_scaled</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Label</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <th>Sentiment_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first of all, it feels like covid again but in...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.057407</td>\n",
       "      <td>0.033231</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9903525710105...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.990353</td>\n",
       "      <td>0.004713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to be a software drivenrganization where softw...</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>0.079768</td>\n",
       "      <td>0.067826</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9933253526687...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.993325</td>\n",
       "      <td>0.996727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you discuss the power of ai to revolutionize t...</td>\n",
       "      <td>CSPAN2</td>\n",
       "      <td>0.076151</td>\n",
       "      <td>0.062230</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9971946477890...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.997195</td>\n",
       "      <td>0.001292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai bots like chatgpt and google's bard gained ...</td>\n",
       "      <td>BBCNEWS</td>\n",
       "      <td>0.077301</td>\n",
       "      <td>0.064009</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9900817275047...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.990082</td>\n",
       "      <td>0.995105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>. &gt;&gt; i could sleep ten hours ai night if i was...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.075782</td>\n",
       "      <td>0.061660</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.8428422808647...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.842842</td>\n",
       "      <td>0.078481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                snip   channel  \\\n",
       "0  first of all, it feels like covid again but in...  FOXNEWSW   \n",
       "1  to be a software drivenrganization where softw...     CSPAN   \n",
       "2  you discuss the power of ai to revolutionize t...    CSPAN2   \n",
       "3  ai bots like chatgpt and google's bard gained ...   BBCNEWS   \n",
       "4  . >> i could sleep ten hours ai night if i was...  FOXNEWSW   \n",
       "\n",
       "   word_complexity  word_complexity_scaled  \\\n",
       "0         0.057407                0.033231   \n",
       "1         0.079768                0.067826   \n",
       "2         0.076151                0.062230   \n",
       "3         0.077301                0.064009   \n",
       "4         0.075782                0.061660   \n",
       "\n",
       "                                           Sentiment Sentiment_Label  \\\n",
       "0  {'label': 'NEGATIVE', 'score': 0.9903525710105...        NEGATIVE   \n",
       "1  {'label': 'POSITIVE', 'score': 0.9933253526687...        POSITIVE   \n",
       "2  {'label': 'NEGATIVE', 'score': 0.9971946477890...        NEGATIVE   \n",
       "3  {'label': 'POSITIVE', 'score': 0.9900817275047...        POSITIVE   \n",
       "4  {'label': 'NEGATIVE', 'score': 0.8428422808647...        NEGATIVE   \n",
       "\n",
       "   Sentiment_Score  Sentiment_scaled  \n",
       "0        -0.990353          0.004713  \n",
       "1         0.993325          0.996727  \n",
       "2        -0.997195          0.001292  \n",
       "3         0.990082          0.995105  \n",
       "4        -0.842842          0.078481  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_sentiment = train[\"Sentiment_Score\"].min()\n",
    "max_sentiment = train[\"Sentiment_Score\"].max()\n",
    "\n",
    "train[\"Sentiment_scaled\"] = (train[\"Sentiment_Score\"] - min_sentiment) / (max_sentiment - min_sentiment)\n",
    "val[\"Sentiment_scaled\"] = (val[\"Sentiment_Score\"] - min_sentiment) / (max_sentiment - min_sentiment)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNNW' 'CNNW' 'CNNW' ... 'CNNW' 'CNNW' 'CNNW']\n",
      "0.07634090161237249\n"
     ]
    }
   ],
   "source": [
    "x_train = train[['word_complexity_scaled', 'Sentiment_scaled']]  # Needs to be 2D\n",
    "x_val = val[['word_complexity_scaled', 'Sentiment_scaled']]\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "val_preds = model.predict(x_val)\n",
    "print(val_preds)\n",
    "\n",
    "# Evaluate\n",
    "val_accuracy = eval(val_preds, y_val)\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "channel\n",
      "CNNW         2725\n",
      "FBC          1608\n",
      "BBCNEWS      1576\n",
      "BLOOMBERG    1441\n",
      "CNBC         1319\n",
      "MSNBCW       1141\n",
      "FOXNEWSW     1106\n",
      "CSPAN         913\n",
      "CSPAN2        904\n",
      "KNTV          842\n",
      "KTVU          766\n",
      "KGO           763\n",
      "KRON          760\n",
      "GBN           717\n",
      "CSPAN3        664\n",
      "KPIX          400\n",
      "SFGTV         360\n",
      "DW            253\n",
      "ALJAZ         230\n",
      "NTV           209\n",
      "KDTV          176\n",
      "1TV           158\n",
      "KSTS          142\n",
      "RUSSIA24      122\n",
      "PRESSTV       115\n",
      "KQED          113\n",
      "BELARUSTV     109\n",
      "RUSSIA1       108\n",
      "RT             50\n",
      "LINKTV         43\n",
      "COM            40\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(train[\"channel\"].value_counts()))\n",
    "print(train[\"channel\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1TV': 0, 'ALJAZ': 1, 'BBCNEWS': 2, 'BELARUSTV': 3, 'BLOOMBERG': 4, 'CNBC': 5, 'CNNW': 6, 'COM': 7, 'CSPAN': 8, 'CSPAN2': 9, 'CSPAN3': 10, 'DW': 11, 'FBC': 12, 'FOXNEWSW': 13, 'GBN': 14, 'KDTV': 15, 'KGO': 16, 'KNTV': 17, 'KPIX': 18, 'KQED': 19, 'KRON': 20, 'KSTS': 21, 'KTVU': 22, 'LINKTV': 23, 'MSNBCW': 24, 'NTV': 25, 'PRESSTV': 26, 'RT': 27, 'RUSSIA1': 28, 'RUSSIA24': 29, 'SFGTV': 30}\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train['channel'])\n",
    "training_labels = torch.tensor(encoder.transform(train['channel']))\n",
    "val_labels = torch.tensor(encoder.transform(val['channel']))\n",
    "\n",
    "class_mapping = dict(zip(encoder.classes_, range(len(encoder.classes_))))\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 2])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "training_data = torch.tensor(train[['word_complexity_scaled', 'Sentiment_scaled']].values)\n",
    "val_data = torch.tensor(val[['word_complexity_scaled', 'Sentiment_scaled']].values)\n",
    "train_tensor = torch.utils.data.TensorDataset(training_data, training_labels)\n",
    "val_tensor = torch.utils.data.TensorDataset(val_data, val_labels)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_tensor, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_tensor, batch_size=batch_size)\n",
    "\n",
    "for X, y in val_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=31, bias=True)\n",
      "  )\n",
      ")\n",
      "Predicted class: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "X = torch.rand(1, 2)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.419219  [   64/19873]\n",
      "loss: 3.365708  [ 6464/19873]\n",
      "loss: 3.298280  [12864/19873]\n",
      "loss: 3.198684  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.282800 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.359068  [   64/19873]\n",
      "loss: 3.266758  [ 6464/19873]\n",
      "loss: 3.176203  [12864/19873]\n",
      "loss: 3.025360  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.195135 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.332412  [   64/19873]\n",
      "loss: 3.202513  [ 6464/19873]\n",
      "loss: 3.096110  [12864/19873]\n",
      "loss: 2.906666  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.138198 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.321265  [   64/19873]\n",
      "loss: 3.162539  [ 6464/19873]\n",
      "loss: 3.049148  [12864/19873]\n",
      "loss: 2.831460  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.097648 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.309592  [   64/19873]\n",
      "loss: 3.132886  [ 6464/19873]\n",
      "loss: 3.021311  [12864/19873]\n",
      "loss: 2.783782  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.068686 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_fxn(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.float()\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.float()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.437797  [   64/19873]\n",
      "loss: 3.448026  [ 6464/19873]\n",
      "loss: 3.438915  [12864/19873]\n",
      "loss: 3.463523  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.9%, Avg loss: 3.419558 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.431638  [   64/19873]\n",
      "loss: 3.432540  [ 6464/19873]\n",
      "loss: 3.427800  [12864/19873]\n",
      "loss: 3.440774  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.9%, Avg loss: 3.415326 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.426932  [   64/19873]\n",
      "loss: 3.420790  [ 6464/19873]\n",
      "loss: 3.418674  [12864/19873]\n",
      "loss: 3.422359  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.8%, Avg loss: 3.412041 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.423030  [   64/19873]\n",
      "loss: 3.411697  [ 6464/19873]\n",
      "loss: 3.411285  [12864/19873]\n",
      "loss: 3.407199  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.0%, Avg loss: 3.409397 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.419549  [   64/19873]\n",
      "loss: 3.404591  [ 6464/19873]\n",
      "loss: 3.405027  [12864/19873]\n",
      "loss: 3.394940  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.2%, Avg loss: 3.407352 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.416288  [   64/19873]\n",
      "loss: 3.399162  [ 6464/19873]\n",
      "loss: 3.399616  [12864/19873]\n",
      "loss: 3.384650  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.8%, Avg loss: 3.405692 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.413256  [   64/19873]\n",
      "loss: 3.394891  [ 6464/19873]\n",
      "loss: 3.394902  [12864/19873]\n",
      "loss: 3.375922  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.5%, Avg loss: 3.404303 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.410249  [   64/19873]\n",
      "loss: 3.391494  [ 6464/19873]\n",
      "loss: 3.390655  [12864/19873]\n",
      "loss: 3.368613  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.3%, Avg loss: 3.403062 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.407370  [   64/19873]\n",
      "loss: 3.388666  [ 6464/19873]\n",
      "loss: 3.386904  [12864/19873]\n",
      "loss: 3.362378  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.5%, Avg loss: 3.401904 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.404592  [   64/19873]\n",
      "loss: 3.386439  [ 6464/19873]\n",
      "loss: 3.383587  [12864/19873]\n",
      "loss: 3.357158  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.5%, Avg loss: 3.400851 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.FloatTensor(weights)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Re-initialize the model\n",
    "model = NeuralNetwork()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# Train the model again with class weights\n",
    "for t in range(10):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing size: 4\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.458644  [   64/19873]\n",
      "loss: 3.527867  [ 6464/19873]\n",
      "loss: 3.445127  [12864/19873]\n",
      "loss: 3.598428  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.458466 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.457662  [   64/19873]\n",
      "loss: 3.525720  [ 6464/19873]\n",
      "loss: 3.444061  [12864/19873]\n",
      "loss: 3.595410  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.457672 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.456709  [   64/19873]\n",
      "loss: 3.523627  [ 6464/19873]\n",
      "loss: 3.443017  [12864/19873]\n",
      "loss: 3.592421  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.456897 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.455788  [   64/19873]\n",
      "loss: 3.521582  [ 6464/19873]\n",
      "loss: 3.441993  [12864/19873]\n",
      "loss: 3.589475  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.456139 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.454893  [   64/19873]\n",
      "loss: 3.519593  [ 6464/19873]\n",
      "loss: 3.440988  [12864/19873]\n",
      "loss: 3.586599  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.455402 \n",
      "\n",
      "Testing size: 8\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.486430  [   64/19873]\n",
      "loss: 3.490359  [ 6464/19873]\n",
      "loss: 3.469829  [12864/19873]\n",
      "loss: 3.418020  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.489648 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.485094  [   64/19873]\n",
      "loss: 3.488075  [ 6464/19873]\n",
      "loss: 3.468391  [12864/19873]\n",
      "loss: 3.416832  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.488316 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.483808  [   64/19873]\n",
      "loss: 3.485874  [ 6464/19873]\n",
      "loss: 3.466993  [12864/19873]\n",
      "loss: 3.415682  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.8%, Avg loss: 3.487029 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.482569  [   64/19873]\n",
      "loss: 3.483753  [ 6464/19873]\n",
      "loss: 3.465632  [12864/19873]\n",
      "loss: 3.414572  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 3.485787 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.481372  [   64/19873]\n",
      "loss: 3.481707  [ 6464/19873]\n",
      "loss: 3.464306  [12864/19873]\n",
      "loss: 3.413496  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.1%, Avg loss: 3.484586 \n",
      "\n",
      "Testing size: 16\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.431023  [   64/19873]\n",
      "loss: 3.440659  [ 6464/19873]\n",
      "loss: 3.424115  [12864/19873]\n",
      "loss: 3.464257  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.426891 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.430449  [   64/19873]\n",
      "loss: 3.438948  [ 6464/19873]\n",
      "loss: 3.423247  [12864/19873]\n",
      "loss: 3.461898  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.426523 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.429921  [   64/19873]\n",
      "loss: 3.437347  [ 6464/19873]\n",
      "loss: 3.422415  [12864/19873]\n",
      "loss: 3.459615  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.426193 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.429434  [   64/19873]\n",
      "loss: 3.435873  [ 6464/19873]\n",
      "loss: 3.421632  [12864/19873]\n",
      "loss: 3.457412  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.425909 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.428995  [   64/19873]\n",
      "loss: 3.434501  [ 6464/19873]\n",
      "loss: 3.420864  [12864/19873]\n",
      "loss: 3.455288  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.425675 \n",
      "\n",
      "Testing size: 32\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.426955  [   64/19873]\n",
      "loss: 3.471366  [ 6464/19873]\n",
      "loss: 3.446016  [12864/19873]\n",
      "loss: 3.450985  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.437333 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.425849  [   64/19873]\n",
      "loss: 3.468748  [ 6464/19873]\n",
      "loss: 3.444219  [12864/19873]\n",
      "loss: 3.449107  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.436750 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.424802  [   64/19873]\n",
      "loss: 3.466256  [ 6464/19873]\n",
      "loss: 3.442524  [12864/19873]\n",
      "loss: 3.447264  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.0%, Avg loss: 3.436212 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.423805  [   64/19873]\n",
      "loss: 3.463827  [ 6464/19873]\n",
      "loss: 3.440642  [12864/19873]\n",
      "loss: 3.445520  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.1%, Avg loss: 3.435974 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.422814  [   64/19873]\n",
      "loss: 3.461501  [ 6464/19873]\n",
      "loss: 3.438880  [12864/19873]\n",
      "loss: 3.444090  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.1%, Avg loss: 3.435854 \n",
      "\n",
      "Testing size: 64\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.422286  [   64/19873]\n",
      "loss: 3.457205  [ 6464/19873]\n",
      "loss: 3.457896  [12864/19873]\n",
      "loss: 3.430914  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.434205 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.422211  [   64/19873]\n",
      "loss: 3.454200  [ 6464/19873]\n",
      "loss: 3.455795  [12864/19873]\n",
      "loss: 3.426773  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.433093 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.422145  [   64/19873]\n",
      "loss: 3.451373  [ 6464/19873]\n",
      "loss: 3.453798  [12864/19873]\n",
      "loss: 3.422827  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.432056 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.422076  [   64/19873]\n",
      "loss: 3.448715  [ 6464/19873]\n",
      "loss: 3.451893  [12864/19873]\n",
      "loss: 3.419099  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.431074 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.422006  [   64/19873]\n",
      "loss: 3.446171  [ 6464/19873]\n",
      "loss: 3.450063  [12864/19873]\n",
      "loss: 3.415655  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.430148 \n",
      "\n",
      "Testing size: 128\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.437363  [   64/19873]\n",
      "loss: 3.441896  [ 6464/19873]\n",
      "loss: 3.432772  [12864/19873]\n",
      "loss: 3.443990  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.2%, Avg loss: 3.438141 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.435461  [   64/19873]\n",
      "loss: 3.438355  [ 6464/19873]\n",
      "loss: 3.430516  [12864/19873]\n",
      "loss: 3.438410  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.0%, Avg loss: 3.435908 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.433696  [   64/19873]\n",
      "loss: 3.435058  [ 6464/19873]\n",
      "loss: 3.428353  [12864/19873]\n",
      "loss: 3.433203  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.3%, Avg loss: 3.433842 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.432061  [   64/19873]\n",
      "loss: 3.432006  [ 6464/19873]\n",
      "loss: 3.426276  [12864/19873]\n",
      "loss: 3.428325  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.4%, Avg loss: 3.431924 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.430496  [   64/19873]\n",
      "loss: 3.429183  [ 6464/19873]\n",
      "loss: 3.424270  [12864/19873]\n",
      "loss: 3.423747  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 3.430127 \n",
      "\n",
      "Testing size: 256\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.417058  [   64/19873]\n",
      "loss: 3.406251  [ 6464/19873]\n",
      "loss: 3.394193  [12864/19873]\n",
      "loss: 3.428434  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.440098 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.415987  [   64/19873]\n",
      "loss: 3.403607  [ 6464/19873]\n",
      "loss: 3.392380  [12864/19873]\n",
      "loss: 3.420142  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.435916 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.414886  [   64/19873]\n",
      "loss: 3.401285  [ 6464/19873]\n",
      "loss: 3.390598  [12864/19873]\n",
      "loss: 3.412721  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.432255 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.413738  [   64/19873]\n",
      "loss: 3.399095  [ 6464/19873]\n",
      "loss: 3.388991  [12864/19873]\n",
      "loss: 3.405995  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.429125 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.412536  [   64/19873]\n",
      "loss: 3.397133  [ 6464/19873]\n",
      "loss: 3.387539  [12864/19873]\n",
      "loss: 3.399880  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.426405 \n",
      "\n",
      "Testing size: 512\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.448421  [   64/19873]\n",
      "loss: 3.421709  [ 6464/19873]\n",
      "loss: 3.434429  [12864/19873]\n",
      "loss: 3.417336  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.7%, Avg loss: 3.423141 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.439457  [   64/19873]\n",
      "loss: 3.413191  [ 6464/19873]\n",
      "loss: 3.425819  [12864/19873]\n",
      "loss: 3.400750  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.5%, Avg loss: 3.418144 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.432093  [   64/19873]\n",
      "loss: 3.406479  [ 6464/19873]\n",
      "loss: 3.418672  [12864/19873]\n",
      "loss: 3.387427  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.9%, Avg loss: 3.414239 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.425885  [   64/19873]\n",
      "loss: 3.401152  [ 6464/19873]\n",
      "loss: 3.412503  [12864/19873]\n",
      "loss: 3.376594  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.5%, Avg loss: 3.411079 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.420619  [   64/19873]\n",
      "loss: 3.396842  [ 6464/19873]\n",
      "loss: 3.407120  [12864/19873]\n",
      "loss: 3.367778  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.4%, Avg loss: 3.408439 \n",
      "\n",
      "Testing size: 1024\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.459437  [   64/19873]\n",
      "loss: 3.456033  [ 6464/19873]\n",
      "loss: 3.422045  [12864/19873]\n",
      "loss: 3.418701  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 3.420739 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.446440  [   64/19873]\n",
      "loss: 3.428461  [ 6464/19873]\n",
      "loss: 3.410248  [12864/19873]\n",
      "loss: 3.393915  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 3.413668 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.437287  [   64/19873]\n",
      "loss: 3.411197  [ 6464/19873]\n",
      "loss: 3.401626  [12864/19873]\n",
      "loss: 3.377142  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 3.408866 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.429521  [   64/19873]\n",
      "loss: 3.400044  [ 6464/19873]\n",
      "loss: 3.394894  [12864/19873]\n",
      "loss: 3.365176  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.5%, Avg loss: 3.405361 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.422414  [   64/19873]\n",
      "loss: 3.392777  [ 6464/19873]\n",
      "loss: 3.389592  [12864/19873]\n",
      "loss: 3.356821  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.2%, Avg loss: 3.402742 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=512):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "sizes_to_test = [4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "for size in sizes_to_test:\n",
    "    print(f\"Testing size: {size}\")\n",
    "    model = NeuralNetwork(size=size)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing learning rate: 0.1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.437389  [   64/19873]\n",
      "loss: 3.407884  [ 6464/19873]\n",
      "loss: 3.334379  [12864/19873]\n",
      "loss: 3.372323  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.9%, Avg loss: 3.352206 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.430663  [   64/19873]\n",
      "loss: 3.397813  [ 6464/19873]\n",
      "loss: 3.313618  [12864/19873]\n",
      "loss: 3.392911  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.366621 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.415348  [   64/19873]\n",
      "loss: 3.387073  [ 6464/19873]\n",
      "loss: 3.298946  [12864/19873]\n",
      "loss: 3.390493  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.370650 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.431707  [   64/19873]\n",
      "loss: 3.395008  [ 6464/19873]\n",
      "loss: 3.319283  [12864/19873]\n",
      "loss: 3.387866  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.356523 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.412568  [   64/19873]\n",
      "loss: 3.405586  [ 6464/19873]\n",
      "loss: 3.339729  [12864/19873]\n",
      "loss: 3.390145  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.354722 \n",
      "\n",
      "Testing learning rate: 0.01\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.453733  [   64/19873]\n",
      "loss: 3.420547  [ 6464/19873]\n",
      "loss: 3.433820  [12864/19873]\n",
      "loss: 3.391527  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.2%, Avg loss: 3.414000 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.423888  [   64/19873]\n",
      "loss: 3.391928  [ 6464/19873]\n",
      "loss: 3.401382  [12864/19873]\n",
      "loss: 3.363069  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.402785 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.405226  [   64/19873]\n",
      "loss: 3.384970  [ 6464/19873]\n",
      "loss: 3.387514  [12864/19873]\n",
      "loss: 3.351761  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.394144 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.389814  [   64/19873]\n",
      "loss: 3.384833  [ 6464/19873]\n",
      "loss: 3.379639  [12864/19873]\n",
      "loss: 3.350237  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.3%, Avg loss: 3.386932 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.377383  [   64/19873]\n",
      "loss: 3.387524  [ 6464/19873]\n",
      "loss: 3.375989  [12864/19873]\n",
      "loss: 3.351687  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.2%, Avg loss: 3.381312 \n",
      "\n",
      "Testing learning rate: 0.001\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.439699  [   64/19873]\n",
      "loss: 3.456103  [ 6464/19873]\n",
      "loss: 3.439999  [12864/19873]\n",
      "loss: 3.443509  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.3%, Avg loss: 3.448109 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.436802  [   64/19873]\n",
      "loss: 3.446575  [ 6464/19873]\n",
      "loss: 3.434359  [12864/19873]\n",
      "loss: 3.432244  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.3%, Avg loss: 3.443216 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.434350  [   64/19873]\n",
      "loss: 3.438743  [ 6464/19873]\n",
      "loss: 3.429384  [12864/19873]\n",
      "loss: 3.422526  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.2%, Avg loss: 3.439084 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.432273  [   64/19873]\n",
      "loss: 3.432093  [ 6464/19873]\n",
      "loss: 3.425160  [12864/19873]\n",
      "loss: 3.414010  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.4%, Avg loss: 3.435523 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.430195  [   64/19873]\n",
      "loss: 3.426266  [ 6464/19873]\n",
      "loss: 3.421412  [12864/19873]\n",
      "loss: 3.406418  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.3%, Avg loss: 3.432454 \n",
      "\n",
      "Testing learning rate: 0.0001\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.431407  [   64/19873]\n",
      "loss: 3.435841  [ 6464/19873]\n",
      "loss: 3.441599  [12864/19873]\n",
      "loss: 3.437104  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.448318 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.431141  [   64/19873]\n",
      "loss: 3.434953  [ 6464/19873]\n",
      "loss: 3.441060  [12864/19873]\n",
      "loss: 3.435732  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.447784 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.430880  [   64/19873]\n",
      "loss: 3.434083  [ 6464/19873]\n",
      "loss: 3.440526  [12864/19873]\n",
      "loss: 3.434383  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.447260 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.430619  [   64/19873]\n",
      "loss: 3.433227  [ 6464/19873]\n",
      "loss: 3.440004  [12864/19873]\n",
      "loss: 3.433058  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.446745 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.430359  [   64/19873]\n",
      "loss: 3.432391  [ 6464/19873]\n",
      "loss: 3.439491  [12864/19873]\n",
      "loss: 3.431756  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.446239 \n",
      "\n",
      "Testing learning rate: 1e-05\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.453931  [   64/19873]\n",
      "loss: 3.469440  [ 6464/19873]\n",
      "loss: 3.445604  [12864/19873]\n",
      "loss: 3.448429  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465378 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.453874  [   64/19873]\n",
      "loss: 3.469347  [ 6464/19873]\n",
      "loss: 3.445537  [12864/19873]\n",
      "loss: 3.448304  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465315 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.453818  [   64/19873]\n",
      "loss: 3.469254  [ 6464/19873]\n",
      "loss: 3.445469  [12864/19873]\n",
      "loss: 3.448179  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465253 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.453762  [   64/19873]\n",
      "loss: 3.469161  [ 6464/19873]\n",
      "loss: 3.445403  [12864/19873]\n",
      "loss: 3.448053  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465190 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.453706  [   64/19873]\n",
      "loss: 3.469069  [ 6464/19873]\n",
      "loss: 3.445335  [12864/19873]\n",
      "loss: 3.447929  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465128 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_to_test = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "for lr in lr_to_test:\n",
    "    print(f\"Testing learning rate: {lr}\")\n",
    "    model = NeuralNetwork(size=256)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.453650  [   64/19873]\n",
      "loss: 3.468778  [ 6464/19873]\n",
      "loss: 3.445128  [12864/19873]\n",
      "loss: 3.446953  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.464507 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.453095  [   64/19873]\n",
      "loss: 3.467870  [ 6464/19873]\n",
      "loss: 3.444466  [12864/19873]\n",
      "loss: 3.445726  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.463897 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.452548  [   64/19873]\n",
      "loss: 3.466980  [ 6464/19873]\n",
      "loss: 3.443809  [12864/19873]\n",
      "loss: 3.444516  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.463295 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.452004  [   64/19873]\n",
      "loss: 3.466105  [ 6464/19873]\n",
      "loss: 3.443157  [12864/19873]\n",
      "loss: 3.443324  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.462704 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.451467  [   64/19873]\n",
      "loss: 3.465248  [ 6464/19873]\n",
      "loss: 3.442511  [12864/19873]\n",
      "loss: 3.442147  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.462119 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=256):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "for t in range(5):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5264889766370516\n"
     ]
    }
   ],
   "source": [
    "# Use TF-IDF vectors as features\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(train['snip'])\n",
    "X_val_tfidf = tfidf.transform(val['snip'])\n",
    "\n",
    "# Train a model on these features\n",
    "model = LogisticRegression(C=1, max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "val_preds = model.predict(X_val_tfidf)\n",
    "accuracy = np.mean(val_preds == y_val)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Features Accuracy: 0.5330700888450148\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "x_train_cv = vectorizer.fit_transform(train['snip'])\n",
    "x_val_cv = vectorizer.transform(val['snip'])\n",
    "\n",
    "x_train = [X_train_tfidf, x_train_cv]\n",
    "x_val = [X_val_tfidf, x_val_cv]\n",
    "# Combine TF-IDF and CountVectorizer features\n",
    "from scipy.sparse import hstack\n",
    "x_train_combined = hstack(x_train)\n",
    "x_val_combined = hstack(x_val)\n",
    "# Train a model on the combined features\n",
    "model = LogisticRegression(C=1, max_iter=1000, class_weight='balanced')\n",
    "model.fit(x_train_combined, y_train)\n",
    "val_preds = model.predict(x_val_combined)\n",
    "accuracy = np.mean(val_preds == y_val)\n",
    "print(f\"Combined Features Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
