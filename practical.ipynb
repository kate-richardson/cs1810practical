{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONE\n",
    "Step 1: Preprocess the data - use the same function across the board\n",
    "Clearing out the capitalization\n",
    "Clearing out em dashes, symbols\n",
    "Clearing out names\n",
    "\n",
    "TO DO\n",
    "Step 2: Implementing the features\n",
    "- Goal: Train a logistic regression on 2 feature representations\n",
    "\n",
    "Step 3: Implementing the regression based on that data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Imports and Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kate/anaconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "val = pd.read_csv('data/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for cleaning text\n",
    "def clean_html(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # Remove HTML tags\n",
    "    clean = re.sub(r'<.*?>', '', str(text))\n",
    "    # Remove extra whitespaces\n",
    "    clean = re.sub(r'\\s+', ' ', clean).strip()\n",
    "    # Replace HTML entities\n",
    "    clean = re.sub(r'&amp;', '&', clean)\n",
    "    clean = re.sub(r'&lt;', '<', clean)\n",
    "    clean = re.sub(r'&gt;', '>', clean)\n",
    "    clean = re.sub(r'&quot;|&#34;', '\"', clean)\n",
    "    clean = re.sub(r'&apos;|&#39;', \"'\", clean)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    snip   channel\n",
      "0      first of all, it feels like covid again but in...  FOXNEWSW\n",
      "1      to be a software drivenrganization where softw...     CSPAN\n",
      "2      you discuss the power of ai to revolutionize t...    CSPAN2\n",
      "3      ai bots like chatgpt and google's bard gained ...   BBCNEWS\n",
      "4      . >> i could sleep ten hours ai night if i was...  FOXNEWSW\n",
      "...                                                  ...       ...\n",
      "19868  cardiovascular science, but they're also pione...  FOXNEWSW\n",
      "19869  i of ai in different fields. have of ai in dif...   BBCNEWS\n",
      "19870  weighing down on the major averages, both tech...      KTVU\n",
      "19871  i also think crypto ai that legislation be fro...    CSPAN2\n",
      "19872  as we have worked to monitor the adoption iden...    CSPAN2\n",
      "\n",
      "[19873 rows x 2 columns]\n",
      "                                                   snip    channel\n",
      "0     . ♪ >> there's a kyu cho right have things tha...  BLOOMBERG\n",
      "1     he says the ai tool helped create a new fronti...       KPIX\n",
      "2     . >> the all new godaddy arrow put your busine...       CNNW\n",
      "3     in some cases they are powered by generative a...      CSPAN\n",
      "4     this was a ivotal it comes to ai. this was a p...    BBCNEWS\n",
      "...                                                 ...        ...\n",
      "3034  however, the ai trade is only one part of the ...       CNBC\n",
      "3035  oz but also was highlighted as a product by cr...     CSPAN2\n",
      "3036  the all new godaddy airo helps you get your bu...       CNBC\n",
      "3037  we are going to be way ahead on ai. we have to...       CNBC\n",
      "3038  his fourth management role after spells at der...    BBCNEWS\n",
      "\n",
      "[3039 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# use the clean_html function to clean the training data\n",
    "train['snip'] = train['snip'].apply(clean_html)\n",
    "val['snip'] = val['snip'].apply(clean_html)\n",
    "\n",
    "print(train)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metric equation\n",
    "def eval(y_pred, y_true):\n",
    "    correct = (y_pred == y_true)   # Boolean array: True if correct, False if wrong\n",
    "    accuracy = correct.sum() / len(y_true)  # Correct / Total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    snip   channel  \\\n",
      "0      first of all, it feels like covid again but in...  FOXNEWSW   \n",
      "1      to be a software drivenrganization where softw...     CSPAN   \n",
      "2      you discuss the power of ai to revolutionize t...    CSPAN2   \n",
      "3      ai bots like chatgpt and google's bard gained ...   BBCNEWS   \n",
      "4      . >> i could sleep ten hours ai night if i was...  FOXNEWSW   \n",
      "...                                                  ...       ...   \n",
      "19868  cardiovascular science, but they're also pione...  FOXNEWSW   \n",
      "19869  i of ai in different fields. have of ai in dif...   BBCNEWS   \n",
      "19870  weighing down on the major averages, both tech...      KTVU   \n",
      "19871  i also think crypto ai that legislation be fro...    CSPAN2   \n",
      "19872  as we have worked to monitor the adoption iden...    CSPAN2   \n",
      "\n",
      "       word_complexity  \n",
      "0             0.057407  \n",
      "1             0.079768  \n",
      "2             0.076151  \n",
      "3             0.077301  \n",
      "4             0.075782  \n",
      "...                ...  \n",
      "19868         0.080295  \n",
      "19869         0.093252  \n",
      "19870         0.080397  \n",
      "19871         0.079097  \n",
      "19872         0.080723  \n",
      "\n",
      "[19873 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(train['snip'])\n",
    "\n",
    "# Calculate word complexity per snip\n",
    "word_complexity = X_tfidf.sum(axis=1) / (X_tfidf != 0).sum(axis=1)\n",
    "word_complexity = np.array(word_complexity).flatten()\n",
    "\n",
    "# Add it to the train dataframe\n",
    "train['word_complexity'] = word_complexity\n",
    "\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   snip    channel  \\\n",
      "0     . ♪ >> there's a kyu cho right have things tha...  BLOOMBERG   \n",
      "1     he says the ai tool helped create a new fronti...       KPIX   \n",
      "2     . >> the all new godaddy arrow put your busine...       CNNW   \n",
      "3     in some cases they are powered by generative a...      CSPAN   \n",
      "4     this was a ivotal it comes to ai. this was a p...    BBCNEWS   \n",
      "...                                                 ...        ...   \n",
      "3034  however, the ai trade is only one part of the ...       CNBC   \n",
      "3035  oz but also was highlighted as a product by cr...     CSPAN2   \n",
      "3036  the all new godaddy airo helps you get your bu...       CNBC   \n",
      "3037  we are going to be way ahead on ai. we have to...       CNBC   \n",
      "3038  his fourth management role after spells at der...    BBCNEWS   \n",
      "\n",
      "      word_complexity  \n",
      "0            0.088216  \n",
      "1            0.081808  \n",
      "2            0.074722  \n",
      "3            0.082919  \n",
      "4            0.083567  \n",
      "...               ...  \n",
      "3034         0.079930  \n",
      "3035         0.081606  \n",
      "3036         0.080997  \n",
      "3037         0.078856  \n",
      "3038         0.075032  \n",
      "\n",
      "[3039 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Transform validation snips using the same TF-IDF vectorizer\n",
    "x_val_tfidf = vectorizer.transform(val['snip'])\n",
    "\n",
    "# Compute complexity\n",
    "val_word_complexity = x_val_tfidf.sum(axis=1) / (x_val_tfidf != 0).sum(axis=1)\n",
    "val_word_complexity = np.array(val_word_complexity).flatten()\n",
    "\n",
    "# Add to val DataFrame\n",
    "val['word_complexity'] = val_word_complexity\n",
    "\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = features, y = labels\n",
    "x_train = train[['word_complexity']]  # Needs to be 2D\n",
    "y_train = train['channel']\n",
    "\n",
    "x_val = val[['word_complexity']]\n",
    "y_val = val['channel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNNW' 'CNNW' 'CNNW' ... 'CNNW' 'CNNW' 'CNNW']\n",
      "0.07535373478117802\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "val_preds = model.predict(x_val)\n",
    "print(val_preds)\n",
    "\n",
    "# Evaluate\n",
    "val_accuracy = eval(val_preds, y_val)\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now do the min/max training to create new average values of scores\n",
    "\n",
    "scaled = current - min/ max-min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0.057407\n",
      "1        0.079768\n",
      "2        0.076151\n",
      "3        0.077301\n",
      "4        0.075782\n",
      "           ...   \n",
      "19868    0.080295\n",
      "19869    0.093252\n",
      "19870    0.080397\n",
      "19871    0.079097\n",
      "19872    0.080723\n",
      "Name: word_complexity, Length: 19873, dtype: float64\n",
      "0.035929255415410526 0.6822632439493239\n"
     ]
    }
   ],
   "source": [
    "min = 1\n",
    "max = 0\n",
    "\n",
    "complexity = train['word_complexity']\n",
    "print(complexity)\n",
    "\n",
    "for i in range(len(complexity)):\n",
    "    if complexity[i] <= min:\n",
    "        min = complexity[i]\n",
    "    if complexity[i] >= max:\n",
    "        max = complexity[i]\n",
    "    else:\n",
    "        min = min\n",
    "        max = max\n",
    "\n",
    "print(min,max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complexity is 2D\n",
    "def scale(complexity):\n",
    "    x_train = (train[['word_complexity']] - min )/ (max-min)\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_complexity = train[\"word_complexity\"].min()\n",
    "max_complexity = train[\"word_complexity\"].max()\n",
    "train[\"word_complexity_scaled\"] = (train[\"word_complexity\"] - min_complexity) / (max_complexity - min_complexity)\n",
    "val[\"word_complexity_scaled\"] = (val[\"word_complexity\"] - min_complexity) / (max_complexity - min_complexity)\n",
    "\n",
    "# X = features, y = labels\n",
    "x_train = train[['word_complexity_scaled']]\n",
    "y_train = train['channel']\n",
    "\n",
    "x_val = val[['word_complexity_scaled']]\n",
    "y_val = val['channel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNNW' 'CNNW' 'CNNW' ... 'CNNW' 'CNNW' 'CNNW']\n",
      "0.07666995722277065\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "val_preds = model.predict(x_val)\n",
    "print(val_preds)\n",
    "\n",
    "# Evaluate\n",
    "val_accuracy = eval(val_preds, y_val)\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FOXNEWSW', 'CSPAN', 'CSPAN2', 'BBCNEWS', 'GBN', 'KPIX', 'KGO', 'KNTV', '1TV', 'KRON', 'CSPAN3', 'SFGTV', 'RUSSIA24', 'KSTS', 'BLOOMBERG', 'MSNBCW', 'PRESSTV', 'KTVU', 'CNNW', 'FBC', 'CNBC', 'RUSSIA1', 'KDTV', 'DW', 'KQED', 'NTV', 'BELARUSTV', 'ALJAZ', 'RT', 'LINKTV', 'COM']\n"
     ]
    }
   ],
   "source": [
    "# create list of all of the channels\n",
    "channels = []\n",
    "\n",
    "for i in range(len(train)):\n",
    "    if train['channel'][i] not in channels:\n",
    "        channels.append(train['channel'][i])\n",
    "    else:\n",
    "        channels = channels\n",
    "\n",
    "# \n",
    "\n",
    "print(channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "train_corpus = list(train['snip'].values)\n",
    "val_corpus = list(val['snip'].values)\n",
    "nlp_sentiment = pipeline(\"sentiment-analysis\")\n",
    "train[\"Sentiment\"] = nlp_sentiment(train_corpus)\n",
    "val[\"Sentiment\"] = nlp_sentiment(val_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snip</th>\n",
       "      <th>channel</th>\n",
       "      <th>word_complexity</th>\n",
       "      <th>word_complexity_scaled</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Label</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first of all, it feels like covid again but in...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.057407</td>\n",
       "      <td>0.033231</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9903525710105...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.990353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to be a software drivenrganization where softw...</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>0.079768</td>\n",
       "      <td>0.067826</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9933253526687...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.993325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you discuss the power of ai to revolutionize t...</td>\n",
       "      <td>CSPAN2</td>\n",
       "      <td>0.076151</td>\n",
       "      <td>0.062230</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9971946477890...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.997195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai bots like chatgpt and google's bard gained ...</td>\n",
       "      <td>BBCNEWS</td>\n",
       "      <td>0.077301</td>\n",
       "      <td>0.064009</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9900817275047...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.990082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>. &gt;&gt; i could sleep ten hours ai night if i was...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.075782</td>\n",
       "      <td>0.061660</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.8428422808647...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.842842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                snip   channel  \\\n",
       "0  first of all, it feels like covid again but in...  FOXNEWSW   \n",
       "1  to be a software drivenrganization where softw...     CSPAN   \n",
       "2  you discuss the power of ai to revolutionize t...    CSPAN2   \n",
       "3  ai bots like chatgpt and google's bard gained ...   BBCNEWS   \n",
       "4  . >> i could sleep ten hours ai night if i was...  FOXNEWSW   \n",
       "\n",
       "   word_complexity  word_complexity_scaled  \\\n",
       "0         0.057407                0.033231   \n",
       "1         0.079768                0.067826   \n",
       "2         0.076151                0.062230   \n",
       "3         0.077301                0.064009   \n",
       "4         0.075782                0.061660   \n",
       "\n",
       "                                           Sentiment Sentiment_Label  \\\n",
       "0  {'label': 'NEGATIVE', 'score': 0.9903525710105...        NEGATIVE   \n",
       "1  {'label': 'POSITIVE', 'score': 0.9933253526687...        POSITIVE   \n",
       "2  {'label': 'NEGATIVE', 'score': 0.9971946477890...        NEGATIVE   \n",
       "3  {'label': 'POSITIVE', 'score': 0.9900817275047...        POSITIVE   \n",
       "4  {'label': 'NEGATIVE', 'score': 0.8428422808647...        NEGATIVE   \n",
       "\n",
       "   Sentiment_Score  \n",
       "0         0.990353  \n",
       "1         0.993325  \n",
       "2         0.997195  \n",
       "3         0.990082  \n",
       "4         0.842842  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Sentiment_Label'] = [x.get('label') for x in train['Sentiment']]\n",
    "train['Sentiment_Score'] = [x.get('score') for x in train['Sentiment']]\n",
    "val['Sentiment_Label'] = [x.get('label') for x in val['Sentiment']]\n",
    "val['Sentiment_Score'] = [x.get('score') for x in val['Sentiment']]\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snip</th>\n",
       "      <th>channel</th>\n",
       "      <th>word_complexity</th>\n",
       "      <th>word_complexity_scaled</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Label</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first of all, it feels like covid again but in...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.057407</td>\n",
       "      <td>0.033231</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9903525710105...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.990353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to be a software drivenrganization where softw...</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>0.079768</td>\n",
       "      <td>0.067826</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9933253526687...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.993325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you discuss the power of ai to revolutionize t...</td>\n",
       "      <td>CSPAN2</td>\n",
       "      <td>0.076151</td>\n",
       "      <td>0.062230</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9971946477890...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.997195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai bots like chatgpt and google's bard gained ...</td>\n",
       "      <td>BBCNEWS</td>\n",
       "      <td>0.077301</td>\n",
       "      <td>0.064009</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9900817275047...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.990082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>. &gt;&gt; i could sleep ten hours ai night if i was...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.075782</td>\n",
       "      <td>0.061660</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.8428422808647...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.842842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                snip   channel  \\\n",
       "0  first of all, it feels like covid again but in...  FOXNEWSW   \n",
       "1  to be a software drivenrganization where softw...     CSPAN   \n",
       "2  you discuss the power of ai to revolutionize t...    CSPAN2   \n",
       "3  ai bots like chatgpt and google's bard gained ...   BBCNEWS   \n",
       "4  . >> i could sleep ten hours ai night if i was...  FOXNEWSW   \n",
       "\n",
       "   word_complexity  word_complexity_scaled  \\\n",
       "0         0.057407                0.033231   \n",
       "1         0.079768                0.067826   \n",
       "2         0.076151                0.062230   \n",
       "3         0.077301                0.064009   \n",
       "4         0.075782                0.061660   \n",
       "\n",
       "                                           Sentiment Sentiment_Label  \\\n",
       "0  {'label': 'NEGATIVE', 'score': 0.9903525710105...        NEGATIVE   \n",
       "1  {'label': 'POSITIVE', 'score': 0.9933253526687...        POSITIVE   \n",
       "2  {'label': 'NEGATIVE', 'score': 0.9971946477890...        NEGATIVE   \n",
       "3  {'label': 'POSITIVE', 'score': 0.9900817275047...        POSITIVE   \n",
       "4  {'label': 'NEGATIVE', 'score': 0.8428422808647...        NEGATIVE   \n",
       "\n",
       "   Sentiment_Score  \n",
       "0        -0.990353  \n",
       "1         0.993325  \n",
       "2        -0.997195  \n",
       "3         0.990082  \n",
       "4        -0.842842  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Sentiment_Score\"] = np.where(\n",
    "    train[\"Sentiment_Label\"] == \"NEGATIVE\", -(train[\"Sentiment_Score\"]), train[\"Sentiment_Score\"]\n",
    ")\n",
    "\n",
    "val[\"Sentiment_Score\"] = np.where(\n",
    "    val[\"Sentiment_Label\"] == \"NEGATIVE\", -(val[\"Sentiment_Score\"]), val[\"Sentiment_Score\"]\n",
    ")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snip</th>\n",
       "      <th>channel</th>\n",
       "      <th>word_complexity</th>\n",
       "      <th>word_complexity_scaled</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Label</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <th>Sentiment_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first of all, it feels like covid again but in...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.057407</td>\n",
       "      <td>0.033231</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9903525710105...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.990353</td>\n",
       "      <td>0.004713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to be a software drivenrganization where softw...</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>0.079768</td>\n",
       "      <td>0.067826</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9933253526687...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.993325</td>\n",
       "      <td>0.996727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you discuss the power of ai to revolutionize t...</td>\n",
       "      <td>CSPAN2</td>\n",
       "      <td>0.076151</td>\n",
       "      <td>0.062230</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9971946477890...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.997195</td>\n",
       "      <td>0.001292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai bots like chatgpt and google's bard gained ...</td>\n",
       "      <td>BBCNEWS</td>\n",
       "      <td>0.077301</td>\n",
       "      <td>0.064009</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9900817275047...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.990082</td>\n",
       "      <td>0.995105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>. &gt;&gt; i could sleep ten hours ai night if i was...</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>0.075782</td>\n",
       "      <td>0.061660</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.8428422808647...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.842842</td>\n",
       "      <td>0.078481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                snip   channel  \\\n",
       "0  first of all, it feels like covid again but in...  FOXNEWSW   \n",
       "1  to be a software drivenrganization where softw...     CSPAN   \n",
       "2  you discuss the power of ai to revolutionize t...    CSPAN2   \n",
       "3  ai bots like chatgpt and google's bard gained ...   BBCNEWS   \n",
       "4  . >> i could sleep ten hours ai night if i was...  FOXNEWSW   \n",
       "\n",
       "   word_complexity  word_complexity_scaled  \\\n",
       "0         0.057407                0.033231   \n",
       "1         0.079768                0.067826   \n",
       "2         0.076151                0.062230   \n",
       "3         0.077301                0.064009   \n",
       "4         0.075782                0.061660   \n",
       "\n",
       "                                           Sentiment Sentiment_Label  \\\n",
       "0  {'label': 'NEGATIVE', 'score': 0.9903525710105...        NEGATIVE   \n",
       "1  {'label': 'POSITIVE', 'score': 0.9933253526687...        POSITIVE   \n",
       "2  {'label': 'NEGATIVE', 'score': 0.9971946477890...        NEGATIVE   \n",
       "3  {'label': 'POSITIVE', 'score': 0.9900817275047...        POSITIVE   \n",
       "4  {'label': 'NEGATIVE', 'score': 0.8428422808647...        NEGATIVE   \n",
       "\n",
       "   Sentiment_Score  Sentiment_scaled  \n",
       "0        -0.990353          0.004713  \n",
       "1         0.993325          0.996727  \n",
       "2        -0.997195          0.001292  \n",
       "3         0.990082          0.995105  \n",
       "4        -0.842842          0.078481  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_sentiment = train[\"Sentiment_Score\"].min()\n",
    "max_sentiment = train[\"Sentiment_Score\"].max()\n",
    "\n",
    "train[\"Sentiment_scaled\"] = (train[\"Sentiment_Score\"] - min_sentiment) / (max_sentiment - min_sentiment)\n",
    "val[\"Sentiment_scaled\"] = (val[\"Sentiment_Score\"] - min_sentiment) / (max_sentiment - min_sentiment)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNNW' 'CNNW' 'CNNW' ... 'CNNW' 'CNNW' 'CNNW']\n",
      "0.07634090161237249\n"
     ]
    }
   ],
   "source": [
    "x_train = train[['word_complexity_scaled', 'Sentiment_scaled']]  # Needs to be 2D\n",
    "x_val = val[['word_complexity_scaled', 'Sentiment_scaled']]\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "val_preds = model.predict(x_val)\n",
    "print(val_preds)\n",
    "\n",
    "# Evaluate\n",
    "val_accuracy = eval(val_preds, y_val)\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "channel\n",
      "CNNW         2725\n",
      "FBC          1608\n",
      "BBCNEWS      1576\n",
      "BLOOMBERG    1441\n",
      "CNBC         1319\n",
      "MSNBCW       1141\n",
      "FOXNEWSW     1106\n",
      "CSPAN         913\n",
      "CSPAN2        904\n",
      "KNTV          842\n",
      "KTVU          766\n",
      "KGO           763\n",
      "KRON          760\n",
      "GBN           717\n",
      "CSPAN3        664\n",
      "KPIX          400\n",
      "SFGTV         360\n",
      "DW            253\n",
      "ALJAZ         230\n",
      "NTV           209\n",
      "KDTV          176\n",
      "1TV           158\n",
      "KSTS          142\n",
      "RUSSIA24      122\n",
      "PRESSTV       115\n",
      "KQED          113\n",
      "BELARUSTV     109\n",
      "RUSSIA1       108\n",
      "RT             50\n",
      "LINKTV         43\n",
      "COM            40\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(train[\"channel\"].value_counts()))\n",
    "print(train[\"channel\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1TV': 0, 'ALJAZ': 1, 'BBCNEWS': 2, 'BELARUSTV': 3, 'BLOOMBERG': 4, 'CNBC': 5, 'CNNW': 6, 'COM': 7, 'CSPAN': 8, 'CSPAN2': 9, 'CSPAN3': 10, 'DW': 11, 'FBC': 12, 'FOXNEWSW': 13, 'GBN': 14, 'KDTV': 15, 'KGO': 16, 'KNTV': 17, 'KPIX': 18, 'KQED': 19, 'KRON': 20, 'KSTS': 21, 'KTVU': 22, 'LINKTV': 23, 'MSNBCW': 24, 'NTV': 25, 'PRESSTV': 26, 'RT': 27, 'RUSSIA1': 28, 'RUSSIA24': 29, 'SFGTV': 30}\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train['channel'])\n",
    "training_labels = torch.tensor(encoder.transform(train['channel']))\n",
    "val_labels = torch.tensor(encoder.transform(val['channel']))\n",
    "\n",
    "class_mapping = dict(zip(encoder.classes_, range(len(encoder.classes_))))\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 2])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "training_data = torch.tensor(train[['word_complexity_scaled', 'Sentiment_scaled']].values)\n",
    "val_data = torch.tensor(val[['word_complexity_scaled', 'Sentiment_scaled']].values)\n",
    "train_tensor = torch.utils.data.TensorDataset(training_data, training_labels)\n",
    "val_tensor = torch.utils.data.TensorDataset(val_data, val_labels)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_tensor, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_tensor, batch_size=batch_size)\n",
    "\n",
    "for X, y in val_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=31, bias=True)\n",
      "  )\n",
      ")\n",
      "Predicted class: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "X = torch.rand(1, 2)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.419219  [   64/19873]\n",
      "loss: 3.365708  [ 6464/19873]\n",
      "loss: 3.298280  [12864/19873]\n",
      "loss: 3.198684  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.282800 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.359068  [   64/19873]\n",
      "loss: 3.266758  [ 6464/19873]\n",
      "loss: 3.176203  [12864/19873]\n",
      "loss: 3.025360  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.195135 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.332412  [   64/19873]\n",
      "loss: 3.202513  [ 6464/19873]\n",
      "loss: 3.096110  [12864/19873]\n",
      "loss: 2.906666  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.138198 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.321265  [   64/19873]\n",
      "loss: 3.162539  [ 6464/19873]\n",
      "loss: 3.049148  [12864/19873]\n",
      "loss: 2.831460  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.097648 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.309592  [   64/19873]\n",
      "loss: 3.132886  [ 6464/19873]\n",
      "loss: 3.021311  [12864/19873]\n",
      "loss: 2.783782  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.068686 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_fxn(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.float()\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.float()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.437797  [   64/19873]\n",
      "loss: 3.448026  [ 6464/19873]\n",
      "loss: 3.438915  [12864/19873]\n",
      "loss: 3.463523  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.9%, Avg loss: 3.419558 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.431638  [   64/19873]\n",
      "loss: 3.432540  [ 6464/19873]\n",
      "loss: 3.427800  [12864/19873]\n",
      "loss: 3.440774  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.9%, Avg loss: 3.415326 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.426932  [   64/19873]\n",
      "loss: 3.420790  [ 6464/19873]\n",
      "loss: 3.418674  [12864/19873]\n",
      "loss: 3.422359  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.8%, Avg loss: 3.412041 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.423030  [   64/19873]\n",
      "loss: 3.411697  [ 6464/19873]\n",
      "loss: 3.411285  [12864/19873]\n",
      "loss: 3.407199  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.0%, Avg loss: 3.409397 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.419549  [   64/19873]\n",
      "loss: 3.404591  [ 6464/19873]\n",
      "loss: 3.405027  [12864/19873]\n",
      "loss: 3.394940  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.2%, Avg loss: 3.407352 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.416288  [   64/19873]\n",
      "loss: 3.399162  [ 6464/19873]\n",
      "loss: 3.399616  [12864/19873]\n",
      "loss: 3.384650  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.8%, Avg loss: 3.405692 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.413256  [   64/19873]\n",
      "loss: 3.394891  [ 6464/19873]\n",
      "loss: 3.394902  [12864/19873]\n",
      "loss: 3.375922  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.5%, Avg loss: 3.404303 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.410249  [   64/19873]\n",
      "loss: 3.391494  [ 6464/19873]\n",
      "loss: 3.390655  [12864/19873]\n",
      "loss: 3.368613  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.3%, Avg loss: 3.403062 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.407370  [   64/19873]\n",
      "loss: 3.388666  [ 6464/19873]\n",
      "loss: 3.386904  [12864/19873]\n",
      "loss: 3.362378  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.5%, Avg loss: 3.401904 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.404592  [   64/19873]\n",
      "loss: 3.386439  [ 6464/19873]\n",
      "loss: 3.383587  [12864/19873]\n",
      "loss: 3.357158  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.5%, Avg loss: 3.400851 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.FloatTensor(weights)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Re-initialize the model\n",
    "model = NeuralNetwork()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# Train the model again with class weights\n",
    "for t in range(10):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing size: 4\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.458644  [   64/19873]\n",
      "loss: 3.527867  [ 6464/19873]\n",
      "loss: 3.445127  [12864/19873]\n",
      "loss: 3.598428  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.458466 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.457662  [   64/19873]\n",
      "loss: 3.525720  [ 6464/19873]\n",
      "loss: 3.444061  [12864/19873]\n",
      "loss: 3.595410  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.457672 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.456709  [   64/19873]\n",
      "loss: 3.523627  [ 6464/19873]\n",
      "loss: 3.443017  [12864/19873]\n",
      "loss: 3.592421  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.456897 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.455788  [   64/19873]\n",
      "loss: 3.521582  [ 6464/19873]\n",
      "loss: 3.441993  [12864/19873]\n",
      "loss: 3.589475  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.456139 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.454893  [   64/19873]\n",
      "loss: 3.519593  [ 6464/19873]\n",
      "loss: 3.440988  [12864/19873]\n",
      "loss: 3.586599  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.455402 \n",
      "\n",
      "Testing size: 8\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.486430  [   64/19873]\n",
      "loss: 3.490359  [ 6464/19873]\n",
      "loss: 3.469829  [12864/19873]\n",
      "loss: 3.418020  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.489648 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.485094  [   64/19873]\n",
      "loss: 3.488075  [ 6464/19873]\n",
      "loss: 3.468391  [12864/19873]\n",
      "loss: 3.416832  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.488316 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.483808  [   64/19873]\n",
      "loss: 3.485874  [ 6464/19873]\n",
      "loss: 3.466993  [12864/19873]\n",
      "loss: 3.415682  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.8%, Avg loss: 3.487029 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.482569  [   64/19873]\n",
      "loss: 3.483753  [ 6464/19873]\n",
      "loss: 3.465632  [12864/19873]\n",
      "loss: 3.414572  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 3.485787 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.481372  [   64/19873]\n",
      "loss: 3.481707  [ 6464/19873]\n",
      "loss: 3.464306  [12864/19873]\n",
      "loss: 3.413496  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.1%, Avg loss: 3.484586 \n",
      "\n",
      "Testing size: 16\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.431023  [   64/19873]\n",
      "loss: 3.440659  [ 6464/19873]\n",
      "loss: 3.424115  [12864/19873]\n",
      "loss: 3.464257  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.426891 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.430449  [   64/19873]\n",
      "loss: 3.438948  [ 6464/19873]\n",
      "loss: 3.423247  [12864/19873]\n",
      "loss: 3.461898  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.426523 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.429921  [   64/19873]\n",
      "loss: 3.437347  [ 6464/19873]\n",
      "loss: 3.422415  [12864/19873]\n",
      "loss: 3.459615  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.426193 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.429434  [   64/19873]\n",
      "loss: 3.435873  [ 6464/19873]\n",
      "loss: 3.421632  [12864/19873]\n",
      "loss: 3.457412  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.425909 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.428995  [   64/19873]\n",
      "loss: 3.434501  [ 6464/19873]\n",
      "loss: 3.420864  [12864/19873]\n",
      "loss: 3.455288  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.425675 \n",
      "\n",
      "Testing size: 32\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.426955  [   64/19873]\n",
      "loss: 3.471366  [ 6464/19873]\n",
      "loss: 3.446016  [12864/19873]\n",
      "loss: 3.450985  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.437333 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.425849  [   64/19873]\n",
      "loss: 3.468748  [ 6464/19873]\n",
      "loss: 3.444219  [12864/19873]\n",
      "loss: 3.449107  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.436750 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.424802  [   64/19873]\n",
      "loss: 3.466256  [ 6464/19873]\n",
      "loss: 3.442524  [12864/19873]\n",
      "loss: 3.447264  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.0%, Avg loss: 3.436212 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.423805  [   64/19873]\n",
      "loss: 3.463827  [ 6464/19873]\n",
      "loss: 3.440642  [12864/19873]\n",
      "loss: 3.445520  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.1%, Avg loss: 3.435974 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.422814  [   64/19873]\n",
      "loss: 3.461501  [ 6464/19873]\n",
      "loss: 3.438880  [12864/19873]\n",
      "loss: 3.444090  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.1%, Avg loss: 3.435854 \n",
      "\n",
      "Testing size: 64\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.422286  [   64/19873]\n",
      "loss: 3.457205  [ 6464/19873]\n",
      "loss: 3.457896  [12864/19873]\n",
      "loss: 3.430914  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.434205 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.422211  [   64/19873]\n",
      "loss: 3.454200  [ 6464/19873]\n",
      "loss: 3.455795  [12864/19873]\n",
      "loss: 3.426773  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.433093 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.422145  [   64/19873]\n",
      "loss: 3.451373  [ 6464/19873]\n",
      "loss: 3.453798  [12864/19873]\n",
      "loss: 3.422827  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.432056 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.422076  [   64/19873]\n",
      "loss: 3.448715  [ 6464/19873]\n",
      "loss: 3.451893  [12864/19873]\n",
      "loss: 3.419099  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.431074 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.422006  [   64/19873]\n",
      "loss: 3.446171  [ 6464/19873]\n",
      "loss: 3.450063  [12864/19873]\n",
      "loss: 3.415655  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.430148 \n",
      "\n",
      "Testing size: 128\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.437363  [   64/19873]\n",
      "loss: 3.441896  [ 6464/19873]\n",
      "loss: 3.432772  [12864/19873]\n",
      "loss: 3.443990  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.2%, Avg loss: 3.438141 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.435461  [   64/19873]\n",
      "loss: 3.438355  [ 6464/19873]\n",
      "loss: 3.430516  [12864/19873]\n",
      "loss: 3.438410  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.0%, Avg loss: 3.435908 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.433696  [   64/19873]\n",
      "loss: 3.435058  [ 6464/19873]\n",
      "loss: 3.428353  [12864/19873]\n",
      "loss: 3.433203  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.3%, Avg loss: 3.433842 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.432061  [   64/19873]\n",
      "loss: 3.432006  [ 6464/19873]\n",
      "loss: 3.426276  [12864/19873]\n",
      "loss: 3.428325  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.4%, Avg loss: 3.431924 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.430496  [   64/19873]\n",
      "loss: 3.429183  [ 6464/19873]\n",
      "loss: 3.424270  [12864/19873]\n",
      "loss: 3.423747  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 3.430127 \n",
      "\n",
      "Testing size: 256\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.417058  [   64/19873]\n",
      "loss: 3.406251  [ 6464/19873]\n",
      "loss: 3.394193  [12864/19873]\n",
      "loss: 3.428434  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.440098 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.415987  [   64/19873]\n",
      "loss: 3.403607  [ 6464/19873]\n",
      "loss: 3.392380  [12864/19873]\n",
      "loss: 3.420142  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.435916 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.414886  [   64/19873]\n",
      "loss: 3.401285  [ 6464/19873]\n",
      "loss: 3.390598  [12864/19873]\n",
      "loss: 3.412721  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.432255 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.413738  [   64/19873]\n",
      "loss: 3.399095  [ 6464/19873]\n",
      "loss: 3.388991  [12864/19873]\n",
      "loss: 3.405995  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.429125 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.412536  [   64/19873]\n",
      "loss: 3.397133  [ 6464/19873]\n",
      "loss: 3.387539  [12864/19873]\n",
      "loss: 3.399880  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.426405 \n",
      "\n",
      "Testing size: 512\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.448421  [   64/19873]\n",
      "loss: 3.421709  [ 6464/19873]\n",
      "loss: 3.434429  [12864/19873]\n",
      "loss: 3.417336  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.7%, Avg loss: 3.423141 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.439457  [   64/19873]\n",
      "loss: 3.413191  [ 6464/19873]\n",
      "loss: 3.425819  [12864/19873]\n",
      "loss: 3.400750  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.5%, Avg loss: 3.418144 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.432093  [   64/19873]\n",
      "loss: 3.406479  [ 6464/19873]\n",
      "loss: 3.418672  [12864/19873]\n",
      "loss: 3.387427  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.9%, Avg loss: 3.414239 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.425885  [   64/19873]\n",
      "loss: 3.401152  [ 6464/19873]\n",
      "loss: 3.412503  [12864/19873]\n",
      "loss: 3.376594  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.5%, Avg loss: 3.411079 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.420619  [   64/19873]\n",
      "loss: 3.396842  [ 6464/19873]\n",
      "loss: 3.407120  [12864/19873]\n",
      "loss: 3.367778  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.4%, Avg loss: 3.408439 \n",
      "\n",
      "Testing size: 1024\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.459437  [   64/19873]\n",
      "loss: 3.456033  [ 6464/19873]\n",
      "loss: 3.422045  [12864/19873]\n",
      "loss: 3.418701  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 3.420739 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.446440  [   64/19873]\n",
      "loss: 3.428461  [ 6464/19873]\n",
      "loss: 3.410248  [12864/19873]\n",
      "loss: 3.393915  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 3.413668 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.437287  [   64/19873]\n",
      "loss: 3.411197  [ 6464/19873]\n",
      "loss: 3.401626  [12864/19873]\n",
      "loss: 3.377142  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 3.408866 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.429521  [   64/19873]\n",
      "loss: 3.400044  [ 6464/19873]\n",
      "loss: 3.394894  [12864/19873]\n",
      "loss: 3.365176  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.5%, Avg loss: 3.405361 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.422414  [   64/19873]\n",
      "loss: 3.392777  [ 6464/19873]\n",
      "loss: 3.389592  [12864/19873]\n",
      "loss: 3.356821  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.2%, Avg loss: 3.402742 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=512):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "sizes_to_test = [4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "for size in sizes_to_test:\n",
    "    print(f\"Testing size: {size}\")\n",
    "    model = NeuralNetwork(size=size)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing learning rate: 0.1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.437389  [   64/19873]\n",
      "loss: 3.407884  [ 6464/19873]\n",
      "loss: 3.334379  [12864/19873]\n",
      "loss: 3.372323  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.9%, Avg loss: 3.352206 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.430663  [   64/19873]\n",
      "loss: 3.397813  [ 6464/19873]\n",
      "loss: 3.313618  [12864/19873]\n",
      "loss: 3.392911  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.366621 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.415348  [   64/19873]\n",
      "loss: 3.387073  [ 6464/19873]\n",
      "loss: 3.298946  [12864/19873]\n",
      "loss: 3.390493  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.370650 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.431707  [   64/19873]\n",
      "loss: 3.395008  [ 6464/19873]\n",
      "loss: 3.319283  [12864/19873]\n",
      "loss: 3.387866  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.356523 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.412568  [   64/19873]\n",
      "loss: 3.405586  [ 6464/19873]\n",
      "loss: 3.339729  [12864/19873]\n",
      "loss: 3.390145  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.354722 \n",
      "\n",
      "Testing learning rate: 0.01\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.453733  [   64/19873]\n",
      "loss: 3.420547  [ 6464/19873]\n",
      "loss: 3.433820  [12864/19873]\n",
      "loss: 3.391527  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.2%, Avg loss: 3.414000 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.423888  [   64/19873]\n",
      "loss: 3.391928  [ 6464/19873]\n",
      "loss: 3.401382  [12864/19873]\n",
      "loss: 3.363069  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.402785 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.405226  [   64/19873]\n",
      "loss: 3.384970  [ 6464/19873]\n",
      "loss: 3.387514  [12864/19873]\n",
      "loss: 3.351761  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.394144 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.389814  [   64/19873]\n",
      "loss: 3.384833  [ 6464/19873]\n",
      "loss: 3.379639  [12864/19873]\n",
      "loss: 3.350237  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.3%, Avg loss: 3.386932 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.377383  [   64/19873]\n",
      "loss: 3.387524  [ 6464/19873]\n",
      "loss: 3.375989  [12864/19873]\n",
      "loss: 3.351687  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.2%, Avg loss: 3.381312 \n",
      "\n",
      "Testing learning rate: 0.001\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.439699  [   64/19873]\n",
      "loss: 3.456103  [ 6464/19873]\n",
      "loss: 3.439999  [12864/19873]\n",
      "loss: 3.443509  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.3%, Avg loss: 3.448109 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.436802  [   64/19873]\n",
      "loss: 3.446575  [ 6464/19873]\n",
      "loss: 3.434359  [12864/19873]\n",
      "loss: 3.432244  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.3%, Avg loss: 3.443216 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.434350  [   64/19873]\n",
      "loss: 3.438743  [ 6464/19873]\n",
      "loss: 3.429384  [12864/19873]\n",
      "loss: 3.422526  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.2%, Avg loss: 3.439084 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.432273  [   64/19873]\n",
      "loss: 3.432093  [ 6464/19873]\n",
      "loss: 3.425160  [12864/19873]\n",
      "loss: 3.414010  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.4%, Avg loss: 3.435523 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.430195  [   64/19873]\n",
      "loss: 3.426266  [ 6464/19873]\n",
      "loss: 3.421412  [12864/19873]\n",
      "loss: 3.406418  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.3%, Avg loss: 3.432454 \n",
      "\n",
      "Testing learning rate: 0.0001\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.431407  [   64/19873]\n",
      "loss: 3.435841  [ 6464/19873]\n",
      "loss: 3.441599  [12864/19873]\n",
      "loss: 3.437104  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.448318 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.431141  [   64/19873]\n",
      "loss: 3.434953  [ 6464/19873]\n",
      "loss: 3.441060  [12864/19873]\n",
      "loss: 3.435732  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.447784 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.430880  [   64/19873]\n",
      "loss: 3.434083  [ 6464/19873]\n",
      "loss: 3.440526  [12864/19873]\n",
      "loss: 3.434383  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.447260 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.430619  [   64/19873]\n",
      "loss: 3.433227  [ 6464/19873]\n",
      "loss: 3.440004  [12864/19873]\n",
      "loss: 3.433058  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.446745 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.430359  [   64/19873]\n",
      "loss: 3.432391  [ 6464/19873]\n",
      "loss: 3.439491  [12864/19873]\n",
      "loss: 3.431756  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.446239 \n",
      "\n",
      "Testing learning rate: 1e-05\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.453931  [   64/19873]\n",
      "loss: 3.469440  [ 6464/19873]\n",
      "loss: 3.445604  [12864/19873]\n",
      "loss: 3.448429  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465378 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.453874  [   64/19873]\n",
      "loss: 3.469347  [ 6464/19873]\n",
      "loss: 3.445537  [12864/19873]\n",
      "loss: 3.448304  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465315 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.453818  [   64/19873]\n",
      "loss: 3.469254  [ 6464/19873]\n",
      "loss: 3.445469  [12864/19873]\n",
      "loss: 3.448179  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465253 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.453762  [   64/19873]\n",
      "loss: 3.469161  [ 6464/19873]\n",
      "loss: 3.445403  [12864/19873]\n",
      "loss: 3.448053  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465190 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.453706  [   64/19873]\n",
      "loss: 3.469069  [ 6464/19873]\n",
      "loss: 3.445335  [12864/19873]\n",
      "loss: 3.447929  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.465128 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_to_test = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "for lr in lr_to_test:\n",
    "    print(f\"Testing learning rate: {lr}\")\n",
    "    model = NeuralNetwork(size=256)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.453650  [   64/19873]\n",
      "loss: 3.468778  [ 6464/19873]\n",
      "loss: 3.445128  [12864/19873]\n",
      "loss: 3.446953  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.464507 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.453095  [   64/19873]\n",
      "loss: 3.467870  [ 6464/19873]\n",
      "loss: 3.444466  [12864/19873]\n",
      "loss: 3.445726  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.463897 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.452548  [   64/19873]\n",
      "loss: 3.466980  [ 6464/19873]\n",
      "loss: 3.443809  [12864/19873]\n",
      "loss: 3.444516  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.463295 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.452004  [   64/19873]\n",
      "loss: 3.466105  [ 6464/19873]\n",
      "loss: 3.443157  [12864/19873]\n",
      "loss: 3.443324  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.462704 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.451467  [   64/19873]\n",
      "loss: 3.465248  [ 6464/19873]\n",
      "loss: 3.442511  [12864/19873]\n",
      "loss: 3.442147  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.4%, Avg loss: 3.462119 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=256):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "for t in range(5):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5264889766370516\n"
     ]
    }
   ],
   "source": [
    "# Use TF-IDF vectors as features\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(train['snip'])\n",
    "X_val_tfidf = tfidf.transform(val['snip'])\n",
    "\n",
    "# Train a model on these features\n",
    "model = LogisticRegression(C=1, max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "val_preds = model.predict(X_val_tfidf)\n",
    "accuracy = np.mean(val_preds == y_val)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Features Accuracy: 0.5330700888450148\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "x_train_cv = vectorizer.fit_transform(train['snip'])\n",
    "x_val_cv = vectorizer.transform(val['snip'])\n",
    "\n",
    "x_train = [X_train_tfidf, x_train_cv]\n",
    "x_val = [X_val_tfidf, x_val_cv]\n",
    "# Combine TF-IDF and CountVectorizer features\n",
    "from scipy.sparse import hstack\n",
    "x_train_combined = hstack(x_train)\n",
    "x_val_combined = hstack(x_val)\n",
    "# Train a model on the combined features\n",
    "model = LogisticRegression(C=1, max_iter=1000, class_weight='balanced')\n",
    "model.fit(x_train_combined, y_train)\n",
    "val_preds = model.predict(x_val_combined)\n",
    "accuracy = np.mean(val_preds == y_val)\n",
    "print(f\"Combined Features Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 71636])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "training_data = torch.tensor(x_train_combined.toarray(), dtype=torch.float32)\n",
    "val_data = torch.tensor(x_val_combined.toarray(), dtype=torch.float32)\n",
    "train_tensor = torch.utils.data.TensorDataset(training_data, training_labels)\n",
    "val_tensor = torch.utils.data.TensorDataset(val_data, val_labels)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_tensor, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_tensor, batch_size=batch_size)\n",
    "\n",
    "for X, y in val_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=71636, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=31, bias=True)\n",
      "  )\n",
      ")\n",
      "Predicted class: tensor([7])\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(71636, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "X = torch.rand(1, 71636)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.434559  [   64/19873]\n",
      "loss: 3.428117  [ 6464/19873]\n",
      "loss: 3.405618  [12864/19873]\n",
      "loss: 3.373003  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 3.389523 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.408284  [   64/19873]\n",
      "loss: 3.376514  [ 6464/19873]\n",
      "loss: 3.322723  [12864/19873]\n",
      "loss: 3.241123  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.308714 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.367684  [   64/19873]\n",
      "loss: 3.293944  [ 6464/19873]\n",
      "loss: 3.182109  [12864/19873]\n",
      "loss: 3.027426  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.195545 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.328003  [   64/19873]\n",
      "loss: 3.206158  [ 6464/19873]\n",
      "loss: 3.045216  [12864/19873]\n",
      "loss: 2.853983  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.114789 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.313701  [   64/19873]\n",
      "loss: 3.155625  [ 6464/19873]\n",
      "loss: 2.972900  [12864/19873]\n",
      "loss: 2.765722  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 3.059111 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_fxn(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.float()\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.float()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.430328  [   64/19873]\n",
      "loss: 3.428810  [ 6464/19873]\n",
      "loss: 3.422170  [12864/19873]\n",
      "loss: 3.432064  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.5%, Avg loss: 3.428553 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.418210  [   64/19873]\n",
      "loss: 3.420495  [ 6464/19873]\n",
      "loss: 3.403784  [12864/19873]\n",
      "loss: 3.422399  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.2%, Avg loss: 3.419557 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.405757  [   64/19873]\n",
      "loss: 3.411347  [ 6464/19873]\n",
      "loss: 3.384280  [12864/19873]\n",
      "loss: 3.411191  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.1%, Avg loss: 3.409535 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.391715  [   64/19873]\n",
      "loss: 3.401117  [ 6464/19873]\n",
      "loss: 3.361994  [12864/19873]\n",
      "loss: 3.398981  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.9%, Avg loss: 3.398097 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.375106  [   64/19873]\n",
      "loss: 3.389582  [ 6464/19873]\n",
      "loss: 3.334901  [12864/19873]\n",
      "loss: 3.386453  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.3%, Avg loss: 3.385687 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.355259  [   64/19873]\n",
      "loss: 3.377243  [ 6464/19873]\n",
      "loss: 3.302362  [12864/19873]\n",
      "loss: 3.372976  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.5%, Avg loss: 3.371131 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.331575  [   64/19873]\n",
      "loss: 3.363435  [ 6464/19873]\n",
      "loss: 3.262287  [12864/19873]\n",
      "loss: 3.358761  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg loss: 3.353942 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.303061  [   64/19873]\n",
      "loss: 3.348527  [ 6464/19873]\n",
      "loss: 3.212983  [12864/19873]\n",
      "loss: 3.344238  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.1%, Avg loss: 3.334226 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.268876  [   64/19873]\n",
      "loss: 3.333044  [ 6464/19873]\n",
      "loss: 3.151452  [12864/19873]\n",
      "loss: 3.330046  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.0%, Avg loss: 3.311452 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.227689  [   64/19873]\n",
      "loss: 3.317555  [ 6464/19873]\n",
      "loss: 3.074322  [12864/19873]\n",
      "loss: 3.316724  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.6%, Avg loss: 3.284940 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.FloatTensor(weights)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Re-initialize the model\n",
    "model = NeuralNetwork()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# Train the model again with class weights\n",
    "for t in range(10):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing size: 4\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.402915  [   64/19873]\n",
      "loss: 3.432564  [ 6464/19873]\n",
      "loss: 3.543827  [12864/19873]\n",
      "loss: 3.447044  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.451771 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.396417  [   64/19873]\n",
      "loss: 3.428012  [ 6464/19873]\n",
      "loss: 3.540450  [12864/19873]\n",
      "loss: 3.440862  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.451913 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.394105  [   64/19873]\n",
      "loss: 3.425907  [ 6464/19873]\n",
      "loss: 3.539143  [12864/19873]\n",
      "loss: 3.435375  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.453221 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.395595  [   64/19873]\n",
      "loss: 3.423532  [ 6464/19873]\n",
      "loss: 3.537412  [12864/19873]\n",
      "loss: 3.431342  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.453774 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.397021  [   64/19873]\n",
      "loss: 3.421161  [ 6464/19873]\n",
      "loss: 3.534748  [12864/19873]\n",
      "loss: 3.427881  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.453381 \n",
      "\n",
      "Testing size: 8\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.479362  [   64/19873]\n",
      "loss: 3.395927  [ 6464/19873]\n",
      "loss: 3.523820  [12864/19873]\n",
      "loss: 3.481232  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.2%, Avg loss: 3.446070 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.479168  [   64/19873]\n",
      "loss: 3.391809  [ 6464/19873]\n",
      "loss: 3.520635  [12864/19873]\n",
      "loss: 3.477242  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.2%, Avg loss: 3.443989 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.475273  [   64/19873]\n",
      "loss: 3.386813  [ 6464/19873]\n",
      "loss: 3.513307  [12864/19873]\n",
      "loss: 3.473772  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.2%, Avg loss: 3.442015 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.469245  [   64/19873]\n",
      "loss: 3.381234  [ 6464/19873]\n",
      "loss: 3.507710  [12864/19873]\n",
      "loss: 3.471232  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.2%, Avg loss: 3.439584 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.464604  [   64/19873]\n",
      "loss: 3.375283  [ 6464/19873]\n",
      "loss: 3.503025  [12864/19873]\n",
      "loss: 3.468647  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.2%, Avg loss: 3.437233 \n",
      "\n",
      "Testing size: 16\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.463140  [   64/19873]\n",
      "loss: 3.396588  [ 6464/19873]\n",
      "loss: 3.449048  [12864/19873]\n",
      "loss: 3.440449  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.431759 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.452724  [   64/19873]\n",
      "loss: 3.394173  [ 6464/19873]\n",
      "loss: 3.440188  [12864/19873]\n",
      "loss: 3.435639  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.428462 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.446311  [   64/19873]\n",
      "loss: 3.391269  [ 6464/19873]\n",
      "loss: 3.431220  [12864/19873]\n",
      "loss: 3.431103  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.425416 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.440876  [   64/19873]\n",
      "loss: 3.384218  [ 6464/19873]\n",
      "loss: 3.422391  [12864/19873]\n",
      "loss: 3.427737  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.5%, Avg loss: 3.424179 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.435932  [   64/19873]\n",
      "loss: 3.375902  [ 6464/19873]\n",
      "loss: 3.411589  [12864/19873]\n",
      "loss: 3.424763  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.9%, Avg loss: 3.420668 \n",
      "\n",
      "Testing size: 32\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.417110  [   64/19873]\n",
      "loss: 3.427480  [ 6464/19873]\n",
      "loss: 3.369092  [12864/19873]\n",
      "loss: 3.389271  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.8%, Avg loss: 3.419802 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.404366  [   64/19873]\n",
      "loss: 3.414600  [ 6464/19873]\n",
      "loss: 3.358005  [12864/19873]\n",
      "loss: 3.383717  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.9%, Avg loss: 3.411963 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.396748  [   64/19873]\n",
      "loss: 3.406552  [ 6464/19873]\n",
      "loss: 3.347377  [12864/19873]\n",
      "loss: 3.380816  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.8%, Avg loss: 3.405378 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.389344  [   64/19873]\n",
      "loss: 3.400475  [ 6464/19873]\n",
      "loss: 3.332312  [12864/19873]\n",
      "loss: 3.377691  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.398625 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.380871  [   64/19873]\n",
      "loss: 3.395662  [ 6464/19873]\n",
      "loss: 3.315721  [12864/19873]\n",
      "loss: 3.374271  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.6%, Avg loss: 3.390683 \n",
      "\n",
      "Testing size: 64\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.418517  [   64/19873]\n",
      "loss: 3.415454  [ 6464/19873]\n",
      "loss: 3.432164  [12864/19873]\n",
      "loss: 3.451925  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.8%, Avg loss: 3.426432 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.406725  [   64/19873]\n",
      "loss: 3.405242  [ 6464/19873]\n",
      "loss: 3.414254  [12864/19873]\n",
      "loss: 3.438085  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.5%, Avg loss: 3.418209 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.397415  [   64/19873]\n",
      "loss: 3.396772  [ 6464/19873]\n",
      "loss: 3.397720  [12864/19873]\n",
      "loss: 3.425625  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.7%, Avg loss: 3.410474 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.388530  [   64/19873]\n",
      "loss: 3.388748  [ 6464/19873]\n",
      "loss: 3.382231  [12864/19873]\n",
      "loss: 3.414591  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.9%, Avg loss: 3.403064 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.379233  [   64/19873]\n",
      "loss: 3.380889  [ 6464/19873]\n",
      "loss: 3.364755  [12864/19873]\n",
      "loss: 3.404135  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.5%, Avg loss: 3.395028 \n",
      "\n",
      "Testing size: 128\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.445489  [   64/19873]\n",
      "loss: 3.439589  [ 6464/19873]\n",
      "loss: 3.412845  [12864/19873]\n",
      "loss: 3.423648  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.3%, Avg loss: 3.429330 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.433228  [   64/19873]\n",
      "loss: 3.437489  [ 6464/19873]\n",
      "loss: 3.400120  [12864/19873]\n",
      "loss: 3.419584  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.0%, Avg loss: 3.421475 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.421005  [   64/19873]\n",
      "loss: 3.434302  [ 6464/19873]\n",
      "loss: 3.388674  [12864/19873]\n",
      "loss: 3.413754  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 3.412535 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.406834  [   64/19873]\n",
      "loss: 3.429973  [ 6464/19873]\n",
      "loss: 3.372707  [12864/19873]\n",
      "loss: 3.406639  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.5%, Avg loss: 3.402120 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.388471  [   64/19873]\n",
      "loss: 3.422845  [ 6464/19873]\n",
      "loss: 3.351058  [12864/19873]\n",
      "loss: 3.397968  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 3.389856 \n",
      "\n",
      "Testing size: 256\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.442314  [   64/19873]\n",
      "loss: 3.412101  [ 6464/19873]\n",
      "loss: 3.414319  [12864/19873]\n",
      "loss: 3.423735  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.9%, Avg loss: 3.423108 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.427003  [   64/19873]\n",
      "loss: 3.404778  [ 6464/19873]\n",
      "loss: 3.397000  [12864/19873]\n",
      "loss: 3.417403  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.9%, Avg loss: 3.415500 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.413430  [   64/19873]\n",
      "loss: 3.397776  [ 6464/19873]\n",
      "loss: 3.377228  [12864/19873]\n",
      "loss: 3.410630  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.5%, Avg loss: 3.406676 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.398846  [   64/19873]\n",
      "loss: 3.389768  [ 6464/19873]\n",
      "loss: 3.353375  [12864/19873]\n",
      "loss: 3.401215  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 3.396121 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.380884  [   64/19873]\n",
      "loss: 3.380497  [ 6464/19873]\n",
      "loss: 3.322924  [12864/19873]\n",
      "loss: 3.390501  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.5%, Avg loss: 3.384108 \n",
      "\n",
      "Testing size: 512\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.434092  [   64/19873]\n",
      "loss: 3.426777  [ 6464/19873]\n",
      "loss: 3.419371  [12864/19873]\n",
      "loss: 3.423434  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.4%, Avg loss: 3.424094 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.420086  [   64/19873]\n",
      "loss: 3.418455  [ 6464/19873]\n",
      "loss: 3.400634  [12864/19873]\n",
      "loss: 3.416853  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.0%, Avg loss: 3.413567 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.405901  [   64/19873]\n",
      "loss: 3.409181  [ 6464/19873]\n",
      "loss: 3.379846  [12864/19873]\n",
      "loss: 3.410385  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.2%, Avg loss: 3.402301 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.390208  [   64/19873]\n",
      "loss: 3.399818  [ 6464/19873]\n",
      "loss: 3.355627  [12864/19873]\n",
      "loss: 3.403380  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.3%, Avg loss: 3.389798 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.371686  [   64/19873]\n",
      "loss: 3.389231  [ 6464/19873]\n",
      "loss: 3.326609  [12864/19873]\n",
      "loss: 3.395491  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.3%, Avg loss: 3.375581 \n",
      "\n",
      "Testing size: 1024\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.427153  [   64/19873]\n",
      "loss: 3.430657  [ 6464/19873]\n",
      "loss: 3.423244  [12864/19873]\n",
      "loss: 3.430078  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.428236 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.414685  [   64/19873]\n",
      "loss: 3.422326  [ 6464/19873]\n",
      "loss: 3.405539  [12864/19873]\n",
      "loss: 3.421941  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.5%, Avg loss: 3.421018 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.401618  [   64/19873]\n",
      "loss: 3.413295  [ 6464/19873]\n",
      "loss: 3.386519  [12864/19873]\n",
      "loss: 3.414127  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.4%, Avg loss: 3.413140 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.387065  [   64/19873]\n",
      "loss: 3.403687  [ 6464/19873]\n",
      "loss: 3.364489  [12864/19873]\n",
      "loss: 3.405830  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.4%, Avg loss: 3.403964 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.370663  [   64/19873]\n",
      "loss: 3.393227  [ 6464/19873]\n",
      "loss: 3.337797  [12864/19873]\n",
      "loss: 3.396659  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.4%, Avg loss: 3.392992 \n",
      "\n",
      "Testing size: 2048\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.431548  [   64/19873]\n",
      "loss: 3.431872  [ 6464/19873]\n",
      "loss: 3.424892  [12864/19873]\n",
      "loss: 3.428817  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 3.422011 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.420356  [   64/19873]\n",
      "loss: 3.423877  [ 6464/19873]\n",
      "loss: 3.407967  [12864/19873]\n",
      "loss: 3.421660  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.7%, Avg loss: 3.413660 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.408260  [   64/19873]\n",
      "loss: 3.415294  [ 6464/19873]\n",
      "loss: 3.388644  [12864/19873]\n",
      "loss: 3.413869  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 3.404573 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.394279  [   64/19873]\n",
      "loss: 3.405428  [ 6464/19873]\n",
      "loss: 3.365691  [12864/19873]\n",
      "loss: 3.404993  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg loss: 3.394303 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.378091  [   64/19873]\n",
      "loss: 3.393972  [ 6464/19873]\n",
      "loss: 3.338054  [12864/19873]\n",
      "loss: 3.394866  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.6%, Avg loss: 3.382417 \n",
      "\n",
      "Testing size: 4096\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.431428  [   64/19873]\n",
      "loss: 3.434120  [ 6464/19873]\n",
      "loss: 3.422175  [12864/19873]\n",
      "loss: 3.427072  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.5%, Avg loss: 3.426499 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.419234  [   64/19873]\n",
      "loss: 3.425740  [ 6464/19873]\n",
      "loss: 3.403899  [12864/19873]\n",
      "loss: 3.419055  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg loss: 3.418358 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.406377  [   64/19873]\n",
      "loss: 3.417174  [ 6464/19873]\n",
      "loss: 3.383790  [12864/19873]\n",
      "loss: 3.410534  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.4%, Avg loss: 3.409494 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.391922  [   64/19873]\n",
      "loss: 3.407772  [ 6464/19873]\n",
      "loss: 3.360447  [12864/19873]\n",
      "loss: 3.401273  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.3%, Avg loss: 3.399295 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.375054  [   64/19873]\n",
      "loss: 3.397227  [ 6464/19873]\n",
      "loss: 3.332397  [12864/19873]\n",
      "loss: 3.391159  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.1%, Avg loss: 3.387359 \n",
      "\n",
      "Testing size: 8192\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.433052  [   64/19873]\n",
      "loss: 3.430113  [ 6464/19873]\n",
      "loss: 3.424755  [12864/19873]\n",
      "loss: 3.423829  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.7%, Avg loss: 3.425800 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.419499  [   64/19873]\n",
      "loss: 3.421037  [ 6464/19873]\n",
      "loss: 3.406061  [12864/19873]\n",
      "loss: 3.415708  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.8%, Avg loss: 3.417444 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.405203  [   64/19873]\n",
      "loss: 3.411750  [ 6464/19873]\n",
      "loss: 3.385007  [12864/19873]\n",
      "loss: 3.407087  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.9%, Avg loss: 3.408262 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.389159  [   64/19873]\n",
      "loss: 3.401875  [ 6464/19873]\n",
      "loss: 3.360141  [12864/19873]\n",
      "loss: 3.397648  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.8%, Avg loss: 3.397748 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.370291  [   64/19873]\n",
      "loss: 3.391054  [ 6464/19873]\n",
      "loss: 3.329724  [12864/19873]\n",
      "loss: 3.387196  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.0%, Avg loss: 3.385425 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=512):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(71636, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "sizes_to_test = [4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "for size in sizes_to_test:\n",
    "    print(f\"Testing size: {size}\")\n",
    "    model = NeuralNetwork(size=size)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing size: 4\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.408898  [   64/19873]\n",
      "loss: 3.566106  [ 6464/19873]\n",
      "loss: 3.425845  [12864/19873]\n",
      "loss: 3.469388  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.413628 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.404488  [   64/19873]\n",
      "loss: 3.562757  [ 6464/19873]\n",
      "loss: 3.423323  [12864/19873]\n",
      "loss: 3.466210  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.410467 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.403158  [   64/19873]\n",
      "loss: 3.558125  [ 6464/19873]\n",
      "loss: 3.420927  [12864/19873]\n",
      "loss: 3.464231  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.406890 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.401541  [   64/19873]\n",
      "loss: 3.554965  [ 6464/19873]\n",
      "loss: 3.418216  [12864/19873]\n",
      "loss: 3.462960  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.402931 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.399511  [   64/19873]\n",
      "loss: 3.552695  [ 6464/19873]\n",
      "loss: 3.415090  [12864/19873]\n",
      "loss: 3.462414  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.398773 \n",
      "\n",
      "Testing size: 8\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.421062  [   64/19873]\n",
      "loss: 3.489963  [ 6464/19873]\n",
      "loss: 3.464026  [12864/19873]\n",
      "loss: 3.449370  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.459817 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.420954  [   64/19873]\n",
      "loss: 3.485709  [ 6464/19873]\n",
      "loss: 3.458102  [12864/19873]\n",
      "loss: 3.442622  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.457910 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.421549  [   64/19873]\n",
      "loss: 3.483339  [ 6464/19873]\n",
      "loss: 3.454328  [12864/19873]\n",
      "loss: 3.437456  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.456362 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.421681  [   64/19873]\n",
      "loss: 3.481622  [ 6464/19873]\n",
      "loss: 3.451307  [12864/19873]\n",
      "loss: 3.434201  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.455880 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.421688  [   64/19873]\n",
      "loss: 3.479647  [ 6464/19873]\n",
      "loss: 3.448720  [12864/19873]\n",
      "loss: 3.431637  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 3.455862 \n",
      "\n",
      "Testing size: 16\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.456981  [   64/19873]\n",
      "loss: 3.451030  [ 6464/19873]\n",
      "loss: 3.466917  [12864/19873]\n",
      "loss: 3.481651  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.7%, Avg loss: 3.435068 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.454716  [   64/19873]\n",
      "loss: 3.450223  [ 6464/19873]\n",
      "loss: 3.463263  [12864/19873]\n",
      "loss: 3.477715  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.7%, Avg loss: 3.432694 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.454301  [   64/19873]\n",
      "loss: 3.448397  [ 6464/19873]\n",
      "loss: 3.459429  [12864/19873]\n",
      "loss: 3.474401  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.9%, Avg loss: 3.430008 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.454433  [   64/19873]\n",
      "loss: 3.447344  [ 6464/19873]\n",
      "loss: 3.455362  [12864/19873]\n",
      "loss: 3.471404  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.3%, Avg loss: 3.427604 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.454349  [   64/19873]\n",
      "loss: 3.446252  [ 6464/19873]\n",
      "loss: 3.452193  [12864/19873]\n",
      "loss: 3.468578  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.8%, Avg loss: 3.425031 \n",
      "\n",
      "Testing size: 32\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.429279  [   64/19873]\n",
      "loss: 3.424176  [ 6464/19873]\n",
      "loss: 3.446534  [12864/19873]\n",
      "loss: 3.440797  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.6%, Avg loss: 3.411859 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.425877  [   64/19873]\n",
      "loss: 3.422336  [ 6464/19873]\n",
      "loss: 3.441524  [12864/19873]\n",
      "loss: 3.437520  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.6%, Avg loss: 3.410935 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.422870  [   64/19873]\n",
      "loss: 3.420912  [ 6464/19873]\n",
      "loss: 3.436922  [12864/19873]\n",
      "loss: 3.434862  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.6%, Avg loss: 3.409955 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.420891  [   64/19873]\n",
      "loss: 3.419730  [ 6464/19873]\n",
      "loss: 3.432899  [12864/19873]\n",
      "loss: 3.432429  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.6%, Avg loss: 3.409148 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.419072  [   64/19873]\n",
      "loss: 3.418516  [ 6464/19873]\n",
      "loss: 3.429073  [12864/19873]\n",
      "loss: 3.430173  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.6%, Avg loss: 3.408359 \n",
      "\n",
      "Testing size: 64\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.436248  [   64/19873]\n",
      "loss: 3.442492  [ 6464/19873]\n",
      "loss: 3.438823  [12864/19873]\n",
      "loss: 3.445508  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.0%, Avg loss: 3.444825 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.433841  [   64/19873]\n",
      "loss: 3.440529  [ 6464/19873]\n",
      "loss: 3.434588  [12864/19873]\n",
      "loss: 3.442109  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.0%, Avg loss: 3.442734 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.430689  [   64/19873]\n",
      "loss: 3.438476  [ 6464/19873]\n",
      "loss: 3.429025  [12864/19873]\n",
      "loss: 3.438211  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.1%, Avg loss: 3.440635 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.427373  [   64/19873]\n",
      "loss: 3.436514  [ 6464/19873]\n",
      "loss: 3.423955  [12864/19873]\n",
      "loss: 3.434780  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.0%, Avg loss: 3.438716 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.424087  [   64/19873]\n",
      "loss: 3.435175  [ 6464/19873]\n",
      "loss: 3.418997  [12864/19873]\n",
      "loss: 3.431994  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.8%, Avg loss: 3.437107 \n",
      "\n",
      "Testing size: 128\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.430741  [   64/19873]\n",
      "loss: 3.426291  [ 6464/19873]\n",
      "loss: 3.433274  [12864/19873]\n",
      "loss: 3.448116  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.441710 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.429105  [   64/19873]\n",
      "loss: 3.424124  [ 6464/19873]\n",
      "loss: 3.429306  [12864/19873]\n",
      "loss: 3.445324  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.439131 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.427411  [   64/19873]\n",
      "loss: 3.422170  [ 6464/19873]\n",
      "loss: 3.425340  [12864/19873]\n",
      "loss: 3.441891  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.1%, Avg loss: 3.436574 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.425795  [   64/19873]\n",
      "loss: 3.419995  [ 6464/19873]\n",
      "loss: 3.421576  [12864/19873]\n",
      "loss: 3.438893  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.5%, Avg loss: 3.434252 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.424201  [   64/19873]\n",
      "loss: 3.417680  [ 6464/19873]\n",
      "loss: 3.417642  [12864/19873]\n",
      "loss: 3.435913  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.6%, Avg loss: 3.431896 \n",
      "\n",
      "Testing size: 256\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.438660  [   64/19873]\n",
      "loss: 3.441538  [ 6464/19873]\n",
      "loss: 3.445272  [12864/19873]\n",
      "loss: 3.441363  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 3.438838 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.436474  [   64/19873]\n",
      "loss: 3.439518  [ 6464/19873]\n",
      "loss: 3.441510  [12864/19873]\n",
      "loss: 3.438514  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.6%, Avg loss: 3.437386 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.434349  [   64/19873]\n",
      "loss: 3.437373  [ 6464/19873]\n",
      "loss: 3.437722  [12864/19873]\n",
      "loss: 3.435447  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.6%, Avg loss: 3.435870 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.432207  [   64/19873]\n",
      "loss: 3.435027  [ 6464/19873]\n",
      "loss: 3.434176  [12864/19873]\n",
      "loss: 3.432465  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.6%, Avg loss: 3.434409 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.430052  [   64/19873]\n",
      "loss: 3.432590  [ 6464/19873]\n",
      "loss: 3.430792  [12864/19873]\n",
      "loss: 3.429618  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.8%, Avg loss: 3.432976 \n",
      "\n",
      "Testing size: 512\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.437609  [   64/19873]\n",
      "loss: 3.429032  [ 6464/19873]\n",
      "loss: 3.439580  [12864/19873]\n",
      "loss: 3.429970  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.5%, Avg loss: 3.431529 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.435633  [   64/19873]\n",
      "loss: 3.427340  [ 6464/19873]\n",
      "loss: 3.436420  [12864/19873]\n",
      "loss: 3.427483  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.6%, Avg loss: 3.430217 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.433636  [   64/19873]\n",
      "loss: 3.425630  [ 6464/19873]\n",
      "loss: 3.433146  [12864/19873]\n",
      "loss: 3.425205  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 0.8%, Avg loss: 3.428866 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.431453  [   64/19873]\n",
      "loss: 3.423969  [ 6464/19873]\n",
      "loss: 3.429735  [12864/19873]\n",
      "loss: 3.422904  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.3%, Avg loss: 3.427503 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.429014  [   64/19873]\n",
      "loss: 3.422258  [ 6464/19873]\n",
      "loss: 3.426240  [12864/19873]\n",
      "loss: 3.420561  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.7%, Avg loss: 3.426087 \n",
      "\n",
      "Testing size: 1024\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.430113  [   64/19873]\n",
      "loss: 3.437578  [ 6464/19873]\n",
      "loss: 3.427781  [12864/19873]\n",
      "loss: 3.427255  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 3.430534 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.428147  [   64/19873]\n",
      "loss: 3.435431  [ 6464/19873]\n",
      "loss: 3.424416  [12864/19873]\n",
      "loss: 3.425018  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 3.428887 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.426129  [   64/19873]\n",
      "loss: 3.433296  [ 6464/19873]\n",
      "loss: 3.421031  [12864/19873]\n",
      "loss: 3.422773  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.4%, Avg loss: 3.427227 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.424110  [   64/19873]\n",
      "loss: 3.431142  [ 6464/19873]\n",
      "loss: 3.417512  [12864/19873]\n",
      "loss: 3.420509  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.5%, Avg loss: 3.425479 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.422173  [   64/19873]\n",
      "loss: 3.429062  [ 6464/19873]\n",
      "loss: 3.413882  [12864/19873]\n",
      "loss: 3.418214  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.8%, Avg loss: 3.423694 \n",
      "\n",
      "Testing size: 2048\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.434841  [   64/19873]\n",
      "loss: 3.431907  [ 6464/19873]\n",
      "loss: 3.435506  [12864/19873]\n",
      "loss: 3.427737  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 1.6%, Avg loss: 3.430232 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.433041  [   64/19873]\n",
      "loss: 3.430317  [ 6464/19873]\n",
      "loss: 3.432545  [12864/19873]\n",
      "loss: 3.425919  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.0%, Avg loss: 3.428935 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.431224  [   64/19873]\n",
      "loss: 3.428733  [ 6464/19873]\n",
      "loss: 3.429508  [12864/19873]\n",
      "loss: 3.424128  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.1%, Avg loss: 3.427620 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.429259  [   64/19873]\n",
      "loss: 3.427093  [ 6464/19873]\n",
      "loss: 3.426401  [12864/19873]\n",
      "loss: 3.422328  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.0%, Avg loss: 3.426247 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.427170  [   64/19873]\n",
      "loss: 3.425376  [ 6464/19873]\n",
      "loss: 3.423066  [12864/19873]\n",
      "loss: 3.420497  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 3.424792 \n",
      "\n",
      "Testing size: 4096\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.433882  [   64/19873]\n",
      "loss: 3.430556  [ 6464/19873]\n",
      "loss: 3.434592  [12864/19873]\n",
      "loss: 3.432803  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.4%, Avg loss: 3.430338 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.431729  [   64/19873]\n",
      "loss: 3.428720  [ 6464/19873]\n",
      "loss: 3.431496  [12864/19873]\n",
      "loss: 3.430742  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.4%, Avg loss: 3.429114 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.429600  [   64/19873]\n",
      "loss: 3.426912  [ 6464/19873]\n",
      "loss: 3.428314  [12864/19873]\n",
      "loss: 3.428704  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.4%, Avg loss: 3.427863 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.427424  [   64/19873]\n",
      "loss: 3.425081  [ 6464/19873]\n",
      "loss: 3.424986  [12864/19873]\n",
      "loss: 3.426643  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.8%, Avg loss: 3.426545 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.425107  [   64/19873]\n",
      "loss: 3.423224  [ 6464/19873]\n",
      "loss: 3.421461  [12864/19873]\n",
      "loss: 3.424523  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.2%, Avg loss: 3.425143 \n",
      "\n",
      "Testing size: 8192\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.435421  [   64/19873]\n",
      "loss: 3.435006  [ 6464/19873]\n",
      "loss: 3.436190  [12864/19873]\n",
      "loss: 3.433785  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.432648 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.433021  [   64/19873]\n",
      "loss: 3.432737  [ 6464/19873]\n",
      "loss: 3.432377  [12864/19873]\n",
      "loss: 3.431265  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.5%, Avg loss: 3.431055 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.430596  [   64/19873]\n",
      "loss: 3.430475  [ 6464/19873]\n",
      "loss: 3.428457  [12864/19873]\n",
      "loss: 3.428746  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.3%, Avg loss: 3.429403 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.428086  [   64/19873]\n",
      "loss: 3.428205  [ 6464/19873]\n",
      "loss: 3.424345  [12864/19873]\n",
      "loss: 3.426182  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.7%, Avg loss: 3.427658 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.425407  [   64/19873]\n",
      "loss: 3.425912  [ 6464/19873]\n",
      "loss: 3.419919  [12864/19873]\n",
      "loss: 3.423527  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.8%, Avg loss: 3.425777 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=512):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(71636, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 2*size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "sizes_to_test = [4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "for size in sizes_to_test:\n",
    "    print(f\"Testing size: {size}\")\n",
    "    model = NeuralNetwork(size=size)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing learning rate: 0.1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.438538  [   64/19873]\n",
      "loss: 3.232512  [ 6464/19873]\n",
      "loss: 2.405616  [12864/19873]\n",
      "loss: 2.543983  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.0%, Avg loss: 2.796548 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.152519  [   64/19873]\n",
      "loss: 2.420685  [ 6464/19873]\n",
      "loss: 1.841949  [12864/19873]\n",
      "loss: 2.246860  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 17.5%, Avg loss: 2.967362 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.870271  [   64/19873]\n",
      "loss: 2.210711  [ 6464/19873]\n",
      "loss: 1.516833  [12864/19873]\n",
      "loss: 2.150987  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.2%, Avg loss: 2.890151 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.235148  [   64/19873]\n",
      "loss: 1.844906  [ 6464/19873]\n",
      "loss: 1.348147  [12864/19873]\n",
      "loss: 2.500719  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.7%, Avg loss: 5.179556 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.653522  [   64/19873]\n",
      "loss: 1.453320  [ 6464/19873]\n",
      "loss: 1.100212  [12864/19873]\n",
      "loss: 2.447454  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 2.715824 \n",
      "\n",
      "Testing learning rate: 0.01\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.438311  [   64/19873]\n",
      "loss: 3.408638  [ 6464/19873]\n",
      "loss: 3.286444  [12864/19873]\n",
      "loss: 3.345181  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.5%, Avg loss: 3.288057 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.209617  [   64/19873]\n",
      "loss: 3.285846  [ 6464/19873]\n",
      "loss: 2.503124  [12864/19873]\n",
      "loss: 3.249554  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.6%, Avg loss: 3.070137 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.020800  [   64/19873]\n",
      "loss: 3.140428  [ 6464/19873]\n",
      "loss: 2.323559  [12864/19873]\n",
      "loss: 3.087327  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.9%, Avg loss: 2.917675 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.917052  [   64/19873]\n",
      "loss: 2.907793  [ 6464/19873]\n",
      "loss: 2.192784  [12864/19873]\n",
      "loss: 2.881922  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.8%, Avg loss: 2.755246 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.745766  [   64/19873]\n",
      "loss: 2.634583  [ 6464/19873]\n",
      "loss: 2.067301  [12864/19873]\n",
      "loss: 2.673718  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.5%, Avg loss: 2.597900 \n",
      "\n",
      "Testing learning rate: 0.001\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.434576  [   64/19873]\n",
      "loss: 3.429962  [ 6464/19873]\n",
      "loss: 3.425077  [12864/19873]\n",
      "loss: 3.427764  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.3%, Avg loss: 3.425235 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.421201  [   64/19873]\n",
      "loss: 3.421767  [ 6464/19873]\n",
      "loss: 3.408158  [12864/19873]\n",
      "loss: 3.419171  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg loss: 3.417814 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.407462  [   64/19873]\n",
      "loss: 3.413454  [ 6464/19873]\n",
      "loss: 3.389299  [12864/19873]\n",
      "loss: 3.410134  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.2%, Avg loss: 3.409501 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.392337  [   64/19873]\n",
      "loss: 3.404465  [ 6464/19873]\n",
      "loss: 3.367292  [12864/19873]\n",
      "loss: 3.400237  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.0%, Avg loss: 3.399927 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.375172  [   64/19873]\n",
      "loss: 3.394237  [ 6464/19873]\n",
      "loss: 3.340902  [12864/19873]\n",
      "loss: 3.389340  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.4%, Avg loss: 3.388576 \n",
      "\n",
      "Testing learning rate: 0.0001\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.434455  [   64/19873]\n",
      "loss: 3.432490  [ 6464/19873]\n",
      "loss: 3.426924  [12864/19873]\n",
      "loss: 3.436389  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.6%, Avg loss: 3.433572 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.433110  [   64/19873]\n",
      "loss: 3.431558  [ 6464/19873]\n",
      "loss: 3.425081  [12864/19873]\n",
      "loss: 3.435608  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.0%, Avg loss: 3.432743 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.431788  [   64/19873]\n",
      "loss: 3.430645  [ 6464/19873]\n",
      "loss: 3.423254  [12864/19873]\n",
      "loss: 3.434834  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.0%, Avg loss: 3.431920 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.430474  [   64/19873]\n",
      "loss: 3.429745  [ 6464/19873]\n",
      "loss: 3.421441  [12864/19873]\n",
      "loss: 3.434055  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.2%, Avg loss: 3.431107 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.429165  [   64/19873]\n",
      "loss: 3.428860  [ 6464/19873]\n",
      "loss: 3.419643  [12864/19873]\n",
      "loss: 3.433272  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.430300 \n",
      "\n",
      "Testing learning rate: 1e-05\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.434235  [   64/19873]\n",
      "loss: 3.434117  [ 6464/19873]\n",
      "loss: 3.427557  [12864/19873]\n",
      "loss: 3.435192  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.3%, Avg loss: 3.439867 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.434109  [   64/19873]\n",
      "loss: 3.434039  [ 6464/19873]\n",
      "loss: 3.427393  [12864/19873]\n",
      "loss: 3.435115  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.3%, Avg loss: 3.439796 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.433984  [   64/19873]\n",
      "loss: 3.433962  [ 6464/19873]\n",
      "loss: 3.427230  [12864/19873]\n",
      "loss: 3.435038  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.3%, Avg loss: 3.439724 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.433858  [   64/19873]\n",
      "loss: 3.433885  [ 6464/19873]\n",
      "loss: 3.427066  [12864/19873]\n",
      "loss: 3.434961  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.3%, Avg loss: 3.439653 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.433733  [   64/19873]\n",
      "loss: 3.433808  [ 6464/19873]\n",
      "loss: 3.426903  [12864/19873]\n",
      "loss: 3.434884  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.4%, Avg loss: 3.439581 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=512):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(71636, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "r_to_test = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "for lr in lr_to_test:\n",
    "    print(f\"Testing learning rate: {lr}\")\n",
    "    model = NeuralNetwork(size=2048)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.437656  [   64/19873]\n",
      "loss: 3.411568  [ 6464/19873]\n",
      "loss: 3.299207  [12864/19873]\n",
      "loss: 3.356930  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.5%, Avg loss: 3.290393 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.206783  [   64/19873]\n",
      "loss: 3.288518  [ 6464/19873]\n",
      "loss: 2.500663  [12864/19873]\n",
      "loss: 3.264573  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.7%, Avg loss: 3.077556 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.024760  [   64/19873]\n",
      "loss: 3.155875  [ 6464/19873]\n",
      "loss: 2.324517  [12864/19873]\n",
      "loss: 3.111621  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.1%, Avg loss: 2.928454 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.940517  [   64/19873]\n",
      "loss: 2.943711  [ 6464/19873]\n",
      "loss: 2.198317  [12864/19873]\n",
      "loss: 2.902158  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.3%, Avg loss: 2.772678 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.780496  [   64/19873]\n",
      "loss: 2.668342  [ 6464/19873]\n",
      "loss: 2.075574  [12864/19873]\n",
      "loss: 2.686620  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 17.9%, Avg loss: 2.616878 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.549902  [   64/19873]\n",
      "loss: 2.439976  [ 6464/19873]\n",
      "loss: 1.938313  [12864/19873]\n",
      "loss: 2.504743  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.2%, Avg loss: 2.485929 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.325297  [   64/19873]\n",
      "loss: 2.272206  [ 6464/19873]\n",
      "loss: 1.803018  [12864/19873]\n",
      "loss: 2.356519  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 21.5%, Avg loss: 2.387650 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.153159  [   64/19873]\n",
      "loss: 2.120801  [ 6464/19873]\n",
      "loss: 1.681291  [12864/19873]\n",
      "loss: 2.234636  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.7%, Avg loss: 2.316141 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.027330  [   64/19873]\n",
      "loss: 1.969004  [ 6464/19873]\n",
      "loss: 1.571426  [12864/19873]\n",
      "loss: 2.124441  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.7%, Avg loss: 2.271550 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.931458  [   64/19873]\n",
      "loss: 1.819063  [ 6464/19873]\n",
      "loss: 1.475349  [12864/19873]\n",
      "loss: 2.019705  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.3%, Avg loss: 2.255732 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.853337  [   64/19873]\n",
      "loss: 1.680074  [ 6464/19873]\n",
      "loss: 1.391549  [12864/19873]\n",
      "loss: 1.922509  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.2%, Avg loss: 2.258900 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.783228  [   64/19873]\n",
      "loss: 1.557801  [ 6464/19873]\n",
      "loss: 1.319806  [12864/19873]\n",
      "loss: 1.832688  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 29.0%, Avg loss: 2.270188 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.714830  [   64/19873]\n",
      "loss: 1.447812  [ 6464/19873]\n",
      "loss: 1.255432  [12864/19873]\n",
      "loss: 1.751049  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.0%, Avg loss: 2.280533 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.644142  [   64/19873]\n",
      "loss: 1.348347  [ 6464/19873]\n",
      "loss: 1.195720  [12864/19873]\n",
      "loss: 1.676870  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.0%, Avg loss: 2.280421 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.569591  [   64/19873]\n",
      "loss: 1.255982  [ 6464/19873]\n",
      "loss: 1.138067  [12864/19873]\n",
      "loss: 1.607711  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 2.270144 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.492670  [   64/19873]\n",
      "loss: 1.167378  [ 6464/19873]\n",
      "loss: 1.082211  [12864/19873]\n",
      "loss: 1.540930  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 2.250878 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.414730  [   64/19873]\n",
      "loss: 1.081942  [ 6464/19873]\n",
      "loss: 1.028415  [12864/19873]\n",
      "loss: 1.475166  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 36.3%, Avg loss: 2.229559 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.339286  [   64/19873]\n",
      "loss: 0.999907  [ 6464/19873]\n",
      "loss: 0.977056  [12864/19873]\n",
      "loss: 1.409488  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 2.207993 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.266528  [   64/19873]\n",
      "loss: 0.920689  [ 6464/19873]\n",
      "loss: 0.927835  [12864/19873]\n",
      "loss: 1.343224  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 2.189759 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.196828  [   64/19873]\n",
      "loss: 0.846721  [ 6464/19873]\n",
      "loss: 0.880510  [12864/19873]\n",
      "loss: 1.275558  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 40.8%, Avg loss: 2.171791 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.124231  [   64/19873]\n",
      "loss: 0.781169  [ 6464/19873]\n",
      "loss: 0.835443  [12864/19873]\n",
      "loss: 1.207343  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 41.7%, Avg loss: 2.153726 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.048813  [   64/19873]\n",
      "loss: 0.723769  [ 6464/19873]\n",
      "loss: 0.792387  [12864/19873]\n",
      "loss: 1.139720  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 42.7%, Avg loss: 2.136228 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.970952  [   64/19873]\n",
      "loss: 0.674516  [ 6464/19873]\n",
      "loss: 0.751539  [12864/19873]\n",
      "loss: 1.073373  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 44.0%, Avg loss: 2.120017 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.893713  [   64/19873]\n",
      "loss: 0.631282  [ 6464/19873]\n",
      "loss: 0.712596  [12864/19873]\n",
      "loss: 1.010021  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.3%, Avg loss: 2.104444 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.821693  [   64/19873]\n",
      "loss: 0.592890  [ 6464/19873]\n",
      "loss: 0.675907  [12864/19873]\n",
      "loss: 0.949475  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 46.1%, Avg loss: 2.091028 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.758758  [   64/19873]\n",
      "loss: 0.557972  [ 6464/19873]\n",
      "loss: 0.641060  [12864/19873]\n",
      "loss: 0.892032  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 2.077885 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.705506  [   64/19873]\n",
      "loss: 0.525854  [ 6464/19873]\n",
      "loss: 0.607637  [12864/19873]\n",
      "loss: 0.837430  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 2.065233 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.661055  [   64/19873]\n",
      "loss: 0.495698  [ 6464/19873]\n",
      "loss: 0.575827  [12864/19873]\n",
      "loss: 0.785862  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 2.055496 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.624160  [   64/19873]\n",
      "loss: 0.467472  [ 6464/19873]\n",
      "loss: 0.545365  [12864/19873]\n",
      "loss: 0.736717  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 48.4%, Avg loss: 2.046685 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.593097  [   64/19873]\n",
      "loss: 0.440859  [ 6464/19873]\n",
      "loss: 0.516185  [12864/19873]\n",
      "loss: 0.690105  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 2.038909 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.565972  [   64/19873]\n",
      "loss: 0.415656  [ 6464/19873]\n",
      "loss: 0.488323  [12864/19873]\n",
      "loss: 0.646233  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 2.033793 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.542547  [   64/19873]\n",
      "loss: 0.392047  [ 6464/19873]\n",
      "loss: 0.461649  [12864/19873]\n",
      "loss: 0.605002  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 2.029102 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.521273  [   64/19873]\n",
      "loss: 0.369764  [ 6464/19873]\n",
      "loss: 0.436252  [12864/19873]\n",
      "loss: 0.566178  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 2.025358 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.501868  [   64/19873]\n",
      "loss: 0.348539  [ 6464/19873]\n",
      "loss: 0.412241  [12864/19873]\n",
      "loss: 0.530041  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 2.022781 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.484032  [   64/19873]\n",
      "loss: 0.328547  [ 6464/19873]\n",
      "loss: 0.389412  [12864/19873]\n",
      "loss: 0.495999  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 2.021942 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.467264  [   64/19873]\n",
      "loss: 0.309747  [ 6464/19873]\n",
      "loss: 0.368049  [12864/19873]\n",
      "loss: 0.464377  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 2.022060 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.451347  [   64/19873]\n",
      "loss: 0.291959  [ 6464/19873]\n",
      "loss: 0.347665  [12864/19873]\n",
      "loss: 0.434998  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 2.023093 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.435873  [   64/19873]\n",
      "loss: 0.275271  [ 6464/19873]\n",
      "loss: 0.328690  [12864/19873]\n",
      "loss: 0.407696  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 2.025789 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.421039  [   64/19873]\n",
      "loss: 0.259614  [ 6464/19873]\n",
      "loss: 0.310963  [12864/19873]\n",
      "loss: 0.382289  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 2.029290 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.406463  [   64/19873]\n",
      "loss: 0.245020  [ 6464/19873]\n",
      "loss: 0.294367  [12864/19873]\n",
      "loss: 0.358701  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 2.034125 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.392285  [   64/19873]\n",
      "loss: 0.231338  [ 6464/19873]\n",
      "loss: 0.278967  [12864/19873]\n",
      "loss: 0.336696  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 2.039627 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.378282  [   64/19873]\n",
      "loss: 0.218761  [ 6464/19873]\n",
      "loss: 0.264490  [12864/19873]\n",
      "loss: 0.316262  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 2.045934 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.364645  [   64/19873]\n",
      "loss: 0.207019  [ 6464/19873]\n",
      "loss: 0.251018  [12864/19873]\n",
      "loss: 0.297199  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 2.053639 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.351315  [   64/19873]\n",
      "loss: 0.196167  [ 6464/19873]\n",
      "loss: 0.238438  [12864/19873]\n",
      "loss: 0.279569  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 2.061524 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.338207  [   64/19873]\n",
      "loss: 0.186070  [ 6464/19873]\n",
      "loss: 0.226921  [12864/19873]\n",
      "loss: 0.263377  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 2.070737 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.325607  [   64/19873]\n",
      "loss: 0.176687  [ 6464/19873]\n",
      "loss: 0.216047  [12864/19873]\n",
      "loss: 0.248480  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 2.080069 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.313167  [   64/19873]\n",
      "loss: 0.168032  [ 6464/19873]\n",
      "loss: 0.206096  [12864/19873]\n",
      "loss: 0.234792  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 2.090119 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.301525  [   64/19873]\n",
      "loss: 0.159968  [ 6464/19873]\n",
      "loss: 0.196750  [12864/19873]\n",
      "loss: 0.222408  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 2.100776 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.290284  [   64/19873]\n",
      "loss: 0.152545  [ 6464/19873]\n",
      "loss: 0.188185  [12864/19873]\n",
      "loss: 0.211208  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 2.111399 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.279466  [   64/19873]\n",
      "loss: 0.145570  [ 6464/19873]\n",
      "loss: 0.180097  [12864/19873]\n",
      "loss: 0.201003  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 2.123302 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.269375  [   64/19873]\n",
      "loss: 0.139006  [ 6464/19873]\n",
      "loss: 0.172655  [12864/19873]\n",
      "loss: 0.191773  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 2.134893 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.259985  [   64/19873]\n",
      "loss: 0.133042  [ 6464/19873]\n",
      "loss: 0.165670  [12864/19873]\n",
      "loss: 0.183569  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.146852 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.251070  [   64/19873]\n",
      "loss: 0.127295  [ 6464/19873]\n",
      "loss: 0.159312  [12864/19873]\n",
      "loss: 0.176204  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.158650 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.242719  [   64/19873]\n",
      "loss: 0.122012  [ 6464/19873]\n",
      "loss: 0.153269  [12864/19873]\n",
      "loss: 0.169573  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.171110 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.234982  [   64/19873]\n",
      "loss: 0.117041  [ 6464/19873]\n",
      "loss: 0.147785  [12864/19873]\n",
      "loss: 0.163688  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.183770 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.227672  [   64/19873]\n",
      "loss: 0.112365  [ 6464/19873]\n",
      "loss: 0.142600  [12864/19873]\n",
      "loss: 0.158370  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.196444 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.220883  [   64/19873]\n",
      "loss: 0.107960  [ 6464/19873]\n",
      "loss: 0.137768  [12864/19873]\n",
      "loss: 0.153465  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.209066 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.214570  [   64/19873]\n",
      "loss: 0.103780  [ 6464/19873]\n",
      "loss: 0.133120  [12864/19873]\n",
      "loss: 0.149076  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.222087 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.208632  [   64/19873]\n",
      "loss: 0.099965  [ 6464/19873]\n",
      "loss: 0.128890  [12864/19873]\n",
      "loss: 0.145086  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.234091 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.203009  [   64/19873]\n",
      "loss: 0.096347  [ 6464/19873]\n",
      "loss: 0.124728  [12864/19873]\n",
      "loss: 0.141444  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.246549 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.197968  [   64/19873]\n",
      "loss: 0.092962  [ 6464/19873]\n",
      "loss: 0.120888  [12864/19873]\n",
      "loss: 0.137994  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.259298 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.192844  [   64/19873]\n",
      "loss: 0.089823  [ 6464/19873]\n",
      "loss: 0.117328  [12864/19873]\n",
      "loss: 0.134876  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.271490 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.188348  [   64/19873]\n",
      "loss: 0.086816  [ 6464/19873]\n",
      "loss: 0.113819  [12864/19873]\n",
      "loss: 0.132109  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 2.283396 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.184223  [   64/19873]\n",
      "loss: 0.084112  [ 6464/19873]\n",
      "loss: 0.110578  [12864/19873]\n",
      "loss: 0.129388  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.295288 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.180021  [   64/19873]\n",
      "loss: 0.081477  [ 6464/19873]\n",
      "loss: 0.107507  [12864/19873]\n",
      "loss: 0.126962  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.307609 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.176283  [   64/19873]\n",
      "loss: 0.079074  [ 6464/19873]\n",
      "loss: 0.104495  [12864/19873]\n",
      "loss: 0.124618  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.319301 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.172767  [   64/19873]\n",
      "loss: 0.076871  [ 6464/19873]\n",
      "loss: 0.101730  [12864/19873]\n",
      "loss: 0.122574  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.330827 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.169469  [   64/19873]\n",
      "loss: 0.074780  [ 6464/19873]\n",
      "loss: 0.099053  [12864/19873]\n",
      "loss: 0.120617  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.341963 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.166257  [   64/19873]\n",
      "loss: 0.072795  [ 6464/19873]\n",
      "loss: 0.096484  [12864/19873]\n",
      "loss: 0.118816  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.353650 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.163377  [   64/19873]\n",
      "loss: 0.070989  [ 6464/19873]\n",
      "loss: 0.094107  [12864/19873]\n",
      "loss: 0.117113  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.364065 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.160569  [   64/19873]\n",
      "loss: 0.069313  [ 6464/19873]\n",
      "loss: 0.091729  [12864/19873]\n",
      "loss: 0.115528  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.375347 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.157956  [   64/19873]\n",
      "loss: 0.067686  [ 6464/19873]\n",
      "loss: 0.089544  [12864/19873]\n",
      "loss: 0.114054  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.386576 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.155733  [   64/19873]\n",
      "loss: 0.066147  [ 6464/19873]\n",
      "loss: 0.087445  [12864/19873]\n",
      "loss: 0.112604  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.396199 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.153490  [   64/19873]\n",
      "loss: 0.064728  [ 6464/19873]\n",
      "loss: 0.085391  [12864/19873]\n",
      "loss: 0.111210  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.405629 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.151336  [   64/19873]\n",
      "loss: 0.063396  [ 6464/19873]\n",
      "loss: 0.083468  [12864/19873]\n",
      "loss: 0.109932  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.416048 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.149448  [   64/19873]\n",
      "loss: 0.062093  [ 6464/19873]\n",
      "loss: 0.081658  [12864/19873]\n",
      "loss: 0.108848  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.425524 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.147798  [   64/19873]\n",
      "loss: 0.060910  [ 6464/19873]\n",
      "loss: 0.079878  [12864/19873]\n",
      "loss: 0.107566  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.435502 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.145971  [   64/19873]\n",
      "loss: 0.059818  [ 6464/19873]\n",
      "loss: 0.078172  [12864/19873]\n",
      "loss: 0.106586  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.444065 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.144436  [   64/19873]\n",
      "loss: 0.058784  [ 6464/19873]\n",
      "loss: 0.076521  [12864/19873]\n",
      "loss: 0.105509  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.453436 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.143032  [   64/19873]\n",
      "loss: 0.057754  [ 6464/19873]\n",
      "loss: 0.074993  [12864/19873]\n",
      "loss: 0.104564  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.461738 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.141557  [   64/19873]\n",
      "loss: 0.056823  [ 6464/19873]\n",
      "loss: 0.073459  [12864/19873]\n",
      "loss: 0.103675  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.470587 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.140448  [   64/19873]\n",
      "loss: 0.055877  [ 6464/19873]\n",
      "loss: 0.072004  [12864/19873]\n",
      "loss: 0.102692  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.479049 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.139203  [   64/19873]\n",
      "loss: 0.055057  [ 6464/19873]\n",
      "loss: 0.070581  [12864/19873]\n",
      "loss: 0.101639  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.487720 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.138337  [   64/19873]\n",
      "loss: 0.054245  [ 6464/19873]\n",
      "loss: 0.069198  [12864/19873]\n",
      "loss: 0.100901  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.495089 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.137272  [   64/19873]\n",
      "loss: 0.053420  [ 6464/19873]\n",
      "loss: 0.067921  [12864/19873]\n",
      "loss: 0.100219  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.503426 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.136190  [   64/19873]\n",
      "loss: 0.052704  [ 6464/19873]\n",
      "loss: 0.066652  [12864/19873]\n",
      "loss: 0.099344  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.511397 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.135242  [   64/19873]\n",
      "loss: 0.052044  [ 6464/19873]\n",
      "loss: 0.065425  [12864/19873]\n",
      "loss: 0.098951  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.519365 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.134663  [   64/19873]\n",
      "loss: 0.051366  [ 6464/19873]\n",
      "loss: 0.064229  [12864/19873]\n",
      "loss: 0.098073  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.526118 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.133717  [   64/19873]\n",
      "loss: 0.050690  [ 6464/19873]\n",
      "loss: 0.063043  [12864/19873]\n",
      "loss: 0.097431  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.533116 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.133131  [   64/19873]\n",
      "loss: 0.050109  [ 6464/19873]\n",
      "loss: 0.061922  [12864/19873]\n",
      "loss: 0.096698  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.539902 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.132522  [   64/19873]\n",
      "loss: 0.049521  [ 6464/19873]\n",
      "loss: 0.060838  [12864/19873]\n",
      "loss: 0.096112  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.547817 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.132029  [   64/19873]\n",
      "loss: 0.048930  [ 6464/19873]\n",
      "loss: 0.059798  [12864/19873]\n",
      "loss: 0.095474  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.553847 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.131046  [   64/19873]\n",
      "loss: 0.048376  [ 6464/19873]\n",
      "loss: 0.058754  [12864/19873]\n",
      "loss: 0.094927  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 2.560931 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.130786  [   64/19873]\n",
      "loss: 0.047858  [ 6464/19873]\n",
      "loss: 0.057803  [12864/19873]\n",
      "loss: 0.094466  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.566334 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.130139  [   64/19873]\n",
      "loss: 0.047326  [ 6464/19873]\n",
      "loss: 0.056822  [12864/19873]\n",
      "loss: 0.093839  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.573215 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.129661  [   64/19873]\n",
      "loss: 0.046965  [ 6464/19873]\n",
      "loss: 0.055868  [12864/19873]\n",
      "loss: 0.093360  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.580444 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.129384  [   64/19873]\n",
      "loss: 0.046473  [ 6464/19873]\n",
      "loss: 0.054967  [12864/19873]\n",
      "loss: 0.092931  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.586575 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.128791  [   64/19873]\n",
      "loss: 0.046140  [ 6464/19873]\n",
      "loss: 0.054112  [12864/19873]\n",
      "loss: 0.092403  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.592888 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.128400  [   64/19873]\n",
      "loss: 0.045717  [ 6464/19873]\n",
      "loss: 0.053224  [12864/19873]\n",
      "loss: 0.092030  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.598113 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.127955  [   64/19873]\n",
      "loss: 0.045324  [ 6464/19873]\n",
      "loss: 0.052412  [12864/19873]\n",
      "loss: 0.091576  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.604170 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.float()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct, test_loss\n",
    "\n",
    "accuracies = []\n",
    "losses = []\n",
    "model = NeuralNetwork(size=2048)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "for e in range(100):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    c, l = test(val_dataloader, model, loss_fn)\n",
    "    accuracies.append(c)\n",
    "    losses.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.plot(range(len(accuracies)), accuracies, color='blue')\n",
    "ax1.set_ylabel('accuracy (%)', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(len(losses)), losses, color='red')\n",
    "ax2.set_ylabel('loss', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.435747  [   64/19873]\n",
      "loss: 3.436984  [ 6464/19873]\n",
      "loss: 3.423784  [12864/19873]\n",
      "loss: 3.430506  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 3.424442 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.424414  [   64/19873]\n",
      "loss: 3.429589  [ 6464/19873]\n",
      "loss: 3.406697  [12864/19873]\n",
      "loss: 3.423506  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.4%, Avg loss: 3.417385 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.411875  [   64/19873]\n",
      "loss: 3.422109  [ 6464/19873]\n",
      "loss: 3.386972  [12864/19873]\n",
      "loss: 3.416448  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.0%, Avg loss: 3.409485 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.397501  [   64/19873]\n",
      "loss: 3.414029  [ 6464/19873]\n",
      "loss: 3.363566  [12864/19873]\n",
      "loss: 3.408613  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.8%, Avg loss: 3.400243 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.380370  [   64/19873]\n",
      "loss: 3.405009  [ 6464/19873]\n",
      "loss: 3.334330  [12864/19873]\n",
      "loss: 3.399773  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.2%, Avg loss: 3.389042 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.359496  [   64/19873]\n",
      "loss: 3.394744  [ 6464/19873]\n",
      "loss: 3.297369  [12864/19873]\n",
      "loss: 3.390066  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.4%, Avg loss: 3.375263 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.333663  [   64/19873]\n",
      "loss: 3.383018  [ 6464/19873]\n",
      "loss: 3.250484  [12864/19873]\n",
      "loss: 3.379443  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.0%, Avg loss: 3.358282 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.301541  [   64/19873]\n",
      "loss: 3.369969  [ 6464/19873]\n",
      "loss: 3.190190  [12864/19873]\n",
      "loss: 3.368025  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.8%, Avg loss: 3.337363 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.261523  [   64/19873]\n",
      "loss: 3.355958  [ 6464/19873]\n",
      "loss: 3.112663  [12864/19873]\n",
      "loss: 3.356130  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.5%, Avg loss: 3.311836 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.212601  [   64/19873]\n",
      "loss: 3.341092  [ 6464/19873]\n",
      "loss: 3.015175  [12864/19873]\n",
      "loss: 3.343928  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.4%, Avg loss: 3.281635 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 3.156607  [   64/19873]\n",
      "loss: 3.325607  [ 6464/19873]\n",
      "loss: 2.900555  [12864/19873]\n",
      "loss: 3.331762  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.7%, Avg loss: 3.248881 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 3.100356  [   64/19873]\n",
      "loss: 3.310010  [ 6464/19873]\n",
      "loss: 2.782646  [12864/19873]\n",
      "loss: 3.319878  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.1%, Avg loss: 3.217921 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 3.053644  [   64/19873]\n",
      "loss: 3.294385  [ 6464/19873]\n",
      "loss: 2.681378  [12864/19873]\n",
      "loss: 3.308574  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.3%, Avg loss: 3.192171 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 3.019547  [   64/19873]\n",
      "loss: 3.279061  [ 6464/19873]\n",
      "loss: 2.606560  [12864/19873]\n",
      "loss: 3.298075  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.5%, Avg loss: 3.171800 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.995025  [   64/19873]\n",
      "loss: 3.263980  [ 6464/19873]\n",
      "loss: 2.554442  [12864/19873]\n",
      "loss: 3.288427  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.9%, Avg loss: 3.155221 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.976243  [   64/19873]\n",
      "loss: 3.249198  [ 6464/19873]\n",
      "loss: 2.517262  [12864/19873]\n",
      "loss: 3.279341  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.2%, Avg loss: 3.140982 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.960443  [   64/19873]\n",
      "loss: 3.234368  [ 6464/19873]\n",
      "loss: 2.489216  [12864/19873]\n",
      "loss: 3.270833  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.4%, Avg loss: 3.128160 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.946062  [   64/19873]\n",
      "loss: 3.219443  [ 6464/19873]\n",
      "loss: 2.466645  [12864/19873]\n",
      "loss: 3.262808  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.5%, Avg loss: 3.115944 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.932242  [   64/19873]\n",
      "loss: 3.204178  [ 6464/19873]\n",
      "loss: 2.447334  [12864/19873]\n",
      "loss: 3.254930  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.7%, Avg loss: 3.104003 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.918388  [   64/19873]\n",
      "loss: 3.188229  [ 6464/19873]\n",
      "loss: 2.429913  [12864/19873]\n",
      "loss: 3.246969  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.0%, Avg loss: 3.091988 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.904379  [   64/19873]\n",
      "loss: 3.171500  [ 6464/19873]\n",
      "loss: 2.413379  [12864/19873]\n",
      "loss: 3.238777  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.2%, Avg loss: 3.079691 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.889898  [   64/19873]\n",
      "loss: 3.153472  [ 6464/19873]\n",
      "loss: 2.397386  [12864/19873]\n",
      "loss: 3.230499  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.3%, Avg loss: 3.066917 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.874798  [   64/19873]\n",
      "loss: 3.134028  [ 6464/19873]\n",
      "loss: 2.381616  [12864/19873]\n",
      "loss: 3.221828  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.5%, Avg loss: 3.053538 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.859104  [   64/19873]\n",
      "loss: 3.113075  [ 6464/19873]\n",
      "loss: 2.365963  [12864/19873]\n",
      "loss: 3.212746  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 3.039474 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.842795  [   64/19873]\n",
      "loss: 3.090544  [ 6464/19873]\n",
      "loss: 2.350269  [12864/19873]\n",
      "loss: 3.203292  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 3.024815 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.825566  [   64/19873]\n",
      "loss: 3.066665  [ 6464/19873]\n",
      "loss: 2.334367  [12864/19873]\n",
      "loss: 3.193300  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 3.009465 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.807546  [   64/19873]\n",
      "loss: 3.041312  [ 6464/19873]\n",
      "loss: 2.318373  [12864/19873]\n",
      "loss: 3.182673  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.8%, Avg loss: 2.993478 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.788725  [   64/19873]\n",
      "loss: 3.014620  [ 6464/19873]\n",
      "loss: 2.302418  [12864/19873]\n",
      "loss: 3.171532  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.0%, Avg loss: 2.976816 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.769231  [   64/19873]\n",
      "loss: 2.986561  [ 6464/19873]\n",
      "loss: 2.286453  [12864/19873]\n",
      "loss: 3.159926  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.9%, Avg loss: 2.959575 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.748878  [   64/19873]\n",
      "loss: 2.957257  [ 6464/19873]\n",
      "loss: 2.270566  [12864/19873]\n",
      "loss: 3.147902  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.0%, Avg loss: 2.941755 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.727619  [   64/19873]\n",
      "loss: 2.927091  [ 6464/19873]\n",
      "loss: 2.254864  [12864/19873]\n",
      "loss: 3.135457  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 2.923431 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.705460  [   64/19873]\n",
      "loss: 2.896220  [ 6464/19873]\n",
      "loss: 2.239366  [12864/19873]\n",
      "loss: 3.122570  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.8%, Avg loss: 2.904781 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.682389  [   64/19873]\n",
      "loss: 2.864832  [ 6464/19873]\n",
      "loss: 2.224101  [12864/19873]\n",
      "loss: 3.109170  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.0%, Avg loss: 2.885772 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.658504  [   64/19873]\n",
      "loss: 2.833104  [ 6464/19873]\n",
      "loss: 2.209030  [12864/19873]\n",
      "loss: 3.095066  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.1%, Avg loss: 2.866515 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.633898  [   64/19873]\n",
      "loss: 2.801257  [ 6464/19873]\n",
      "loss: 2.194093  [12864/19873]\n",
      "loss: 3.080351  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.4%, Avg loss: 2.846987 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.608611  [   64/19873]\n",
      "loss: 2.769652  [ 6464/19873]\n",
      "loss: 2.179307  [12864/19873]\n",
      "loss: 3.065254  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.2%, Avg loss: 2.827385 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.582777  [   64/19873]\n",
      "loss: 2.738472  [ 6464/19873]\n",
      "loss: 2.164609  [12864/19873]\n",
      "loss: 3.049722  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.5%, Avg loss: 2.807802 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.556555  [   64/19873]\n",
      "loss: 2.707896  [ 6464/19873]\n",
      "loss: 2.150168  [12864/19873]\n",
      "loss: 3.033706  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.8%, Avg loss: 2.788370 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.530036  [   64/19873]\n",
      "loss: 2.678265  [ 6464/19873]\n",
      "loss: 2.135878  [12864/19873]\n",
      "loss: 3.017173  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 2.769083 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.503131  [   64/19873]\n",
      "loss: 2.649528  [ 6464/19873]\n",
      "loss: 2.121723  [12864/19873]\n",
      "loss: 3.000148  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.9%, Avg loss: 2.749978 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.476052  [   64/19873]\n",
      "loss: 2.621646  [ 6464/19873]\n",
      "loss: 2.107623  [12864/19873]\n",
      "loss: 2.982771  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 17.0%, Avg loss: 2.731099 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.448774  [   64/19873]\n",
      "loss: 2.594715  [ 6464/19873]\n",
      "loss: 2.093524  [12864/19873]\n",
      "loss: 2.965055  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 17.7%, Avg loss: 2.712488 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.421456  [   64/19873]\n",
      "loss: 2.568472  [ 6464/19873]\n",
      "loss: 2.079258  [12864/19873]\n",
      "loss: 2.946767  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.1%, Avg loss: 2.694068 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.394282  [   64/19873]\n",
      "loss: 2.543114  [ 6464/19873]\n",
      "loss: 2.064756  [12864/19873]\n",
      "loss: 2.928093  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.5%, Avg loss: 2.675935 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.367260  [   64/19873]\n",
      "loss: 2.518657  [ 6464/19873]\n",
      "loss: 2.050091  [12864/19873]\n",
      "loss: 2.908989  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.7%, Avg loss: 2.658173 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.340421  [   64/19873]\n",
      "loss: 2.494872  [ 6464/19873]\n",
      "loss: 2.035129  [12864/19873]\n",
      "loss: 2.889508  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.1%, Avg loss: 2.640734 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.313695  [   64/19873]\n",
      "loss: 2.471828  [ 6464/19873]\n",
      "loss: 2.019938  [12864/19873]\n",
      "loss: 2.869658  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.4%, Avg loss: 2.623597 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.287265  [   64/19873]\n",
      "loss: 2.449543  [ 6464/19873]\n",
      "loss: 2.004514  [12864/19873]\n",
      "loss: 2.849471  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.2%, Avg loss: 2.606848 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.261180  [   64/19873]\n",
      "loss: 2.427897  [ 6464/19873]\n",
      "loss: 1.988893  [12864/19873]\n",
      "loss: 2.829144  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.4%, Avg loss: 2.590457 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.235534  [   64/19873]\n",
      "loss: 2.407012  [ 6464/19873]\n",
      "loss: 1.973148  [12864/19873]\n",
      "loss: 2.808642  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.6%, Avg loss: 2.574406 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 2.210368  [   64/19873]\n",
      "loss: 2.386666  [ 6464/19873]\n",
      "loss: 1.957326  [12864/19873]\n",
      "loss: 2.788117  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.7%, Avg loss: 2.558695 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.185746  [   64/19873]\n",
      "loss: 2.366866  [ 6464/19873]\n",
      "loss: 1.941457  [12864/19873]\n",
      "loss: 2.767655  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.8%, Avg loss: 2.543361 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 2.161652  [   64/19873]\n",
      "loss: 2.347686  [ 6464/19873]\n",
      "loss: 1.925550  [12864/19873]\n",
      "loss: 2.747214  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 20.2%, Avg loss: 2.528311 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.138196  [   64/19873]\n",
      "loss: 2.328927  [ 6464/19873]\n",
      "loss: 1.909659  [12864/19873]\n",
      "loss: 2.726967  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 21.0%, Avg loss: 2.513564 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 2.115358  [   64/19873]\n",
      "loss: 2.310638  [ 6464/19873]\n",
      "loss: 1.893889  [12864/19873]\n",
      "loss: 2.706970  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 21.5%, Avg loss: 2.499148 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.093118  [   64/19873]\n",
      "loss: 2.292660  [ 6464/19873]\n",
      "loss: 1.878209  [12864/19873]\n",
      "loss: 2.687260  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 22.0%, Avg loss: 2.485036 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.071528  [   64/19873]\n",
      "loss: 2.275282  [ 6464/19873]\n",
      "loss: 1.862666  [12864/19873]\n",
      "loss: 2.667834  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 22.5%, Avg loss: 2.471209 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 2.050644  [   64/19873]\n",
      "loss: 2.258087  [ 6464/19873]\n",
      "loss: 1.847255  [12864/19873]\n",
      "loss: 2.648695  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 22.8%, Avg loss: 2.457685 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 2.030367  [   64/19873]\n",
      "loss: 2.241218  [ 6464/19873]\n",
      "loss: 1.832108  [12864/19873]\n",
      "loss: 2.629835  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.0%, Avg loss: 2.444394 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 2.010677  [   64/19873]\n",
      "loss: 2.224586  [ 6464/19873]\n",
      "loss: 1.817143  [12864/19873]\n",
      "loss: 2.611409  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.3%, Avg loss: 2.431397 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.991668  [   64/19873]\n",
      "loss: 2.208247  [ 6464/19873]\n",
      "loss: 1.802384  [12864/19873]\n",
      "loss: 2.593270  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.7%, Avg loss: 2.418614 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1.973242  [   64/19873]\n",
      "loss: 2.191947  [ 6464/19873]\n",
      "loss: 1.787853  [12864/19873]\n",
      "loss: 2.575454  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.1%, Avg loss: 2.406089 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.955430  [   64/19873]\n",
      "loss: 2.175835  [ 6464/19873]\n",
      "loss: 1.773554  [12864/19873]\n",
      "loss: 2.557828  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.4%, Avg loss: 2.393846 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.938166  [   64/19873]\n",
      "loss: 2.159845  [ 6464/19873]\n",
      "loss: 1.759470  [12864/19873]\n",
      "loss: 2.540558  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.5%, Avg loss: 2.381806 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.921453  [   64/19873]\n",
      "loss: 2.143860  [ 6464/19873]\n",
      "loss: 1.745595  [12864/19873]\n",
      "loss: 2.523429  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.5%, Avg loss: 2.370017 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.905298  [   64/19873]\n",
      "loss: 2.128011  [ 6464/19873]\n",
      "loss: 1.731931  [12864/19873]\n",
      "loss: 2.506552  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.9%, Avg loss: 2.358394 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.889641  [   64/19873]\n",
      "loss: 2.112088  [ 6464/19873]\n",
      "loss: 1.718474  [12864/19873]\n",
      "loss: 2.489970  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.1%, Avg loss: 2.347089 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.874398  [   64/19873]\n",
      "loss: 2.096267  [ 6464/19873]\n",
      "loss: 1.705191  [12864/19873]\n",
      "loss: 2.473606  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.3%, Avg loss: 2.335949 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.859595  [   64/19873]\n",
      "loss: 2.080477  [ 6464/19873]\n",
      "loss: 1.692103  [12864/19873]\n",
      "loss: 2.457469  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.6%, Avg loss: 2.325034 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.845230  [   64/19873]\n",
      "loss: 2.064658  [ 6464/19873]\n",
      "loss: 1.679234  [12864/19873]\n",
      "loss: 2.441474  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.7%, Avg loss: 2.314271 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.831249  [   64/19873]\n",
      "loss: 2.048792  [ 6464/19873]\n",
      "loss: 1.666585  [12864/19873]\n",
      "loss: 2.425731  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.8%, Avg loss: 2.303757 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.817656  [   64/19873]\n",
      "loss: 2.032935  [ 6464/19873]\n",
      "loss: 1.654115  [12864/19873]\n",
      "loss: 2.410090  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.1%, Avg loss: 2.293439 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.804366  [   64/19873]\n",
      "loss: 2.017061  [ 6464/19873]\n",
      "loss: 1.641838  [12864/19873]\n",
      "loss: 2.394660  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.2%, Avg loss: 2.283302 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.791428  [   64/19873]\n",
      "loss: 2.001213  [ 6464/19873]\n",
      "loss: 1.629717  [12864/19873]\n",
      "loss: 2.379361  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.5%, Avg loss: 2.273373 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.778751  [   64/19873]\n",
      "loss: 1.985323  [ 6464/19873]\n",
      "loss: 1.617744  [12864/19873]\n",
      "loss: 2.364108  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.9%, Avg loss: 2.263652 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.766497  [   64/19873]\n",
      "loss: 1.969324  [ 6464/19873]\n",
      "loss: 1.605917  [12864/19873]\n",
      "loss: 2.349032  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 27.0%, Avg loss: 2.254139 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.754501  [   64/19873]\n",
      "loss: 1.953310  [ 6464/19873]\n",
      "loss: 1.594299  [12864/19873]\n",
      "loss: 2.334045  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 27.5%, Avg loss: 2.244827 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 1.742792  [   64/19873]\n",
      "loss: 1.937234  [ 6464/19873]\n",
      "loss: 1.582835  [12864/19873]\n",
      "loss: 2.319221  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 27.7%, Avg loss: 2.235639 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.731247  [   64/19873]\n",
      "loss: 1.921205  [ 6464/19873]\n",
      "loss: 1.571578  [12864/19873]\n",
      "loss: 2.304525  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.4%, Avg loss: 2.226640 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.719907  [   64/19873]\n",
      "loss: 1.905106  [ 6464/19873]\n",
      "loss: 1.560471  [12864/19873]\n",
      "loss: 2.289937  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.5%, Avg loss: 2.217893 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.708828  [   64/19873]\n",
      "loss: 1.889028  [ 6464/19873]\n",
      "loss: 1.549599  [12864/19873]\n",
      "loss: 2.275442  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.9%, Avg loss: 2.209240 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1.697914  [   64/19873]\n",
      "loss: 1.873014  [ 6464/19873]\n",
      "loss: 1.538898  [12864/19873]\n",
      "loss: 2.261028  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 29.2%, Avg loss: 2.200821 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.687141  [   64/19873]\n",
      "loss: 1.856918  [ 6464/19873]\n",
      "loss: 1.528336  [12864/19873]\n",
      "loss: 2.246813  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 29.5%, Avg loss: 2.192552 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 1.676650  [   64/19873]\n",
      "loss: 1.840930  [ 6464/19873]\n",
      "loss: 1.517974  [12864/19873]\n",
      "loss: 2.232686  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 29.6%, Avg loss: 2.184472 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1.666319  [   64/19873]\n",
      "loss: 1.824897  [ 6464/19873]\n",
      "loss: 1.507766  [12864/19873]\n",
      "loss: 2.218652  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.0%, Avg loss: 2.176571 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.656162  [   64/19873]\n",
      "loss: 1.808956  [ 6464/19873]\n",
      "loss: 1.497719  [12864/19873]\n",
      "loss: 2.204711  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.168796 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1.646108  [   64/19873]\n",
      "loss: 1.792944  [ 6464/19873]\n",
      "loss: 1.487823  [12864/19873]\n",
      "loss: 2.190853  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.5%, Avg loss: 2.161232 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.636199  [   64/19873]\n",
      "loss: 1.777005  [ 6464/19873]\n",
      "loss: 1.478114  [12864/19873]\n",
      "loss: 2.177066  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.6%, Avg loss: 2.153847 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.626377  [   64/19873]\n",
      "loss: 1.761114  [ 6464/19873]\n",
      "loss: 1.468595  [12864/19873]\n",
      "loss: 2.163434  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.8%, Avg loss: 2.146601 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 1.616681  [   64/19873]\n",
      "loss: 1.745208  [ 6464/19873]\n",
      "loss: 1.459191  [12864/19873]\n",
      "loss: 2.149889  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.3%, Avg loss: 2.139568 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 1.607141  [   64/19873]\n",
      "loss: 1.729413  [ 6464/19873]\n",
      "loss: 1.449952  [12864/19873]\n",
      "loss: 2.136418  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.6%, Avg loss: 2.132643 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 1.597645  [   64/19873]\n",
      "loss: 1.713666  [ 6464/19873]\n",
      "loss: 1.440857  [12864/19873]\n",
      "loss: 2.123090  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.7%, Avg loss: 2.125890 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 1.588262  [   64/19873]\n",
      "loss: 1.698008  [ 6464/19873]\n",
      "loss: 1.431887  [12864/19873]\n",
      "loss: 2.109876  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.119351 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 1.578969  [   64/19873]\n",
      "loss: 1.682447  [ 6464/19873]\n",
      "loss: 1.423060  [12864/19873]\n",
      "loss: 2.096859  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 2.112912 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 1.569713  [   64/19873]\n",
      "loss: 1.667018  [ 6464/19873]\n",
      "loss: 1.414347  [12864/19873]\n",
      "loss: 2.083835  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.5%, Avg loss: 2.106654 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 1.560619  [   64/19873]\n",
      "loss: 1.651729  [ 6464/19873]\n",
      "loss: 1.405790  [12864/19873]\n",
      "loss: 2.070986  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.100540 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 1.551573  [   64/19873]\n",
      "loss: 1.636538  [ 6464/19873]\n",
      "loss: 1.397381  [12864/19873]\n",
      "loss: 2.058181  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 2.094550 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 1.542595  [   64/19873]\n",
      "loss: 1.621482  [ 6464/19873]\n",
      "loss: 1.389077  [12864/19873]\n",
      "loss: 2.045567  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 2.088711 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 1.533710  [   64/19873]\n",
      "loss: 1.606539  [ 6464/19873]\n",
      "loss: 1.380915  [12864/19873]\n",
      "loss: 2.033046  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 2.083030 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 1.524928  [   64/19873]\n",
      "loss: 1.591791  [ 6464/19873]\n",
      "loss: 1.372871  [12864/19873]\n",
      "loss: 2.020605  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 2.077502 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies_2 = []\n",
    "losses_2 = []\n",
    "model = NeuralNetwork(size=2048)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "for e in range(100):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    c, l = test(val_dataloader, model, loss_fn)\n",
    "    accuracies_2.append(c)\n",
    "    losses_2.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.916873  [   64/19873]\n",
      "loss: 0.789562  [ 6464/19873]\n",
      "loss: 0.876064  [12864/19873]\n",
      "loss: 1.222396  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.1%, Avg loss: 1.896070 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.911897  [   64/19873]\n",
      "loss: 0.784264  [ 6464/19873]\n",
      "loss: 0.871240  [12864/19873]\n",
      "loss: 1.214360  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.3%, Avg loss: 1.895721 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.906975  [   64/19873]\n",
      "loss: 0.778981  [ 6464/19873]\n",
      "loss: 0.866459  [12864/19873]\n",
      "loss: 1.206344  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.4%, Avg loss: 1.895393 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.902072  [   64/19873]\n",
      "loss: 0.773779  [ 6464/19873]\n",
      "loss: 0.861685  [12864/19873]\n",
      "loss: 1.198368  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.3%, Avg loss: 1.895070 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.897187  [   64/19873]\n",
      "loss: 0.768602  [ 6464/19873]\n",
      "loss: 0.856921  [12864/19873]\n",
      "loss: 1.190386  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.4%, Avg loss: 1.894773 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.892368  [   64/19873]\n",
      "loss: 0.763497  [ 6464/19873]\n",
      "loss: 0.852198  [12864/19873]\n",
      "loss: 1.182415  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 1.894503 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.887544  [   64/19873]\n",
      "loss: 0.758438  [ 6464/19873]\n",
      "loss: 0.847507  [12864/19873]\n",
      "loss: 1.174478  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 1.894292 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.882759  [   64/19873]\n",
      "loss: 0.753434  [ 6464/19873]\n",
      "loss: 0.842823  [12864/19873]\n",
      "loss: 1.166543  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.8%, Avg loss: 1.894030 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.877989  [   64/19873]\n",
      "loss: 0.748468  [ 6464/19873]\n",
      "loss: 0.838167  [12864/19873]\n",
      "loss: 1.158682  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.8%, Avg loss: 1.893794 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.873250  [   64/19873]\n",
      "loss: 0.743550  [ 6464/19873]\n",
      "loss: 0.833540  [12864/19873]\n",
      "loss: 1.150833  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 45.9%, Avg loss: 1.893615 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "for e in range(10):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model, loss_fn, optimizer)\n",
    "    c, l = test(val_dataloader, model, loss_fn)\n",
    "    accuracies_2.append(c)\n",
    "    losses_2.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.plot(range(len(accuracies_2)), accuracies_2, color='blue')\n",
    "ax1.set_ylabel('accuracy (%)', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(len(losses_2)), losses_2, color='red')\n",
    "ax2.set_ylabel('loss', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.435303  [   64/19873]\n",
      "loss: 3.433383  [ 6464/19873]\n",
      "loss: 3.425956  [12864/19873]\n",
      "loss: 3.436632  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.0%, Avg loss: 3.429720 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.421176  [   64/19873]\n",
      "loss: 3.429466  [ 6464/19873]\n",
      "loss: 3.411180  [12864/19873]\n",
      "loss: 3.428868  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.5%, Avg loss: 3.422830 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.406147  [   64/19873]\n",
      "loss: 3.419096  [ 6464/19873]\n",
      "loss: 3.394997  [12864/19873]\n",
      "loss: 3.422561  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.6%, Avg loss: 3.415558 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.395527  [   64/19873]\n",
      "loss: 3.410601  [ 6464/19873]\n",
      "loss: 3.370490  [12864/19873]\n",
      "loss: 3.415933  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 2.9%, Avg loss: 3.407149 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.380742  [   64/19873]\n",
      "loss: 3.402539  [ 6464/19873]\n",
      "loss: 3.353793  [12864/19873]\n",
      "loss: 3.408164  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.3%, Avg loss: 3.396933 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.364247  [   64/19873]\n",
      "loss: 3.395374  [ 6464/19873]\n",
      "loss: 3.320056  [12864/19873]\n",
      "loss: 3.401972  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.384807 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.341234  [   64/19873]\n",
      "loss: 3.381009  [ 6464/19873]\n",
      "loss: 3.295383  [12864/19873]\n",
      "loss: 3.385710  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 4.3%, Avg loss: 3.370329 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.317808  [   64/19873]\n",
      "loss: 3.368081  [ 6464/19873]\n",
      "loss: 3.240517  [12864/19873]\n",
      "loss: 3.373111  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 5.9%, Avg loss: 3.352362 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.285914  [   64/19873]\n",
      "loss: 3.356534  [ 6464/19873]\n",
      "loss: 3.169676  [12864/19873]\n",
      "loss: 3.360544  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 6.3%, Avg loss: 3.331250 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.247414  [   64/19873]\n",
      "loss: 3.341382  [ 6464/19873]\n",
      "loss: 3.095937  [12864/19873]\n",
      "loss: 3.346057  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.6%, Avg loss: 3.305726 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 3.190787  [   64/19873]\n",
      "loss: 3.320016  [ 6464/19873]\n",
      "loss: 2.990844  [12864/19873]\n",
      "loss: 3.336720  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 7.9%, Avg loss: 3.274809 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 3.130690  [   64/19873]\n",
      "loss: 3.319453  [ 6464/19873]\n",
      "loss: 2.910291  [12864/19873]\n",
      "loss: 3.333508  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.3%, Avg loss: 3.243438 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 3.093174  [   64/19873]\n",
      "loss: 3.298279  [ 6464/19873]\n",
      "loss: 2.769455  [12864/19873]\n",
      "loss: 3.315609  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 8.8%, Avg loss: 3.213024 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 3.038980  [   64/19873]\n",
      "loss: 3.287221  [ 6464/19873]\n",
      "loss: 2.682013  [12864/19873]\n",
      "loss: 3.310664  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.3%, Avg loss: 3.187288 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 3.011700  [   64/19873]\n",
      "loss: 3.269957  [ 6464/19873]\n",
      "loss: 2.608154  [12864/19873]\n",
      "loss: 3.298039  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 3.168189 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.984171  [   64/19873]\n",
      "loss: 3.271544  [ 6464/19873]\n",
      "loss: 2.555408  [12864/19873]\n",
      "loss: 3.295652  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.6%, Avg loss: 3.152353 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.966114  [   64/19873]\n",
      "loss: 3.263028  [ 6464/19873]\n",
      "loss: 2.505580  [12864/19873]\n",
      "loss: 3.295261  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.9%, Avg loss: 3.139694 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.946092  [   64/19873]\n",
      "loss: 3.251665  [ 6464/19873]\n",
      "loss: 2.475087  [12864/19873]\n",
      "loss: 3.292845  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 11.6%, Avg loss: 3.127079 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.931578  [   64/19873]\n",
      "loss: 3.219734  [ 6464/19873]\n",
      "loss: 2.484648  [12864/19873]\n",
      "loss: 3.277603  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.0%, Avg loss: 3.115566 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.932744  [   64/19873]\n",
      "loss: 3.217957  [ 6464/19873]\n",
      "loss: 2.447702  [12864/19873]\n",
      "loss: 3.272774  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.5%, Avg loss: 3.105189 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.922554  [   64/19873]\n",
      "loss: 3.187448  [ 6464/19873]\n",
      "loss: 2.459144  [12864/19873]\n",
      "loss: 3.253635  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.7%, Avg loss: 3.094009 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.905022  [   64/19873]\n",
      "loss: 3.198759  [ 6464/19873]\n",
      "loss: 2.452143  [12864/19873]\n",
      "loss: 3.275882  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.8%, Avg loss: 3.083102 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.903258  [   64/19873]\n",
      "loss: 3.154045  [ 6464/19873]\n",
      "loss: 2.395744  [12864/19873]\n",
      "loss: 3.247340  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.7%, Avg loss: 3.070593 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.893383  [   64/19873]\n",
      "loss: 3.176259  [ 6464/19873]\n",
      "loss: 2.410132  [12864/19873]\n",
      "loss: 3.252085  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 12.9%, Avg loss: 3.058961 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.883545  [   64/19873]\n",
      "loss: 3.122593  [ 6464/19873]\n",
      "loss: 2.375305  [12864/19873]\n",
      "loss: 3.235960  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.4%, Avg loss: 3.046358 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.850118  [   64/19873]\n",
      "loss: 3.136980  [ 6464/19873]\n",
      "loss: 2.360550  [12864/19873]\n",
      "loss: 3.217020  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.3%, Avg loss: 3.032693 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.863816  [   64/19873]\n",
      "loss: 3.073217  [ 6464/19873]\n",
      "loss: 2.355459  [12864/19873]\n",
      "loss: 3.207226  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.5%, Avg loss: 3.018373 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.825307  [   64/19873]\n",
      "loss: 3.050124  [ 6464/19873]\n",
      "loss: 2.320811  [12864/19873]\n",
      "loss: 3.232358  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.8%, Avg loss: 3.004747 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.796802  [   64/19873]\n",
      "loss: 3.037454  [ 6464/19873]\n",
      "loss: 2.321666  [12864/19873]\n",
      "loss: 3.210397  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 13.9%, Avg loss: 2.988190 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.793423  [   64/19873]\n",
      "loss: 3.017641  [ 6464/19873]\n",
      "loss: 2.345078  [12864/19873]\n",
      "loss: 3.206713  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.2%, Avg loss: 2.973689 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.803565  [   64/19873]\n",
      "loss: 2.973118  [ 6464/19873]\n",
      "loss: 2.300521  [12864/19873]\n",
      "loss: 3.187179  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.4%, Avg loss: 2.956198 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.717770  [   64/19873]\n",
      "loss: 2.965879  [ 6464/19873]\n",
      "loss: 2.282946  [12864/19873]\n",
      "loss: 3.145153  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.7%, Avg loss: 2.940132 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.716528  [   64/19873]\n",
      "loss: 2.934612  [ 6464/19873]\n",
      "loss: 2.252134  [12864/19873]\n",
      "loss: 3.154399  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.8%, Avg loss: 2.923950 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.699731  [   64/19873]\n",
      "loss: 2.887890  [ 6464/19873]\n",
      "loss: 2.259753  [12864/19873]\n",
      "loss: 3.142485  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 14.9%, Avg loss: 2.904261 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.666926  [   64/19873]\n",
      "loss: 2.874518  [ 6464/19873]\n",
      "loss: 2.210109  [12864/19873]\n",
      "loss: 3.128900  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.2%, Avg loss: 2.885989 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.613228  [   64/19873]\n",
      "loss: 2.833631  [ 6464/19873]\n",
      "loss: 2.209918  [12864/19873]\n",
      "loss: 3.090231  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.4%, Avg loss: 2.868398 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.643148  [   64/19873]\n",
      "loss: 2.819476  [ 6464/19873]\n",
      "loss: 2.210443  [12864/19873]\n",
      "loss: 3.096502  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.6%, Avg loss: 2.849350 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.621670  [   64/19873]\n",
      "loss: 2.815476  [ 6464/19873]\n",
      "loss: 2.168428  [12864/19873]\n",
      "loss: 3.076034  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 2.831191 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.571206  [   64/19873]\n",
      "loss: 2.789398  [ 6464/19873]\n",
      "loss: 2.136971  [12864/19873]\n",
      "loss: 3.081536  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 15.8%, Avg loss: 2.811930 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.600019  [   64/19873]\n",
      "loss: 2.721188  [ 6464/19873]\n",
      "loss: 2.162649  [12864/19873]\n",
      "loss: 2.996585  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.1%, Avg loss: 2.791603 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.539439  [   64/19873]\n",
      "loss: 2.758739  [ 6464/19873]\n",
      "loss: 2.166703  [12864/19873]\n",
      "loss: 3.034481  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.3%, Avg loss: 2.773415 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.470910  [   64/19873]\n",
      "loss: 2.629654  [ 6464/19873]\n",
      "loss: 2.111783  [12864/19873]\n",
      "loss: 3.045852  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 16.9%, Avg loss: 2.755659 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.584340  [   64/19873]\n",
      "loss: 2.653347  [ 6464/19873]\n",
      "loss: 2.094525  [12864/19873]\n",
      "loss: 2.936937  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 17.4%, Avg loss: 2.735146 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.465574  [   64/19873]\n",
      "loss: 2.641146  [ 6464/19873]\n",
      "loss: 2.100055  [12864/19873]\n",
      "loss: 2.990763  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 17.3%, Avg loss: 2.719423 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.467986  [   64/19873]\n",
      "loss: 2.581521  [ 6464/19873]\n",
      "loss: 2.082347  [12864/19873]\n",
      "loss: 2.957105  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 17.5%, Avg loss: 2.698485 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.409384  [   64/19873]\n",
      "loss: 2.558115  [ 6464/19873]\n",
      "loss: 2.083901  [12864/19873]\n",
      "loss: 2.910581  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.0%, Avg loss: 2.680787 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.422258  [   64/19873]\n",
      "loss: 2.550656  [ 6464/19873]\n",
      "loss: 1.994004  [12864/19873]\n",
      "loss: 2.956402  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.3%, Avg loss: 2.663420 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.283886  [   64/19873]\n",
      "loss: 2.528315  [ 6464/19873]\n",
      "loss: 2.070432  [12864/19873]\n",
      "loss: 2.878716  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.3%, Avg loss: 2.648510 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.334109  [   64/19873]\n",
      "loss: 2.498611  [ 6464/19873]\n",
      "loss: 2.036412  [12864/19873]\n",
      "loss: 2.915639  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 18.5%, Avg loss: 2.628202 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.430035  [   64/19873]\n",
      "loss: 2.504264  [ 6464/19873]\n",
      "loss: 2.011130  [12864/19873]\n",
      "loss: 2.878916  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 2.613108 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 2.268857  [   64/19873]\n",
      "loss: 2.419435  [ 6464/19873]\n",
      "loss: 1.969747  [12864/19873]\n",
      "loss: 2.876119  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.2%, Avg loss: 2.595380 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.240798  [   64/19873]\n",
      "loss: 2.429831  [ 6464/19873]\n",
      "loss: 1.963517  [12864/19873]\n",
      "loss: 2.864640  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.5%, Avg loss: 2.578703 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 2.212887  [   64/19873]\n",
      "loss: 2.390513  [ 6464/19873]\n",
      "loss: 2.005246  [12864/19873]\n",
      "loss: 2.836380  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 19.9%, Avg loss: 2.561573 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.272862  [   64/19873]\n",
      "loss: 2.425971  [ 6464/19873]\n",
      "loss: 1.929882  [12864/19873]\n",
      "loss: 2.752852  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 20.5%, Avg loss: 2.546619 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 2.190085  [   64/19873]\n",
      "loss: 2.412265  [ 6464/19873]\n",
      "loss: 1.922190  [12864/19873]\n",
      "loss: 2.748745  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 20.8%, Avg loss: 2.529417 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.125372  [   64/19873]\n",
      "loss: 2.368660  [ 6464/19873]\n",
      "loss: 1.924127  [12864/19873]\n",
      "loss: 2.776886  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 21.1%, Avg loss: 2.516365 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.094884  [   64/19873]\n",
      "loss: 2.326725  [ 6464/19873]\n",
      "loss: 1.907171  [12864/19873]\n",
      "loss: 2.681341  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 21.4%, Avg loss: 2.502509 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 2.091086  [   64/19873]\n",
      "loss: 2.331502  [ 6464/19873]\n",
      "loss: 1.866216  [12864/19873]\n",
      "loss: 2.763723  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 22.0%, Avg loss: 2.485784 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 2.091828  [   64/19873]\n",
      "loss: 2.272460  [ 6464/19873]\n",
      "loss: 1.826729  [12864/19873]\n",
      "loss: 2.750047  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 22.1%, Avg loss: 2.471797 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 2.110376  [   64/19873]\n",
      "loss: 2.319203  [ 6464/19873]\n",
      "loss: 1.897441  [12864/19873]\n",
      "loss: 2.647849  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 22.3%, Avg loss: 2.458948 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 2.079826  [   64/19873]\n",
      "loss: 2.259358  [ 6464/19873]\n",
      "loss: 1.907268  [12864/19873]\n",
      "loss: 2.694066  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.3%, Avg loss: 2.445553 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 2.070531  [   64/19873]\n",
      "loss: 2.321712  [ 6464/19873]\n",
      "loss: 1.870445  [12864/19873]\n",
      "loss: 2.644352  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.1%, Avg loss: 2.434299 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.974220  [   64/19873]\n",
      "loss: 2.262561  [ 6464/19873]\n",
      "loss: 1.815862  [12864/19873]\n",
      "loss: 2.716800  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.5%, Avg loss: 2.423703 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.966478  [   64/19873]\n",
      "loss: 2.191677  [ 6464/19873]\n",
      "loss: 1.741750  [12864/19873]\n",
      "loss: 2.667395  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 2.407624 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.995752  [   64/19873]\n",
      "loss: 2.211467  [ 6464/19873]\n",
      "loss: 1.771068  [12864/19873]\n",
      "loss: 2.636520  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 2.395801 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.928159  [   64/19873]\n",
      "loss: 2.216326  [ 6464/19873]\n",
      "loss: 1.773091  [12864/19873]\n",
      "loss: 2.563038  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.2%, Avg loss: 2.381169 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.905897  [   64/19873]\n",
      "loss: 2.121317  [ 6464/19873]\n",
      "loss: 1.843947  [12864/19873]\n",
      "loss: 2.562106  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.5%, Avg loss: 2.370221 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.924825  [   64/19873]\n",
      "loss: 2.172984  [ 6464/19873]\n",
      "loss: 1.744552  [12864/19873]\n",
      "loss: 2.568248  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 24.9%, Avg loss: 2.360708 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.954252  [   64/19873]\n",
      "loss: 2.101141  [ 6464/19873]\n",
      "loss: 1.727708  [12864/19873]\n",
      "loss: 2.518382  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.4%, Avg loss: 2.343865 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.938911  [   64/19873]\n",
      "loss: 2.166534  [ 6464/19873]\n",
      "loss: 1.796974  [12864/19873]\n",
      "loss: 2.523363  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.4%, Avg loss: 2.334183 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.947743  [   64/19873]\n",
      "loss: 2.134845  [ 6464/19873]\n",
      "loss: 1.730815  [12864/19873]\n",
      "loss: 2.542279  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 25.7%, Avg loss: 2.324772 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.931748  [   64/19873]\n",
      "loss: 2.108595  [ 6464/19873]\n",
      "loss: 1.690398  [12864/19873]\n",
      "loss: 2.474205  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.2%, Avg loss: 2.315487 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.818860  [   64/19873]\n",
      "loss: 2.063519  [ 6464/19873]\n",
      "loss: 1.728021  [12864/19873]\n",
      "loss: 2.506834  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.4%, Avg loss: 2.299814 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.835712  [   64/19873]\n",
      "loss: 2.009931  [ 6464/19873]\n",
      "loss: 1.668350  [12864/19873]\n",
      "loss: 2.396073  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 2.290869 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.913054  [   64/19873]\n",
      "loss: 2.052933  [ 6464/19873]\n",
      "loss: 1.670374  [12864/19873]\n",
      "loss: 2.392363  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 26.8%, Avg loss: 2.278922 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.814734  [   64/19873]\n",
      "loss: 2.032506  [ 6464/19873]\n",
      "loss: 1.661774  [12864/19873]\n",
      "loss: 2.461719  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 27.2%, Avg loss: 2.270433 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.863153  [   64/19873]\n",
      "loss: 2.123239  [ 6464/19873]\n",
      "loss: 1.657315  [12864/19873]\n",
      "loss: 2.420398  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 27.2%, Avg loss: 2.259337 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 1.835639  [   64/19873]\n",
      "loss: 2.032495  [ 6464/19873]\n",
      "loss: 1.667662  [12864/19873]\n",
      "loss: 2.350670  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 27.8%, Avg loss: 2.253398 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.701149  [   64/19873]\n",
      "loss: 1.981713  [ 6464/19873]\n",
      "loss: 1.646642  [12864/19873]\n",
      "loss: 2.396535  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.2%, Avg loss: 2.240113 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.801584  [   64/19873]\n",
      "loss: 2.018407  [ 6464/19873]\n",
      "loss: 1.612653  [12864/19873]\n",
      "loss: 2.262798  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.5%, Avg loss: 2.233845 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.743808  [   64/19873]\n",
      "loss: 2.003913  [ 6464/19873]\n",
      "loss: 1.584492  [12864/19873]\n",
      "loss: 2.335727  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.5%, Avg loss: 2.220297 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1.732970  [   64/19873]\n",
      "loss: 1.863067  [ 6464/19873]\n",
      "loss: 1.610538  [12864/19873]\n",
      "loss: 2.364657  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.8%, Avg loss: 2.210589 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.781963  [   64/19873]\n",
      "loss: 2.003650  [ 6464/19873]\n",
      "loss: 1.620235  [12864/19873]\n",
      "loss: 2.299322  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 28.7%, Avg loss: 2.208408 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 1.748018  [   64/19873]\n",
      "loss: 1.930883  [ 6464/19873]\n",
      "loss: 1.540378  [12864/19873]\n",
      "loss: 2.301876  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 29.3%, Avg loss: 2.198759 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1.661351  [   64/19873]\n",
      "loss: 1.921940  [ 6464/19873]\n",
      "loss: 1.584822  [12864/19873]\n",
      "loss: 2.236001  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 29.5%, Avg loss: 2.181399 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.697485  [   64/19873]\n",
      "loss: 1.871634  [ 6464/19873]\n",
      "loss: 1.550197  [12864/19873]\n",
      "loss: 2.265516  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 29.6%, Avg loss: 2.185659 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1.740355  [   64/19873]\n",
      "loss: 1.867178  [ 6464/19873]\n",
      "loss: 1.570935  [12864/19873]\n",
      "loss: 2.248105  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.178717 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.660490  [   64/19873]\n",
      "loss: 1.940453  [ 6464/19873]\n",
      "loss: 1.545779  [12864/19873]\n",
      "loss: 2.238877  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.2%, Avg loss: 2.167894 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.663184  [   64/19873]\n",
      "loss: 1.844988  [ 6464/19873]\n",
      "loss: 1.555760  [12864/19873]\n",
      "loss: 2.192686  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.6%, Avg loss: 2.147909 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 1.648780  [   64/19873]\n",
      "loss: 1.845898  [ 6464/19873]\n",
      "loss: 1.508896  [12864/19873]\n",
      "loss: 2.211290  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.7%, Avg loss: 2.150726 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 1.738052  [   64/19873]\n",
      "loss: 1.804199  [ 6464/19873]\n",
      "loss: 1.525149  [12864/19873]\n",
      "loss: 2.204281  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 30.9%, Avg loss: 2.144012 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 1.613749  [   64/19873]\n",
      "loss: 1.730989  [ 6464/19873]\n",
      "loss: 1.502622  [12864/19873]\n",
      "loss: 2.199765  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.4%, Avg loss: 2.134980 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 1.618451  [   64/19873]\n",
      "loss: 1.830508  [ 6464/19873]\n",
      "loss: 1.549737  [12864/19873]\n",
      "loss: 2.242154  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.7%, Avg loss: 2.123661 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 1.659502  [   64/19873]\n",
      "loss: 1.787352  [ 6464/19873]\n",
      "loss: 1.438630  [12864/19873]\n",
      "loss: 2.176654  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.8%, Avg loss: 2.115735 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 1.687409  [   64/19873]\n",
      "loss: 1.759031  [ 6464/19873]\n",
      "loss: 1.418063  [12864/19873]\n",
      "loss: 2.128254  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 2.118218 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 1.648623  [   64/19873]\n",
      "loss: 1.770069  [ 6464/19873]\n",
      "loss: 1.493962  [12864/19873]\n",
      "loss: 2.130601  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.111763 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 1.629756  [   64/19873]\n",
      "loss: 1.801179  [ 6464/19873]\n",
      "loss: 1.533820  [12864/19873]\n",
      "loss: 2.145813  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 2.097606 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 1.616085  [   64/19873]\n",
      "loss: 1.696256  [ 6464/19873]\n",
      "loss: 1.432214  [12864/19873]\n",
      "loss: 2.139893  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 2.093371 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 1.573698  [   64/19873]\n",
      "loss: 1.689984  [ 6464/19873]\n",
      "loss: 1.478355  [12864/19873]\n",
      "loss: 2.164626  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.088574 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 1.608140  [   64/19873]\n",
      "loss: 1.716438  [ 6464/19873]\n",
      "loss: 1.427154  [12864/19873]\n",
      "loss: 2.102035  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 2.083086 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=256):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(71636, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "accuracies_3 = []\n",
    "losses_3 = []\n",
    "model_3 = NeuralNetwork(size=2048)\n",
    "optimizer = torch.optim.SGD(model_3.parameters(), lr=0.001)\n",
    "for e in range(100):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model_3, loss_fn, optimizer)\n",
    "    c, l = test(val_dataloader, model_3, loss_fn)\n",
    "    accuracies_3.append(c)\n",
    "    losses_3.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=512):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(71636, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "accuracies_4 = []\n",
    "losses_4 = []\n",
    "model_4 = NeuralNetwork(size=2048)\n",
    "optimizer = torch.optim.Adam(model_4.parameters(), lr=0.001)\n",
    "for e in range(100):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model_4, loss_fn, optimizer)\n",
    "    c, l = test(val_dataloader, model_4, loss_fn)\n",
    "    accuracies_4.append(c)\n",
    "    losses_4.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 266636])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=200000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(train['snip'])\n",
    "X_val_tfidf = tfidf.transform(val['snip'])\n",
    "\n",
    "x_train = [X_train_tfidf, x_train_cv]\n",
    "x_val = [X_val_tfidf, x_val_cv]\n",
    "# Combine TF-IDF and CountVectorizer features\n",
    "from scipy.sparse import hstack\n",
    "x_train_combined = hstack(x_train)\n",
    "x_val_combined = hstack(x_val)\n",
    "\n",
    "training_data = torch.tensor(x_train_combined.toarray(), dtype=torch.float32)\n",
    "val_data = torch.tensor(x_val_combined.toarray(), dtype=torch.float32)\n",
    "train_tensor = torch.utils.data.TensorDataset(training_data, training_labels)\n",
    "val_tensor = torch.utils.data.TensorDataset(val_data, val_labels)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_tensor, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_tensor, batch_size=batch_size)\n",
    "\n",
    "for X, y in val_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.433814  [   64/19873]\n",
      "loss: 3.403761  [ 6464/19873]\n",
      "loss: 3.288266  [12864/19873]\n",
      "loss: 3.335662  [19264/19873]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg loss: 3.287629 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.203996  [   64/19873]\n",
      "loss: 3.273270  [ 6464/19873]\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size=512):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(266636, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, 31),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model_new_param = NeuralNetwork(size=2048)\n",
    "optimizer = torch.optim.SGD(model_new_param.parameters(), lr=0.01)\n",
    "accuracies_new_param = []\n",
    "losses_new_param = []\n",
    "for e in range(100):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_fxn(train_dataloader, model_new_param, loss_fn, optimizer)\n",
    "    c, l = test(val_dataloader, model_new_param, loss_fn)\n",
    "    accuracies_new_param.append(c)\n",
    "    losses_new_param.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
